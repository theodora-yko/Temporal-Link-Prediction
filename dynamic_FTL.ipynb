{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib/python3.8/site-packages (22.0.4)\n",
      "Requirement already satisfied: install in ./venv/lib/python3.8/site-packages (1.3.5)\n",
      "Collecting stellargraph\n",
      "  Using cached stellargraph-1.2.1-py3-none-any.whl (435 kB)\n",
      "Collecting chardet\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.2-cp38-cp38-macosx_10_9_x86_64.whl (24.1 MB)\n",
      "Requirement already satisfied: numpy>=1.14 in ./venv/lib/python3.8/site-packages (from stellargraph) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in ./venv/lib/python3.8/site-packages (from stellargraph) (1.10.1)\n",
      "Requirement already satisfied: pandas>=0.24 in ./venv/lib/python3.8/site-packages (from stellargraph) (2.0.3)\n",
      "Requirement already satisfied: networkx>=2.2 in ./venv/lib/python3.8/site-packages (from stellargraph) (3.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in ./venv/lib/python3.8/site-packages (from stellargraph) (3.7.5)\n",
      "Collecting tensorflow>=2.1.0\n",
      "  Using cached tensorflow-2.13.1-cp38-cp38-macosx_10_15_x86_64.whl (216.2 MB)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in ./venv/lib/python3.8/site-packages (from stellargraph) (1.3.2)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib>=2.2->stellargraph) (6.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.8/site-packages (from matplotlib>=2.2->stellargraph) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from matplotlib>=2.2->stellargraph) (24.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib>=2.2->stellargraph) (10.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib>=2.2->stellargraph) (1.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib>=2.2->stellargraph) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.8/site-packages (from matplotlib>=2.2->stellargraph) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.8/site-packages (from matplotlib>=2.2->stellargraph) (2.9.0.post0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.8/site-packages (from matplotlib>=2.2->stellargraph) (4.51.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.8/site-packages (from pandas>=0.24->stellargraph) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.8/site-packages (from pandas>=0.24->stellargraph) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./venv/lib/python3.8/site-packages (from scikit-learn>=0.20->stellargraph) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.8/site-packages (from scikit-learn>=0.20->stellargraph) (3.4.0)\n",
      "Collecting wrapt\n",
      "  Using cached wrapt-1.16.0-cp38-cp38-macosx_10_9_x86_64.whl (37 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-macosx_10_14_x86_64.whl (1.7 MB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.62.2-cp38-cp38-macosx_10_10_universal2.whl (10.1 MB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.8/site-packages (from tensorflow>=2.1.0->stellargraph) (1.16.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting numpy>=1.14\n",
      "  Using cached numpy-1.24.3-cp38-cp38-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from tensorflow>=2.1.0->stellargraph) (56.0.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl (26.5 MB)\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.11.0-cp38-cp38-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib>=2.2->stellargraph) (3.18.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./venv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->stellargraph) (7.1.0)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp38-cp38-macosx_10_9_x86_64.whl (121 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Using cached MarkupSafe-2.1.5-cp38-cp38-macosx_10_9_x86_64.whl (14 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, MarkupSafe, keras, idna, grpcio, google-pasta, gast, charset-normalizer, chardet, certifi, cachetools, absl-py, werkzeug, smart-open, rsa, requests, pyasn1-modules, opt-einsum, markdown, h5py, astunparse, requests-oauthlib, google-auth, gensim, google-auth-oauthlib, tensorboard, tensorflow, stellargraph\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 certifi-2024.2.2 chardet-5.2.0 charset-normalizer-3.3.2 flatbuffers-24.3.25 gast-0.4.0 gensim-4.3.2 google-auth-2.29.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.62.2 h5py-3.11.0 idna-3.7 keras-2.13.1 libclang-18.1.1 markdown-3.6 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.25.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 requests-2.31.0 requests-oauthlib-2.0.0 rsa-4.9 smart-open-7.0.4 stellargraph-1.2.1 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.4.0 typing-extensions-4.5.0 urllib3-2.2.1 werkzeug-3.0.2 wheel-0.43.0 wrapt-1.16.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/yko/Documents/GitHub/Temporal-Link-Prediction/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install pip install stellargraph chardet gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 18:48:50.129761: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "from utils import * \n",
    "import stellargraph as sg\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import EdgeSplitter, BiasedRandomWalk, TemporalRandomWalk\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "from math import isclose\n",
    "from sklearn.decomposition import PCA\n",
    "import multiprocessing\n",
    "import sklearn.model_selection \n",
    "from gensim.models import Word2Vec\n",
    "# python3 -m pip install tqdm seaborn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM_Time() & Weekly Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class temporalNetwork(): \n",
    "    def __init__(self, start_date, end_date, display_progress=False, location_grouping='kma', origin=None, facility_id=None, intermediate=None):\n",
    "        \"\"\" \n",
    "        Note: \n",
    "            start_date and end_date should be both None as they are used as a signal to \n",
    "            CM_Time's run_simulation to whether construct a new graph or update the graph with new information \n",
    "        \"\"\"\n",
    "        self.display_progress = display_progress\n",
    "        self.start_date=start_date\n",
    "        self.end_date=end_date\n",
    "        self.network=None\n",
    "        self.origin_location_list=None\n",
    "        self.location_grouping=location_grouping\n",
    "        self.inbound_data = None \n",
    "        self.outbound_data = None\n",
    "\n",
    "    def construct_network_graph(self):\n",
    "        \"\"\" \n",
    "        Given an inbound and outbound dataframe, construct a network graph and stores it in the class variable self.network\n",
    "        Args: \n",
    "            an_inbound_df (pd.DataFrame): inbound dataframe\n",
    "            an_outbound_df (pd.DataFrame): outbound dataframe\n",
    "            start_date (datetime): start date of the network graph\n",
    "            end_date (datetime): end date of the network graph\n",
    "            location_grouping(string): 'kma' or 'zip3'\n",
    "        \"\"\"\n",
    "        # pull data & construct an empty multiDiGraph\n",
    "\n",
    "        date = self.start_date.strftime(\"%Y-%m-%d\") + \"_\" + self.end_date.strftime(\"%Y-%m-%d\")\n",
    "        an_inbound_df, an_outbound_df = pd.read_csv(f\"inbound_{self.location_grouping}_data/inbound_data_{date}.csv\"), pd.read_csv(f\"outbound_{self.location_grouping}_data/outbound_data_{date}.csv\")\n",
    "        an_inbound_df.load_date, an_outbound_df.load_date = pd.to_datetime(an_inbound_df.load_date), pd.to_datetime(an_outbound_df.load_date)\n",
    "        for colin, colout in zip(an_inbound_df.columns, an_outbound_df.columns): \n",
    "            if colin not in [\"total_loads\", \"load_date\"]: \n",
    "                an_inbound_df[colin] = an_inbound_df[colin].astype(str)\n",
    "            if colout not in [\"total_loads\", \"load_date\"]:\n",
    "                an_outbound_df[colout] = an_outbound_df[colout].astype(str)\n",
    "\n",
    "        network_graph = nx.MultiDiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # network_graph = nx.DiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # idf, odf = an_inbound_df.copy(), an_outbound_df.copy()\n",
    "\n",
    "        # idf.to_csv(f\"data/inbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "        # odf.to_csv(f\"data/outbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "\n",
    "        # add nodes & edges \n",
    "        node_1 = f\"origin_{self.location_grouping}_id\"\n",
    "        node_2 = f\"facility_{self.location_grouping}_id\"\n",
    "        node_3 = f\"destination_{self.location_grouping}_id\"\n",
    "\n",
    "        network_graph = add_nodes_given_df(network_graph, an_inbound_df, [node_1, 'facility_id']) \n",
    "        network_graph = add_nodes_given_df(network_graph, an_outbound_df, ['facility_id', node_2, node_3]) \n",
    "\n",
    "        network_graph = add_edges_given_graph(network_graph, an_inbound_df, an_outbound_df, self.location_grouping)\n",
    "        \n",
    "        # update the variables \n",
    "        self.network = network_graph\n",
    "        self.origin_location_list = an_inbound_df[f'origin_{self.location_grouping}_id'].unique()\n",
    "        self.inbound_data, self.outbound_data = an_inbound_df, an_outbound_df\n",
    "        if self.display_progress: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\") \n",
    "        \n",
    "    def move_to_next_week(self): \n",
    "        \"\"\" \n",
    "        Given the new week's inbound and outbound dataframes, \n",
    "        update self.network graph, self.start_date, and self.end_date to a week after current start date and end date \n",
    "        \n",
    "        Args:\n",
    "            next_inbound_df (pd.DataFrame): new week's inbound dataframe\n",
    "            next_outbound_df (pd.DataFrame): new week's outbound dataframe\n",
    "            display_progress (boolean): whether to display the progress of the function or not\n",
    "        \"\"\"\n",
    "        # update the dates, pull new week's data, & store some informations\n",
    "        self.start_date, self.end_date = self.start_date + timedelta(days=7), self.end_date + timedelta(days=7)\n",
    "        date = self.start_date.strftime(\"%Y-%m-%d\") + \"_\" + self.end_date.strftime(\"%Y-%m-%d\")\n",
    "        next_inbound_df, next_outbound_df = pd.read_csv(f\"inbound_{self.location_grouping}_data/inbound_data_{date}.csv\"), pd.read_csv(f\"outbound_{self.location_grouping}_data/outbound_data_{date}.csv\")\n",
    "        next_inbound_df.load_date, next_outbound_df.load_date = pd.to_datetime(next_inbound_df.load_date), pd.to_datetime(next_outbound_df.load_date)\n",
    "        for colin, colout in zip(next_inbound_df.columns, next_outbound_df.columns): \n",
    "            if colin not in [\"total_loads\", \"load_date\"]: \n",
    "                next_inbound_df[colin] = next_inbound_df[colin].astype(str)\n",
    "            if colout not in [\"total_loads\", \"load_date\"]:\n",
    "                next_outbound_df[colout] = next_outbound_df[colout].astype(str)\n",
    "\n",
    "        network_graph = nx.MultiDiGraph(name=f\"new network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # network_graph = nx.DiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # idf, odf = an_inbound_df.copy(), an_outbound_df.copy()\n",
    "\n",
    "        # idf.to_csv(f\"data/inbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "        # odf.to_csv(f\"data/outbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "\n",
    "        # add nodes & edges \n",
    "        node_1 = f\"origin_{self.location_grouping}_id\"\n",
    "        node_2 = f\"facility_{self.location_grouping}_id\"\n",
    "        node_3 = f\"destination_{self.location_grouping}_id\"\n",
    "\n",
    "        network_graph = add_nodes_given_df(network_graph, next_inbound_df, [node_1, 'facility_id']) \n",
    "        network_graph = add_nodes_given_df(network_graph, next_outbound_df, ['facility_id', node_2, node_3]) \n",
    "\n",
    "        network_graph = add_edges_given_graph(network_graph, next_inbound_df, next_outbound_df, self.location_grouping)\n",
    "\n",
    "         # update the variables \n",
    "        self.network = network_graph\n",
    "        self.origin_location_list = next_inbound_df[f'origin_{self.location_grouping}_id'].unique()\n",
    "        self.inbound_data, self.outbound_data = next_inbound_df, next_outbound_df\n",
    "        if self.display_progress: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\") \n",
    "       \n",
    "\n",
    "    def print_network_information(self, given_network, print_network_time=False): \n",
    "        \"\"\"\n",
    "        Given a network, print out the information of the network\n",
    "        Args: \n",
    "            given_network (nx.MultiDiGraph): a network graph\n",
    "        Returns: N/A\n",
    "        \"\"\"\n",
    "        print(\"---------------------------------------------------------------------------------------------\") \n",
    "        print(given_network)\n",
    "        print(f\"Is the given network a DAG for load_network?: {nx.is_directed_acyclic_graph(given_network)}\")\n",
    "        print(f\"Number of self loops: {nx.number_of_selfloops(given_network)}\")\n",
    "        if print_network_time: print(f\"Current time of the graph: {given_network.graph['start_date']} to {given_network.graph['end_date']}\")\n",
    "        else: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_Finder():\n",
    "    def __init__(self, location_grouping='kma', origin_location_list=None, network = None):\n",
    "        self.network=network\n",
    "        self.processed_network=None\n",
    "        self.processed=False \n",
    "        self.origin_location_list=origin_location_list\n",
    "        self.match_failure = None\n",
    "        self.remove_failure = 0\n",
    "        self.location_grouping=location_grouping\n",
    "        \n",
    "    def group_to_DiGraph(self, display_progress = False):\n",
    "        \"\"\"\n",
    "        #TODO: explain why aggregate_faciility_zip then group_to_DiGraph (kma-> facility_zip -> kma to kma->kma->kma, aggregate to faciliy KMA)\n",
    "        Assuming that self.network is constructed, \n",
    "        sums the edge weights for edges with the same nodes in self.network variable and \n",
    "        stores the new graph with aggregated edges in self.processed_network variable and returns False if successful \n",
    "\n",
    "        Args:\n",
    "            display_progress (boolean): whether to display the progress of the function or not\n",
    "\n",
    "        NOTE) disregards temporal factor \n",
    "        \"\"\"\n",
    "        if not self.network: \n",
    "            print(\"Please construct the network first\")\n",
    "            return None \n",
    "            \n",
    "        new_name = self.network.name + \" reduced\"\n",
    "        self.processed_network = nx.DiGraph(name=new_name)\n",
    "        self.processed_network.add_nodes_from(self.network)\n",
    "\n",
    "        if display_progress: print(\"Aggregating nodes by KMA...\")\n",
    "        for n1, n2 in self.network.edges():\n",
    "            sum = 0 \n",
    "            for inner_dict in self.network.get_edge_data(n1, n2).values(): \n",
    "                sum += inner_dict['capacity']\n",
    "            self.processed_network.add_edge(n1, n2, capacity = sum)\n",
    "        \n",
    "        nx.set_edge_attributes(self.processed_network, to_integer(self.network.graph['end_date']), 'time')\n",
    "        self.processed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_Time(): \n",
    "    def __init__(self, location_grouping='kma', origin=None, facility_id=None, intermediate=None): \n",
    "        self.start_date=None\n",
    "        self.end_date=None\n",
    "        self.cm_network = None   \n",
    "        self.cm_finder = None\n",
    "        self.origin=origin\n",
    "        self.facility_id=facility_id\n",
    "        self.intermediate=intermediate\n",
    "        self.location_grouping = location_grouping\n",
    "        self.weekly_graphs = {}\n",
    "    \n",
    "    def update_dates(self): \n",
    "        \"\"\"\n",
    "        Updates the start and end date by 7 days\n",
    "        \"\"\" \n",
    "        self.start_date += timedelta(days=7)\n",
    "        self.end_date += timedelta(days=7)\n",
    "\n",
    "    def construct_or_update_tg(self, filter_key='load_count', display_progress=False, display_path_info=False):\n",
    "        \"\"\" \n",
    "        Either (1) creates a network graph given a start and end date \n",
    "            or (2) updates the network graph to the next week's graph \n",
    "\n",
    "        Args: \n",
    "            start_date, end_date: start & end date of the first two weeks of the simulation\n",
    "\n",
    "        Returns: n/a\n",
    "        \"\"\"        \n",
    "        # construct or update cm_finder.network \n",
    "        if not self.cm_network: \n",
    "            self.cm_network = temporalNetwork(self.start_date, self.end_date, display_progress, self.location_grouping, self.origin, self.facility_id, self.intermediate)\n",
    "            self.cm_network.construct_network_graph()\n",
    "            self.cm_finder = CM_Finder(location_grouping=self.location_grouping)\n",
    "        else: \n",
    "            self.cm_network.move_to_next_week()\n",
    "\n",
    "        self.cm_finder.origin_location_list = self.cm_network.origin_location_list\n",
    "        self.cm_finder.network = self.cm_network.network\n",
    "        \n",
    "        # self.cm_finder.group_to_DiGraph(display_progress = display_progress)\n",
    "        self.weekly_graphs[self.end_date] = self.cm_finder.network\n",
    "\n",
    "    def temporal_query(self, start_date, temporal=True, looback = 7, number_of_weeks=None, termination_date = None, \\\n",
    "                    filter_key = \"load_count\", display_progress=False, display_path_info = False): \n",
    "        \"\"\"\n",
    "        Given a start date, run the simulation for number_of_weeks or until termination_date is reached.\n",
    "\n",
    "        Args: \n",
    "            start_date: start date of the first week of the simulation\n",
    "            temporal: if True, run the simulation for every two weeks, if False, run the simulation from start_date until end_date\n",
    "            number_of_weeks: number of weeks to run the simulation for\n",
    "            termination_date: date to stop the simulation\n",
    "            filter_key: key to filter the network on (load_count or path_score)\n",
    "            display_progress: if True, display progress bar\n",
    "            display_path_info: if True, display path info\n",
    "\n",
    "        Returns: a dictionary of simulation result for each week\n",
    "            \n",
    "        Note:\n",
    "        * termination_date: termination date of the entire analysis, when end_date reaches termination_date, the query loop terminates,\n",
    "        * end_date: the end date of the two-week window, will be updated every week\n",
    "\n",
    "        - Once the parameters (location_grouping, origin, facility_id, intermediate) are used to initialise the cm_time class, \n",
    "          they will be used for any further analysis until new initialisation happens.\n",
    "          query_weekly method will only perform analysis, no alterations can be made by calling solely this.\n",
    "\n",
    "        - If temporal=True, --> end_date != termination_date, eventually at the end of simulations, end_date = termination_date\n",
    "            and number_of_weeks is given, end_date = start_date + 13 days for the first simulation, termination_date = start_date + 7 days * number_of_weeks\n",
    "            and termination_date is given, end_date = start_date + 13 days for the first simulation and termination_date=termination_date for the simulation\n",
    "          If temporal=False --> end_date = termination_date \n",
    "            and number_of_weeks is given, end_date = start_date + 7 days * number_of_weeks for the simulation \n",
    "            and termination_date is given, end_date=termination_date for the simulation\n",
    "\n",
    "        * simulation_results: a dictionary with key as the end_date and value as the simulation result\n",
    "        \"\"\"\n",
    "\n",
    "        # create information needed for a new query with the given start_date and number_of_weeks\n",
    "        # possible bug when end_date > termination_date.\n",
    "        if temporal: \n",
    "            self.start_date, self.end_date = start_date, start_date + timedelta(days=looback-1)\n",
    "\n",
    "            if number_of_weeks: termination_date = self.start_date + timedelta(days=7) * number_of_weeks\n",
    "            elif termination_date: termination_date = termination_date\n",
    "            else: raise Exception(\"Neither number of weeks nor termination date was given to set the simulation time period.\")\n",
    "        \n",
    "            # run simulation for every two weeks until termination_date\n",
    "            while self.end_date <= termination_date:    \n",
    "                self.construct_or_update_tg(filter_key=filter_key, \\\n",
    "                                    display_progress=display_progress, display_path_info=display_path_info)\n",
    "                self.update_dates() \n",
    "                \n",
    "        else: \n",
    "            if number_of_weeks: self.start_date, self.end_date = start_date, start_date + timedelta(days=7) * number_of_weeks\n",
    "            elif termination_date: self.start_date, self.end_date = start_date, termination_date\n",
    "            else: raise Exception(\"Neither number of weeks nor termination date was given to set the simulation time period.\")\n",
    "            self.run_single_simulation(filter_key=filter_key, \\\n",
    "                                display_progress=display_progress, display_path_info=display_path_info)\n",
    "\n",
    "        return self.weekly_graphs\n",
    "\n",
    "    def if_edge(node1, node2, curr_graph): \n",
    "        adjacency_matrix = curr_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k1, k2 = list(weekly_graphs.keys())[:2]\n",
    "# weekly_graphs[k1] == weekly_graphs[k2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_time = CM_Time(location_grouping = 'kma') \n",
    "weekly_graphs = cm_time.temporal_query(start_date=datetime(2021,1,1).date(), looback=7, termination_date= datetime(2021,1, 15).date(), \\\n",
    "                    display_progress=False, display_path_info = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_time = CM_Time(location_grouping = 'zip3') \n",
    "weekly_graphs = cm_time.temporal_query(start_date=datetime(2021,1,1).date(), looback=7, termination_date= datetime(2023,6, 1).date(), \\\n",
    "                    display_progress=False, display_path_info = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operator_l2(u, v):\n",
    "    return (u - v) ** 2\n",
    "\n",
    "def operator_sub(u, v):\n",
    "    return (u - v)\n",
    "\n",
    "binary_operator = operator_l2\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def link_examples_to_features(link_examples, transform_node):\n",
    "    return [\n",
    "        operator_l2(transform_node(src), transform_node(dst)) for src, dst in link_examples\n",
    "    ]\n",
    "\n",
    "def link_examples_to_features_sub(link_examples, transform_node):\n",
    "    return [\n",
    "        operator_sub(transform_node(src), transform_node(dst)) for src, dst in link_examples\n",
    "    ]\n",
    "\n",
    "def link_prediction_classifier(max_iter=2000):\n",
    "    lr_clf = LogisticRegressionCV(Cs=10, cv=10, scoring=\"roc_auc\", max_iter=max_iter, penalty=\"l2\") #, solver=\"liblinear\")\n",
    "    return Pipeline(steps=[(\"sc\", StandardScaler()), (\"clf\", lr_clf)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(graph, prediction_window_size=2): \n",
    "    # graph = stellar graph\n",
    "    # identify edges based on dates\n",
    "    edges, weights = np.array(graph.edges(include_edge_weight=True)[0]), np.array(graph.edges(include_edge_weight=True)[1])\n",
    "    lower_lim = sorted(list(set(weights)))[-prediction_window_size]\n",
    "    index_test, index_train = np.where(weights >= lower_lim)[0], np.where(weights < lower_lim)[0]\n",
    "\n",
    "    # create test & train edge sets\n",
    "    test_edges, test_labels = edges[index_test], weights[index_test]\n",
    "    train_edges, train_labels = edges[index_train], weights[index_train]\n",
    "    test_weighted_edges = np.rec.fromarrays([test_edges[:,0], test_edges[:,1], test_labels])\n",
    "    train_weighted_edges = np.rec.fromarrays([train_edges[:,0], train_edges[:,1], train_labels])\n",
    "    # print(set(test_labels), set(train_labels))\n",
    "\n",
    "    # create test and train graph \n",
    "    test_graph,train_graph = nx.MultiDiGraph(), nx.MultiDiGraph()\n",
    "    test_graph.add_weighted_edges_from(test_weighted_edges,weight='time')\n",
    "    train_graph.add_weighted_edges_from(train_weighted_edges,weight='time')\n",
    "    test_graph, train_graph = StellarGraph.from_networkx(test_graph, edge_weight_attr='time', edge_type_attr='directed'), \\\n",
    "                        StellarGraph.from_networkx(train_graph, edge_weight_attr='time', edge_type_attr='directed'), \n",
    "\n",
    "    # create pos & neg edges \n",
    "    edge_splitter_test = EdgeSplitter(test_graph, graph)\n",
    "    graph_test, examples_test, labels_test = edge_splitter_test.train_test_split( #result_graph, [u, v], edge_data_labels (1 or 0)\n",
    "        p=0.1, method=\"global\",\n",
    "    )\n",
    "\n",
    "    ## train graph \n",
    "    edge_splitter_train = EdgeSplitter(train_graph, graph)\n",
    "    graph_train, examples, labels = edge_splitter_train.train_test_split(\n",
    "        p=0.1, method=\"global\"\n",
    "    )\n",
    "    \n",
    "    # (\n",
    "    #     examples_train,\n",
    "    #     examples_model_selection,\n",
    "    #     labels_train,\n",
    "    #     labels_model_selection,\n",
    "    # ) = sklearn.model_selection.train_test_split(examples, labels, train_size=0.75, test_size=0.25)\n",
    "\n",
    "    # concatenate graph_train and train_graph as by time t, we have learned all previous edges up to t \n",
    "    test_graph_e = graph_test.edges(include_edge_weight=True)\n",
    "    test_graph_edges, test_graph_weights= np.array(test_graph_e[0]),np.array(test_graph_e[1])\n",
    "    test_weighted_edges = np.rec.fromarrays([test_graph_edges[:,0], test_graph_edges[:,1], test_graph_weights])\n",
    "    train_graph_e = train_graph.edges(include_edge_weight=True)\n",
    "    train_graph_edges, train_graph_weights= np.array(train_graph_e[0]),np.array(train_graph_e[1])\n",
    "    train_weighted_edges = np.rec.fromarrays([train_graph_edges[:,0], train_graph_edges[:,1], train_graph_weights])\n",
    "    union_graph_test = nx.MultiDiGraph()\n",
    "    union_graph_test.add_weighted_edges_from(test_weighted_edges)\n",
    "    union_graph_test.add_weighted_edges_from(train_weighted_edges)\n",
    "    union_graph_test = StellarGraph.from_networkx(union_graph_test, edge_weight_attr='time', edge_type_attr='directed')\n",
    "\n",
    "    if set(graph_train.edges(include_edge_weight=True)[1]).intersection(set(graph_test.edges(include_edge_weight=True)[1])): \n",
    "        raise(Exception)\n",
    "    return union_graph_test, graph_test, examples_test, labels_test, graph_train, examples, labels #, examples_train,examples_model_selection,labels_train,labels_model_selection,\n",
    "\n",
    "# union_graph_test, graph_test, examples_test, labels_test, graph_train, examples, labels = data_split(graph)\n",
    "# print(len(graph_test.edges()), len(graph_train.edges()))\n",
    "# print(set(graph_train.edges(include_edge_weight=True)[1]).intersection(set(graph_test.edges(include_edge_weight=True)[1])))\n",
    "# graph_test, examples_test, labels_test, graph_train, examples, labels, examples_train,examples_model_selection,labels_train,labels_model_selection, = data_split(graph)\n",
    "\n",
    "def temporal_model(graph, num_walks_per_node=10, walk_length = 10, context_window_size = 2): \n",
    "    num_cw = len(graph.nodes()) * num_walks_per_node * (walk_length - context_window_size + 1)\n",
    "    temporal_rw = TemporalRandomWalk(graph)\n",
    "    temporal_walks = temporal_rw.run(\n",
    "        num_cw=num_cw,\n",
    "        cw_size=context_window_size,\n",
    "        max_walk_length=walk_length,\n",
    "        walk_bias=\"exponential\",\n",
    "    )\n",
    "    \n",
    "    embedding_size = 128\n",
    "    temporal_model = Word2Vec(\n",
    "        temporal_walks,\n",
    "        vector_size=embedding_size,\n",
    "        window=context_window_size,\n",
    "        min_count=0,\n",
    "        sg=1,\n",
    "        workers=2,\n",
    "        epochs=1,)\n",
    "\n",
    "    unseen_node_embedding = np.zeros(embedding_size)\n",
    "\n",
    "    def temporal_embedding(u):\n",
    "        try:\n",
    "            return temporal_model.wv[u]\n",
    "        except KeyError:\n",
    "            return unseen_node_embedding\n",
    "    return temporal_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class TemporalPrediction():\n",
    "    def __init__(self, edge_list, num_walks_per_node=10, walk_length = 10, context_window_size = 2, exper_num = 10):\n",
    "        edge1, edge2, edge3 = edge_list\n",
    "        edge1_tuple, edge2_tuple, edge3_tuple = tuple(edge1), tuple(edge2), tuple(edge3)\n",
    "        # needed for temporal embedding\n",
    "        self.num_walks_per_node = num_walks_per_node\n",
    "        self.walk_length = walk_length\n",
    "        self.context_window_size = context_window_size\n",
    "\n",
    "        # weight optimization\n",
    "        self.target_edges = [edge1, edge2, edge3]\n",
    "        self.naive_regret = 0\n",
    "        self.past_naive_regrets = []\n",
    "\n",
    "        # record purposes \n",
    "        self.test_edges = []\n",
    "        self.test_edge_labels = []\n",
    "        self.past_test_edges = [] \n",
    "        self.past_test_edge_labels =[]\n",
    "        self.curr_false_positive_set = set()\n",
    "        self.test_errors = []\n",
    "        self.predicted_probs = [] \n",
    "        self.target_regrets = {edge1_tuple: [0], edge2_tuple: [0], edge3_tuple: [0]}\n",
    "        self.naive_target_regrets =  {edge1_tuple: [0], edge2_tuple: [0], edge3_tuple: [0]}\n",
    "        self.past_weight_vectors = {edge1_tuple: [[1/exper_num] * exper_num], edge2_tuple:  [[1/exper_num] * exper_num], edge3_tuple:  [[1/exper_num] * exper_num]}\n",
    "\n",
    "    def fit_classifier(self, embeddings, labels):\n",
    "        temporal_clf = link_prediction_classifier()\n",
    "        temporal_clf.fit(embeddings, labels)\n",
    "        return temporal_clf\n",
    "\n",
    "    def evaluate_score(self, clf, link_features, link_labels, threshold= 0.5, return_idces=False):\n",
    "        self.predicted_probs = clf.predict_proba(link_features)\n",
    "        positive_column = list(clf.classes_).index(1)\n",
    "        if return_idces: \n",
    "            false_positive_idces = np.where((link_labels == 0) & (self.predicted_probs[:, positive_column] > threshold))[0]\n",
    "            # true_positive_idces = np.where((link_labels == 1) & (predicted[:, positive_column] > 0.5))[0]\n",
    "            return roc_auc_score(link_labels, self.predicted_probs[:, positive_column]), false_positive_idces #, true_positive_idces\n",
    "        return roc_auc_score(link_labels, self.predicted_probs[:, positive_column])\n",
    "\n",
    "    def update_curr_false_positive(self, new_edge_set): \n",
    "        not_false_positive_anymore = self.curr_false_positive_set.intersection(new_edge_set)\n",
    "        print(f\"Not false positive anymore: {len(not_false_positive_anymore)}\")\n",
    "        if not_false_positive_anymore: \n",
    "            self.naive_regret -= len(not_false_positive_anymore)\n",
    "            self.curr_false_positive_set = self.curr_false_positive_set - not_false_positive_anymore\n",
    "            print(f\"New Reduced Regret: {len(self.curr_false_positive_set)}\")            \n",
    "\n",
    "    def predict_probs(self, graph, compute_naive_test_error=False, display_progress=False): \n",
    "        # update regret by checking if they are in the new graph just given  \n",
    "        self.update_curr_false_positive(set(graph.edges()))\n",
    "\n",
    "        # convert networkx graph to stellargraph & split data\n",
    "        graph = StellarGraph.from_networkx(graph, edge_weight_attr='time', edge_type_attr='directed')\n",
    "        union_graph_test, graph_test, examples_test, labels_test, graph_train, examples, labels = data_split(graph)\n",
    "        self.test_edges, self.test_edge_labels = examples_test, labels_test\n",
    "        self.past_test_edges.append(examples_test)\n",
    "        self.past_test_edge_labels.append(labels_test)\n",
    "\n",
    "        # fit & learn \n",
    "        temporal_embedding = temporal_model(graph_train, \n",
    "                            num_walks_per_node=self.num_walks_per_node, walk_length=self.walk_length, \\\n",
    "                            context_window_size=self.context_window_size)\n",
    "        temporal_link_features = link_examples_to_features(examples, temporal_embedding)\n",
    "        temporal_clf = self.fit_classifier(temporal_link_features, labels) #fit classifier\n",
    "        \n",
    "        # random walk on the union_graph_test to get the embeddings of potential edges  \n",
    "        temporal_embedding = temporal_model(union_graph_test, \\\n",
    "                                            num_walks_per_node=self.num_walks_per_node, walk_length=self.walk_length, \\\n",
    "                                            context_window_size=self.context_window_size)\n",
    "        temporal_link_features_test = link_examples_to_features(examples_test, temporal_embedding)\n",
    "\n",
    "        # compute probability of potential edges         \n",
    "        temporal_score_test, false_positive_idces = self.evaluate_score(temporal_clf, temporal_link_features_test, labels_test, return_idces=True)\n",
    "\n",
    "        if compute_naive_test_error: #calculate test score\n",
    "            self.test_errors.append(temporal_score_test)\n",
    "            if display_progress: print(f\"Temporal Test Score (ROC AUC): {temporal_score_test:.2f}\")\n",
    "            false_positive_edges = [examples_test[i] for i in false_positive_idces]\n",
    "            \n",
    "            # update regret by adding new false_positive_edges \n",
    "            if false_positive_edges: \n",
    "                false_positive_edges = set([tuple(x) for x in false_positive_edges])\n",
    "                print(f\"New False Positives: {len(false_positive_edges - self.curr_false_positive_set)}\")\n",
    "                self.curr_false_positive_set = self.curr_false_positive_set.union(set(false_positive_edges))\n",
    "                self.naive_regret = len(self.curr_false_positive_set)\n",
    "            self.past_naive_regrets.append(self.naive_regret)\n",
    "            print(f\"Current Regret: {self.naive_regret}\")\n",
    "\n",
    "    def optimize_weights_for_edge(self, learning_rate=0.1, leader_thresholds = np.arange(0.1, 1.1, 0.1)): \n",
    "        # let us choose three edges arbitrarily, then update the weights for each edge and see if they differ \n",
    "        # update regret by checking if they are in the new graph --> weighted majority algorithm \n",
    "        for edge in self.target_edges: \n",
    "            is_tested = np.all(self.test_edges == edge, axis=1)\n",
    "            if sum(is_tested):  #['511', '74'] in lp.test_edges\n",
    "                print(f\"{edge} in test edges\")\n",
    "                edge_index = np.where(is_tested)[0][0]\n",
    "                edge_existence = self.test_edge_labels[edge_index]\n",
    "                edge_existence_prob = self.predicted_probs[edge_index,1] \n",
    "                self.weighted_majority_algorithm_per_edge(tuple(edge), edge_existence_prob, edge_existence, learning_rate, leader_thresholds)\n",
    "\n",
    "    def weighted_majority_algorithm_per_edge(self, edge_tuple, edge_existence_prob, edge_existence, learning_rate = 0.1, leader_thresholds = np.arange(0.1, 1.1, 0.1)):\n",
    "        # question: wouldn't it eventually just continuously decrease the weights of higher threshold leaders? \n",
    "        wait_leaders = [1 if edge_existence_prob > threshold else 0 for threshold in leader_thresholds]\n",
    "        not_wait_leaders = [1 if edge_existence_prob < threshold else 0 for threshold in leader_thresholds]\n",
    "        weight_vector = self.past_weight_vectors[edge_tuple][-1]\n",
    "        print(wait_leaders) \n",
    "        print(weight_vector)\n",
    "        print(np.dot(wait_leaders, weight_vector), np.dot(not_wait_leaders, weight_vector))\n",
    "\n",
    "        # make decision based on each leader's recommendation & weight vector\n",
    "        final_wait_decision = True if np.dot(wait_leaders, weight_vector) > np.dot(not_wait_leaders, weight_vector) else False \n",
    "        naive_decision = True if edge_existence_prob > 0.5 else False\n",
    "\n",
    "        # record regret by checking if they are in the new graph\n",
    "        curr_regret = self.target_regrets[edge_tuple][-1]\n",
    "        if final_wait_decision != edge_existence:\n",
    "            if final_wait_decision == 1: curr_regret += 2 #false positive treated most harshly \n",
    "            else: curr_regret += 1 #false negative treated less harshly\n",
    "        self.target_regrets[edge_tuple].append(curr_regret)\n",
    "        \n",
    "        # update naive regret \n",
    "        curr_naive_regret = self.naive_target_regrets[edge_tuple][-1]\n",
    "        if naive_decision != edge_existence: \n",
    "            if final_wait_decision == 1: curr_naive_regret += 2 #false positive treated most harshly \n",
    "            else: curr_naive_regret += 1 #false negative treated less harshly\n",
    "        self.naive_target_regrets[edge_tuple].append(curr_naive_regret)\n",
    "\n",
    "        # update & record weight vector \n",
    "        leader_made_right_decision = [1 if wait_decision == edge_existence  else 0 for wait_decision in wait_leaders]\n",
    "        weight_vector = [weight_i * (1-learning_rate) if not right_wait else weight_i \\\n",
    "                        for right_wait, weight_i in zip(leader_made_right_decision, weight_vector)]\n",
    "        self.past_weight_vectors[edge_tuple].append(weight_vector)\n",
    "\n",
    "    def run_expert_simulation(self, graph, time, learning_rate=0.1, display_progress=False):\n",
    "        self.predict_probs(graph, compute_naive_test_error=True, display_progress=display_progress)\n",
    "        self.optimize_weights_for_edge(learning_rate=learning_rate)\n",
    "        if display_progress: \n",
    "            self.print_current_info(graph.graph['end_date'])\n",
    "\n",
    "    def print_current_info(self, curr_time): \n",
    "        edge1, edge2, edge3 = self.target_regrets.keys()\n",
    "        current_expert_regret = [self.target_regrets[edge1][-1], self.target_regrets[edge2][-1], self.target_regrets[edge3][-1]]\n",
    "        current_naive_regret = [self.naive_target_regrets[edge1][-1], self.naive_target_regrets[edge2][-1], self.naive_target_regrets[edge3][-1]]\n",
    "        \n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Current Week: {curr_time}\")\n",
    "        # print(f\"training error: {np.round(self.train_errors[-1],2)}, test error: {np.round(self.test_errors[-1],2)}\")\n",
    "        print(f\"Current Expert Regret: {current_expert_regret}\")\n",
    "        print(f\"Current Naive Regret: {current_naive_regret}\")\n",
    "        print(f\"Current Traget Edge Weight vectors: {self.past_weight_vectors}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "    def graph_train_test_errors(self, time_range, graph_errors = True, graph_regret = False, title=None): \n",
    "        n = min(len(self.train_errors), len(self.test_errors))\n",
    "        if title: plt.title(title)\n",
    "        if graph_errors: \n",
    "            # plt.plot(time_range[:n], self.train_errors[:n], label='train')\n",
    "            plt.plot(time_range[:n], self.test_errors[:n], label='test')\n",
    "        if graph_regret: \n",
    "            plt.plot(time_range[:n], self.curr_regret[:n], label='regret')\n",
    "        plt.legend() \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. given a route, are we doing this? \n",
    "2. or are we doing this for the entire graph? \n",
    "\n",
    "Since we are making the prediction on all potential edges for a graph, how would we do this..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('104', '799'),\n",
       " ('109', '781'),\n",
       " ('115', '515'),\n",
       " ('118', '820'),\n",
       " ('12', '303'),\n",
       " ('12', '481'),\n",
       " ('12', '787'),\n",
       " ('12', '802'),\n",
       " ('123', '806'),\n",
       " ('125', '527'),\n",
       " ('13', '60'),\n",
       " ('13', '775'),\n",
       " ('13', '809'),\n",
       " ('131', '890'),\n",
       " ('14', '47'),\n",
       " ('14', '66'),\n",
       " ('150', '41'),\n",
       " ('160', '660'),\n",
       " ('170', '532'),\n",
       " ('18', '983'),\n",
       " ('18', '992'),\n",
       " ('180', '47'),\n",
       " ('181', '10'),\n",
       " ('181', '123'),\n",
       " ('181', '155'),\n",
       " ('181', '171'),\n",
       " ('181', '180'),\n",
       " ('181', '27'),\n",
       " ('181', '325'),\n",
       " ('181', '43'),\n",
       " ('181', '469'),\n",
       " ('181', '618'),\n",
       " ('186', '329'),\n",
       " ('191', '880'),\n",
       " ('193', '46'),\n",
       " ('194', '846'),\n",
       " ('20', '201'),\n",
       " ('20', '27'),\n",
       " ('20', '71'),\n",
       " ('201', '769'),\n",
       " ('211', '229'),\n",
       " ('217', '171'),\n",
       " ('226', '759'),\n",
       " ('25', '435'),\n",
       " ('25', '850'),\n",
       " ('25', '853'),\n",
       " ('26', '118'),\n",
       " ('26', '481'),\n",
       " ('275', '613'),\n",
       " ('278', '463'),\n",
       " ('278', '47'),\n",
       " ('28', '201'),\n",
       " ('28', '392'),\n",
       " ('28', '427'),\n",
       " ('28', '721'),\n",
       " ('281', '356'),\n",
       " ('282', '47'),\n",
       " ('286', '297'),\n",
       " ('29', '219'),\n",
       " ('291', '81'),\n",
       " ('295', '175'),\n",
       " ('30', '546'),\n",
       " ('30', '662'),\n",
       " ('300', '12'),\n",
       " ('301', '407'),\n",
       " ('302', '306'),\n",
       " ('302', '514'),\n",
       " ('305', '585'),\n",
       " ('314', '453'),\n",
       " ('32', '527'),\n",
       " ('325', '431'),\n",
       " ('328', '908'),\n",
       " ('33', '843'),\n",
       " ('331', '851'),\n",
       " ('334', '89'),\n",
       " ('335', '900'),\n",
       " ('349', '306'),\n",
       " ('35', '908'),\n",
       " ('352', '386'),\n",
       " ('359', '432'),\n",
       " ('36', '371'),\n",
       " ('36', '395'),\n",
       " ('36', '526'),\n",
       " ('36', '760'),\n",
       " ('365', '26'),\n",
       " ('37', '546'),\n",
       " ('371', '540'),\n",
       " ('372', '39'),\n",
       " ('38', '775'),\n",
       " ('381', '762'),\n",
       " ('386', '604'),\n",
       " ('389', '959'),\n",
       " ('39', '926'),\n",
       " ('391', '591'),\n",
       " ('392', '740'),\n",
       " ('392', '781'),\n",
       " ('398', '957'),\n",
       " ('402', '480'),\n",
       " ('41', '21'),\n",
       " ('41', '326'),\n",
       " ('41', '455'),\n",
       " ('411', '704'),\n",
       " ('42', '184'),\n",
       " ('42', '282'),\n",
       " ('42', '350'),\n",
       " ('42', '437'),\n",
       " ('42', '441'),\n",
       " ('42', '513'),\n",
       " ('42', '554'),\n",
       " ('42', '801'),\n",
       " ('42', '836'),\n",
       " ('432', '46'),\n",
       " ('442', '724'),\n",
       " ('45', '120'),\n",
       " ('450', '760'),\n",
       " ('455', '488'),\n",
       " ('455', '71'),\n",
       " ('458', '511'),\n",
       " ('46', '273'),\n",
       " ('46', '336'),\n",
       " ('46', '385'),\n",
       " ('460', '923'),\n",
       " ('465', '640'),\n",
       " ('47', '125'),\n",
       " ('47', '18'),\n",
       " ('47', '199'),\n",
       " ('47', '407'),\n",
       " ('47', '479'),\n",
       " ('47', '618'),\n",
       " ('47', '64'),\n",
       " ('479', '190'),\n",
       " ('480', '775'),\n",
       " ('494', '489'),\n",
       " ('501', '691'),\n",
       " ('503', '501'),\n",
       " ('503', '801'),\n",
       " ('503', '802'),\n",
       " ('513', '955'),\n",
       " ('514', '172'),\n",
       " ('520', '278'),\n",
       " ('522', '18'),\n",
       " ('524', '44'),\n",
       " ('526', '155'),\n",
       " ('530', '398'),\n",
       " ('535', '23'),\n",
       " ('535', '47'),\n",
       " ('537', '87'),\n",
       " ('538', '626'),\n",
       " ('553', '47'),\n",
       " ('557', '741'),\n",
       " ('563', '125'),\n",
       " ('564', '972'),\n",
       " ('570', '522'),\n",
       " ('571', '29'),\n",
       " ('601', '171'),\n",
       " ('601', '452'),\n",
       " ('601', '531'),\n",
       " ('601', '553'),\n",
       " ('601', '750'),\n",
       " ('601', '970'),\n",
       " ('604', '458'),\n",
       " ('604', '543'),\n",
       " ('604', '841'),\n",
       " ('605', '921'),\n",
       " ('606', '226'),\n",
       " ('610', '118'),\n",
       " ('613', '19'),\n",
       " ('626', '210'),\n",
       " ('626', '43'),\n",
       " ('631', '560'),\n",
       " ('641', '25'),\n",
       " ('647', '456'),\n",
       " ('648', '274'),\n",
       " ('661', '560'),\n",
       " ('668', '650'),\n",
       " ('675', '217'),\n",
       " ('678', '661'),\n",
       " ('681', '648'),\n",
       " ('683', '22'),\n",
       " ('683', '469'),\n",
       " ('685', '150'),\n",
       " ('688', '799'),\n",
       " ('691', '45'),\n",
       " ('70', '74'),\n",
       " ('71', '721'),\n",
       " ('724', '179'),\n",
       " ('729', '302'),\n",
       " ('74', '287'),\n",
       " ('74', '392'),\n",
       " ('74', '500'),\n",
       " ('74', '554'),\n",
       " ('74', '970'),\n",
       " ('740', '286'),\n",
       " ('746', '496'),\n",
       " ('751', '229'),\n",
       " ('751', '293'),\n",
       " ('751', '640'),\n",
       " ('751', '740'),\n",
       " ('752', '41'),\n",
       " ('754', '956'),\n",
       " ('755', '843'),\n",
       " ('756', '607'),\n",
       " ('759', '41'),\n",
       " ('759', '510'),\n",
       " ('760', '146'),\n",
       " ('761', '370'),\n",
       " ('761', '604'),\n",
       " ('761', '752'),\n",
       " ('761', '762'),\n",
       " ('761', '787'),\n",
       " ('761', '820'),\n",
       " ('761', '970'),\n",
       " ('765', '554'),\n",
       " ('769', '539'),\n",
       " ('770', '894'),\n",
       " ('774', '275'),\n",
       " ('782', '352'),\n",
       " ('785', '126'),\n",
       " ('785', '365'),\n",
       " ('787', '559'),\n",
       " ('790', '368'),\n",
       " ('790', '908'),\n",
       " ('80', '370'),\n",
       " ('801', '270'),\n",
       " ('803', '907'),\n",
       " ('805', '142'),\n",
       " ('806', '633'),\n",
       " ('806', '774'),\n",
       " ('81', '326'),\n",
       " ('83', '45'),\n",
       " ('833', '31'),\n",
       " ('841', '681'),\n",
       " ('843', '479'),\n",
       " ('843', '917'),\n",
       " ('846', '809'),\n",
       " ('851', '46'),\n",
       " ('852', '465'),\n",
       " ('853', '668'),\n",
       " ('87', '380'),\n",
       " ('870', '740'),\n",
       " ('879', '411'),\n",
       " ('89', '557'),\n",
       " ('890', '287'),\n",
       " ('900', '12'),\n",
       " ('906', '290'),\n",
       " ('917', '452'),\n",
       " ('917', '907'),\n",
       " ('917', '926'),\n",
       " ('917', '928'),\n",
       " ('921', '802'),\n",
       " ('924', '281'),\n",
       " ('925', '18'),\n",
       " ('928', '291'),\n",
       " ('932', '45'),\n",
       " ('94', '750'),\n",
       " ('94', '985'),\n",
       " ('941', '94'),\n",
       " ('945', '320'),\n",
       " ('946', '641'),\n",
       " ('959', '623'),\n",
       " ('972', '74'),\n",
       " ('983', '64'),\n",
       " ('985', '34'),\n",
       " ('985', '986'),\n",
       " ('986', '728'),\n",
       " ('993', '707')}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common = set([tuple(x) for x in lp.past_test_edges[0]]) \n",
    "for test_edges in lp.past_test_edges[1:]: \n",
    "    test_edge_set = set([tuple(x) for x in test_edges])\n",
    "    common.intersection(test_edge_set)\n",
    "common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not false positive anymore: 0\n",
      "** Sampled 133 positive and 133 negative edges. **\n",
      "** Sampled 336 positive and 336 negative edges. **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/126 [00:18<38:13, 18.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Test Score (ROC AUC): 0.89\n",
      "New False Positives: 7\n",
      "Current Regret: 7\n",
      "--------------------------------------------------\n",
      "Current Week: 2021-01-07\n",
      "Current Expert Regret: [0, 0, 0]\n",
      "Current Naive Regret: [0, 0, 0]\n",
      "Current Traget Edge Weight vectors: {('104', '799'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], ('993', '707'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], ('181', '155'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]}\n",
      "--------------------------------------------------\n",
      "Not false positive anymore: 0\n",
      "** Sampled 125 positive and 125 negative edges. **\n",
      "** Sampled 474 positive and 474 negative edges. **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 2/126 [00:36<37:51, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Test Score (ROC AUC): 0.77\n",
      "New False Positives: 24\n",
      "Current Regret: 31\n",
      "['181', '155'] in test edges\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "0.8999999999999999 0.1\n",
      "--------------------------------------------------\n",
      "Current Week: 2021-01-14\n",
      "Current Expert Regret: [0, 0, 0]\n",
      "Current Naive Regret: [0, 0, 0]\n",
      "Current Traget Edge Weight vectors: {('104', '799'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], ('993', '707'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], ('181', '155'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.09000000000000001]]}\n",
      "--------------------------------------------------\n",
      "Not false positive anymore: 0\n",
      "** Sampled 271 positive and 271 negative edges. **\n",
      "** Sampled 352 positive and 352 negative edges. **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 3/126 [00:55<38:24, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Test Score (ROC AUC): 0.89\n",
      "New False Positives: 58\n",
      "Current Regret: 89\n",
      "--------------------------------------------------\n",
      "Current Week: 2021-01-21\n",
      "Current Expert Regret: [0, 0, 0]\n",
      "Current Naive Regret: [0, 0, 0]\n",
      "Current Traget Edge Weight vectors: {('104', '799'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], ('993', '707'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], ('181', '155'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.09000000000000001]]}\n",
      "--------------------------------------------------\n",
      "Not false positive anymore: 0\n",
      "** Sampled 256 positive and 256 negative edges. **\n",
      "** Sampled 355 positive and 355 negative edges. **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 4/126 [01:14<37:57, 18.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Test Score (ROC AUC): 0.85\n",
      "New False Positives: 49\n",
      "Current Regret: 138\n",
      "--------------------------------------------------\n",
      "Current Week: 2021-01-28\n",
      "Current Expert Regret: [0, 0, 0]\n",
      "Current Naive Regret: [0, 0, 0]\n",
      "Current Traget Edge Weight vectors: {('104', '799'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], ('993', '707'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], ('181', '155'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.09000000000000001]]}\n",
      "--------------------------------------------------\n",
      "Not false positive anymore: 3\n",
      "New Reduced Regret: 135\n",
      "** Sampled 247 positive and 247 negative edges. **\n",
      "** Sampled 328 positive and 328 negative edges. **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 5/126 [01:32<37:14, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Test Score (ROC AUC): 0.90\n",
      "New False Positives: 18\n",
      "Current Regret: 153\n",
      "--------------------------------------------------\n",
      "Current Week: 2021-02-04\n",
      "Current Expert Regret: [0, 0, 0]\n",
      "Current Naive Regret: [0, 0, 0]\n",
      "Current Traget Edge Weight vectors: {('104', '799'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], ('993', '707'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], ('181', '155'): [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.09000000000000001]]}\n",
      "--------------------------------------------------\n",
      "Not false positive anymore: 1\n",
      "New Reduced Regret: 152\n",
      "** Sampled 128 positive and 128 negative edges. **\n",
      "** Sampled 492 positive and 492 negative edges. **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 5/126 [01:49<44:08, 21.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# for t in range(N-lookback, N): \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m time, graph \u001b[39min\u001b[39;00m tqdm(weekly_graphs\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m     14\u001b[0m     \u001b[39m# compute probability \u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     lp\u001b[39m.\u001b[39;49mrun_expert_simulation(graph, time, display_progress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     17\u001b[0m     \u001b[39m# play pt and observe loss vector t  [0, 1]N\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m     \u001b[39m# when the correct answer is received, penalize each mistaken expert by multiplying its weight by learning_rate\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m# update Lt = Lt1 + t\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[78], line 137\u001b[0m, in \u001b[0;36mTemporalPrediction.run_expert_simulation\u001b[0;34m(self, graph, time, learning_rate, display_progress)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_expert_simulation\u001b[39m(\u001b[39mself\u001b[39m, graph, time, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, display_progress\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_probs(graph, compute_naive_test_error\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, display_progress\u001b[39m=\u001b[39;49mdisplay_progress)\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimize_weights_for_edge(learning_rate\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m    139\u001b[0m     \u001b[39mif\u001b[39;00m display_progress: \n",
      "Cell \u001b[0;32mIn[78], line 69\u001b[0m, in \u001b[0;36mTemporalPrediction.predict_probs\u001b[0;34m(self, graph, compute_naive_test_error, display_progress)\u001b[0m\n\u001b[1;32m     66\u001b[0m temporal_clf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_classifier(temporal_link_features, labels) \u001b[39m#fit classifier\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m# random walk on the union_graph_test to get the embeddings of potential edges  \u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m temporal_embedding \u001b[39m=\u001b[39m temporal_model(union_graph_test, \\\n\u001b[1;32m     70\u001b[0m                                     num_walks_per_node\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_walks_per_node, walk_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwalk_length, \\\n\u001b[1;32m     71\u001b[0m                                     context_window_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext_window_size)\n\u001b[1;32m     72\u001b[0m temporal_link_features_test \u001b[39m=\u001b[39m link_examples_to_features(examples_test, temporal_embedding)\n\u001b[1;32m     74\u001b[0m \u001b[39m# compute probability of potential edges         \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 65\u001b[0m, in \u001b[0;36mtemporal_model\u001b[0;34m(graph, num_walks_per_node, walk_length, context_window_size)\u001b[0m\n\u001b[1;32m     63\u001b[0m num_cw \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(graph\u001b[39m.\u001b[39mnodes()) \u001b[39m*\u001b[39m num_walks_per_node \u001b[39m*\u001b[39m (walk_length \u001b[39m-\u001b[39m context_window_size \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m temporal_rw \u001b[39m=\u001b[39m TemporalRandomWalk(graph)\n\u001b[0;32m---> 65\u001b[0m temporal_walks \u001b[39m=\u001b[39m temporal_rw\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     66\u001b[0m     num_cw\u001b[39m=\u001b[39;49mnum_cw,\n\u001b[1;32m     67\u001b[0m     cw_size\u001b[39m=\u001b[39;49mcontext_window_size,\n\u001b[1;32m     68\u001b[0m     max_walk_length\u001b[39m=\u001b[39;49mwalk_length,\n\u001b[1;32m     69\u001b[0m     walk_bias\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mexponential\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     72\u001b[0m embedding_size \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[1;32m     73\u001b[0m temporal_model \u001b[39m=\u001b[39m Word2Vec(\n\u001b[1;32m     74\u001b[0m     temporal_walks,\n\u001b[1;32m     75\u001b[0m     vector_size\u001b[39m=\u001b[39membedding_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     80\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,)\n",
      "File \u001b[0;32m~/Documents/GitHub/Temporal-Link-Prediction/venv/lib/python3.8/site-packages/stellargraph/data/explorer.py:1116\u001b[0m, in \u001b[0;36mTemporalRandomWalk.run\u001b[0;34m(self, num_cw, cw_size, max_walk_length, initial_edge_bias, walk_bias, p_walk_success_threshold, seed)\u001b[0m\n\u001b[1;32m   1112\u001b[0m t \u001b[39m=\u001b[39m times[first_edge_index]\n\u001b[1;32m   1114\u001b[0m remaining_length \u001b[39m=\u001b[39m num_cw \u001b[39m-\u001b[39m num_cw_curr \u001b[39m+\u001b[39m cw_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1116\u001b[0m walk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_walk(\n\u001b[1;32m   1117\u001b[0m     src, dst, t, \u001b[39mmin\u001b[39;49m(max_walk_length, remaining_length), walk_bias, np_rs\n\u001b[1;32m   1118\u001b[0m )\n\u001b[1;32m   1119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(walk) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cw_size:\n\u001b[1;32m   1120\u001b[0m     walks\u001b[39m.\u001b[39mappend(walk)\n",
      "File \u001b[0;32m~/Documents/GitHub/Temporal-Link-Prediction/venv/lib/python3.8/site-packages/stellargraph/data/explorer.py:1183\u001b[0m, in \u001b[0;36mTemporalRandomWalk._walk\u001b[0;34m(self, src, dst, t, length, bias_type, np_rs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m node, time \u001b[39m=\u001b[39m dst, t\n\u001b[1;32m   1182\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(length \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[0;32m-> 1183\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(node, time\u001b[39m=\u001b[39;49mtime, bias_type\u001b[39m=\u001b[39;49mbias_type, np_rs\u001b[39m=\u001b[39;49mnp_rs)\n\u001b[1;32m   1185\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m         node, time \u001b[39m=\u001b[39m result\n",
      "File \u001b[0;32m~/Documents/GitHub/Temporal-Link-Prediction/venv/lib/python3.8/site-packages/stellargraph/data/explorer.py:1164\u001b[0m, in \u001b[0;36mTemporalRandomWalk._step\u001b[0;34m(self, node, time, bias_type, np_rs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_step\u001b[39m(\u001b[39mself\u001b[39m, node, time, bias_type, np_rs):\n\u001b[1;32m   1160\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[39m    Perform 1 temporal step from a node. Returns None if a dead-end is reached.\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m \n\u001b[1;32m   1163\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m     neighbours, times \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph\u001b[39m.\u001b[39;49mneighbor_arrays(node, include_edge_weight\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1165\u001b[0m     neighbours \u001b[39m=\u001b[39m neighbours[times \u001b[39m>\u001b[39m time]\n\u001b[1;32m   1166\u001b[0m     times \u001b[39m=\u001b[39m times[times \u001b[39m>\u001b[39m time]\n",
      "File \u001b[0;32m~/Documents/GitHub/Temporal-Link-Prediction/venv/lib/python3.8/site-packages/stellargraph/core/graph.py:790\u001b[0m, in \u001b[0;36mStellarGraph.neighbor_arrays\u001b[0;34m(self, node, include_edge_weight, edge_types, use_ilocs)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[39mObtains the collection of neighbouring nodes connected to the given node\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[39mas an array of node_ids. If `include_edge_weight` edge is `True` then\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39m    of edge weights is also returned in a tuple `(neighbor_array, edge_weight_array)`\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_ilocs:\n\u001b[0;32m--> 790\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_nodes\u001b[39m.\u001b[39;49mids\u001b[39m.\u001b[39;49mto_iloc([node])[\u001b[39m0\u001b[39m]\n\u001b[1;32m    792\u001b[0m edge_ilocs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edges\u001b[39m.\u001b[39medge_ilocs(node, ins\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, outs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    793\u001b[0m source \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edges\u001b[39m.\u001b[39msources[edge_ilocs]\n",
      "File \u001b[0;32m~/Documents/GitHub/Temporal-Link-Prediction/venv/lib/python3.8/site-packages/stellargraph/core/element_data.py:95\u001b[0m, in \u001b[0;36mExternalIdIndex.to_iloc\u001b[0;34m(self, ids, smaller_type, strict)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_iloc\u001b[39m(\u001b[39mself\u001b[39m, ids, smaller_type\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     82\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m    Convert external IDs ``ids`` to integer locations.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m        smaller_type is False)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     internal_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index\u001b[39m.\u001b[39;49mget_indexer(ids)\n\u001b[1;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m strict:\n\u001b[1;32m     97\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire_valid(ids, internal_ids)\n",
      "File \u001b[0;32m~/Documents/GitHub/Temporal-Link-Prediction/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:3789\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3785\u001b[0m \u001b[39mif\u001b[39;00m is_dtype_equal(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, target\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mequals(target):\n\u001b[1;32m   3786\u001b[0m     \u001b[39m# Only call equals if we have same dtype to avoid inference/casting\u001b[39;00m\n\u001b[1;32m   3787\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(target), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp)\n\u001b[0;32m-> 3789\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dtype_equal(\n\u001b[1;32m   3790\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, target\u001b[39m.\u001b[39;49mdtype\n\u001b[1;32m   3791\u001b[0m ) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_partial_index(target):\n\u001b[1;32m   3792\u001b[0m     \u001b[39m# _should_partial_index e.g. IntervalIndex with numeric scalars\u001b[39;00m\n\u001b[1;32m   3793\u001b[0m     \u001b[39m#  that can be matched to Interval scalars.\u001b[39;00m\n\u001b[1;32m   3794\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_find_common_type_compat(target)\n\u001b[1;32m   3796\u001b[0m     this \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Temporal-Link-Prediction/venv/lib/python3.8/site-packages/pandas/core/dtypes/common.py:551\u001b[0m, in \u001b[0;36mis_dtype_equal\u001b[0;34m(source, target)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    548\u001b[0m     \u001b[39mreturn\u001b[39;00m _is_dtype(arr_or_dtype, condition)\n\u001b[0;32m--> 551\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_dtype_equal\u001b[39m(source, target) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    552\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[39m    Check if two dtypes are equal.\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    578\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(target, \u001b[39mstr\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# follow the leader \n",
    "# N, lookback = 136, 1\n",
    "# leader_num, learning_rate = 10, 0.1\n",
    "# leader_thresholds, weight_vector, loss_vector = np.arange(0.1, 1.1, 1/leader_num), [1/leader_num]*leader_num, [0] * leader_num\n",
    "\n",
    "# Input: learning rate  > 0\n",
    "# Initialize: let L0  RN be the all-zero vector\n",
    "edge_list = [['125', '527'], \n",
    "       ['13', '60'],  \n",
    "       ['181', '155']]\n",
    "lp = TemporalPrediction(edge_list)\n",
    "# for t in range(N-lookback, N): \n",
    "for time, graph in tqdm(weekly_graphs.items()):\n",
    "    # compute probability \n",
    "    lp.run_expert_simulation(graph, time, display_progress=True)\n",
    "    \n",
    "    # play pt and observe loss vector t  [0, 1]N\n",
    "\n",
    "    # when the correct answer is received, penalize each mistaken expert by multiplying its weight by learning_rate\n",
    "    # update Lt = Lt1 + t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = [np.array(['273', '604'], dtype=object), np.array(['664', '973'], dtype=object), np.array(['142', '806'], dtype=object), np.array(['981', '475'], dtype=object), np.array(['744', '20'], dtype=object), np.array(['28', '115'], dtype=object), np.array(['18', '268'], dtype=object)]\n",
    "# print(test)\n",
    "# print(set([tuple(x) for x in test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1, v2 = np.array([1,2,3]) , np.array([1,2,2])\n",
    "np.dot(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
