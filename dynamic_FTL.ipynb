{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 23:47:21.125832: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "from utils import * \n",
    "import stellargraph as sg\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import EdgeSplitter, BiasedRandomWalk, TemporalRandomWalk\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "from math import isclose\n",
    "from sklearn.decomposition import PCA\n",
    "import multiprocessing\n",
    "import sklearn.model_selection \n",
    "from gensim.models import Word2Vec\n",
    "# python3 -m pip install tqdm seaborn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM_Time() & Weekly Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class temporalNetwork(): \n",
    "    def __init__(self, start_date, end_date, display_progress=False, location_grouping='kma', origin=None, facility_id=None, intermediate=None):\n",
    "        \"\"\" \n",
    "        Note: \n",
    "            start_date and end_date should be both None as they are used as a signal to \n",
    "            CM_Time's run_simulation to whether construct a new graph or update the graph with new information \n",
    "        \"\"\"\n",
    "        self.display_progress = display_progress\n",
    "        self.start_date=start_date\n",
    "        self.end_date=end_date\n",
    "        self.network=None\n",
    "        self.origin_location_list=None\n",
    "        self.location_grouping=location_grouping\n",
    "        self.inbound_data = None \n",
    "        self.outbound_data = None\n",
    "\n",
    "    def construct_network_graph(self):\n",
    "        \"\"\" \n",
    "        Given an inbound and outbound dataframe, construct a network graph and stores it in the class variable self.network\n",
    "        Args: \n",
    "            an_inbound_df (pd.DataFrame): inbound dataframe\n",
    "            an_outbound_df (pd.DataFrame): outbound dataframe\n",
    "            start_date (datetime): start date of the network graph\n",
    "            end_date (datetime): end date of the network graph\n",
    "            location_grouping(string): 'kma' or 'zip3'\n",
    "        \"\"\"\n",
    "        # pull data & construct an empty multiDiGraph\n",
    "\n",
    "        date = self.start_date.strftime(\"%Y-%m-%d\") + \"_\" + self.end_date.strftime(\"%Y-%m-%d\")\n",
    "        an_inbound_df, an_outbound_df = pd.read_csv(f\"../inbound_data/inbound_data_{date}.csv\"), pd.read_csv(f\"../outbound_data/outbound_data_{date}.csv\")\n",
    "        an_inbound_df.load_date, an_outbound_df.load_date = pd.to_datetime(an_inbound_df.load_date), pd.to_datetime(an_outbound_df.load_date)\n",
    "        for colin, colout in zip(an_inbound_df.columns, an_outbound_df.columns): \n",
    "            if colin not in [\"total_loads\", \"load_date\"]: \n",
    "                an_inbound_df[colin] = an_inbound_df[colin].astype(str)\n",
    "            if colout not in [\"total_loads\", \"load_date\"]:\n",
    "                an_outbound_df[colout] = an_outbound_df[colout].astype(str)\n",
    "\n",
    "        network_graph = nx.MultiDiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # network_graph = nx.DiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # idf, odf = an_inbound_df.copy(), an_outbound_df.copy()\n",
    "\n",
    "        # idf.to_csv(f\"data/inbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "        # odf.to_csv(f\"data/outbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "\n",
    "        # add nodes & edges \n",
    "        node_1 = f\"origin_{self.location_grouping}_id\"\n",
    "        node_2 = f\"facility_{self.location_grouping}_id\"\n",
    "        node_3 = f\"destination_{self.location_grouping}_id\"\n",
    "\n",
    "        network_graph = add_nodes_given_df(network_graph, an_inbound_df, [node_1, 'facility_id']) \n",
    "        network_graph = add_nodes_given_df(network_graph, an_outbound_df, ['facility_id', node_2, node_3]) \n",
    "\n",
    "        network_graph = add_edges_given_graph(network_graph, an_inbound_df, an_outbound_df, self.location_grouping)\n",
    "        \n",
    "        # update the variables \n",
    "        self.network = network_graph\n",
    "        self.origin_location_list = an_inbound_df[f'origin_{self.location_grouping}_id'].unique()\n",
    "        self.inbound_data, self.outbound_data = an_inbound_df, an_outbound_df\n",
    "        if self.display_progress: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\") \n",
    "        \n",
    "    def move_to_next_week(self): \n",
    "        \"\"\" \n",
    "        Given the new week's inbound and outbound dataframes, \n",
    "        update self.network graph, self.start_date, and self.end_date to a week after current start date and end date \n",
    "        \n",
    "        Args:\n",
    "            next_inbound_df (pd.DataFrame): new week's inbound dataframe\n",
    "            next_outbound_df (pd.DataFrame): new week's outbound dataframe\n",
    "            display_progress (boolean): whether to display the progress of the function or not\n",
    "        \"\"\"\n",
    "        # update the dates, pull new week's data, & store some informations\n",
    "        self.start_date, self.end_date = self.start_date + timedelta(days=7), self.end_date + timedelta(days=7)\n",
    "        date = self.start_date.strftime(\"%Y-%m-%d\") + \"_\" + self.end_date.strftime(\"%Y-%m-%d\")\n",
    "        next_inbound_df, next_outbound_df = pd.read_csv(f\"../inbound_data/inbound_data_{date}.csv\"), pd.read_csv(f\"../outbound_data/outbound_data_{date}.csv\")\n",
    "        next_inbound_df.load_date, next_outbound_df.load_date = pd.to_datetime(next_inbound_df.load_date), pd.to_datetime(next_outbound_df.load_date)\n",
    "        for colin, colout in zip(next_inbound_df.columns, next_outbound_df.columns): \n",
    "            if colin not in [\"total_loads\", \"load_date\"]: \n",
    "                next_inbound_df[colin] = next_inbound_df[colin].astype(str)\n",
    "            if colout not in [\"total_loads\", \"load_date\"]:\n",
    "                next_outbound_df[colout] = next_outbound_df[colout].astype(str)\n",
    "\n",
    "        # next_inbound_df.to_csv(f\"data/inbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "        # next_outbound_df.to_csv(f\"data/outbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "\n",
    "        ## if not len(next_inbound_df) and not len(next_outbound_df): \n",
    "        curr_nodes, curr_edges = set(self.network.nodes()), list(self.network.edges(data=True, keys=True)) # needed for efficient removal of nodes & edges\n",
    "        \n",
    "        node_1 = f\"origin_{self.location_grouping}_id\"\n",
    "        node_2 = f\"facility_{self.location_grouping}_id\"\n",
    "        node_3 = f\"destination_{self.location_grouping}_id\"\n",
    "\n",
    "        # or statement is used to check if either inbound or outbound df exists, \n",
    "        # so we can add any final delivery data to the graph\n",
    "        if len(next_inbound_df) or len(next_outbound_df): \n",
    "            # add new nodes \n",
    "            new_nodes = set()\n",
    "            if len(next_inbound_df): new_nodes = set(next_inbound_df[[node_1, 'facility_id']].to_numpy().flatten())\n",
    "            if len(next_outbound_df): new_nodes.union(set(next_outbound_df[[node_2, node_3, 'facility_id']].to_numpy().flatten()))\n",
    "            new_nodes = new_nodes.difference(curr_nodes)\n",
    "            self.network.add_nodes_from(new_nodes)\n",
    "\n",
    "            # add new edges\n",
    "            self.network = add_edges_given_graph(self.network, next_inbound_df, next_outbound_df, self.location_grouping)\n",
    "\n",
    "        # remove old edges\n",
    "        past_edges = []\n",
    "        for edge in curr_edges: \n",
    "            if edge[2] < to_integer(self.start_date): \n",
    "                self.network.remove_edge(edge[0], edge[1], edge[2])\n",
    "                past_edges.append(edge)\n",
    "\n",
    "        # remove isolated nodes\n",
    "        isolated_nodes = list(nx.isolates(self.network))\n",
    "        self.network.remove_nodes_from(isolated_nodes)\n",
    "        \n",
    "        # update variables\n",
    "        self.network.graph['start_date']= self.start_date\n",
    "        self.network.graph['end_date']= self.end_date\n",
    "        self.inbound_data, self.outbound_data = next_inbound_df, next_outbound_df\n",
    "        if self.location_grouping == 'kma': self.origin_location_list = list(set([x.split(\" \")[0] for x in self.network.nodes() if len(x) == 6])) \n",
    "        else: self.origin_location_list = list(set([x.split(\" \")[0] for x in self.network.nodes() if len(x) == 3]))\n",
    "        \n",
    "        if self.display_progress: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\")  \n",
    "\n",
    "    def print_network_information(self, given_network, print_network_time=False): \n",
    "        \"\"\"\n",
    "        Given a network, print out the information of the network\n",
    "        Args: \n",
    "            given_network (nx.MultiDiGraph): a network graph\n",
    "        Returns: N/A\n",
    "        \"\"\"\n",
    "        print(\"---------------------------------------------------------------------------------------------\") \n",
    "        print(given_network)\n",
    "        print(f\"Is the given network a DAG for load_network?: {nx.is_directed_acyclic_graph(given_network)}\")\n",
    "        print(f\"Number of self loops: {nx.number_of_selfloops(given_network)}\")\n",
    "        if print_network_time: print(f\"Current time of the graph: {given_network.graph['start_date']} to {given_network.graph['end_date']}\")\n",
    "        else: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_Finder():\n",
    "    def __init__(self, location_grouping='kma', origin_location_list=None, network = None):\n",
    "        self.network=network\n",
    "        self.processed_network=None\n",
    "        self.processed=False \n",
    "        self.origin_location_list=origin_location_list\n",
    "        self.match_failure = None\n",
    "        self.remove_failure = 0\n",
    "        self.location_grouping=location_grouping\n",
    "        \n",
    "    def group_to_DiGraph(self, display_progress = False):\n",
    "        \"\"\"\n",
    "        #TODO: explain why aggregate_faciility_zip then group_to_DiGraph (kma-> facility_zip -> kma to kma->kma->kma, aggregate to faciliy KMA)\n",
    "        Assuming that self.network is constructed, \n",
    "        sums the edge weights for edges with the same nodes in self.network variable and \n",
    "        stores the new graph with aggregated edges in self.processed_network variable and returns False if successful \n",
    "\n",
    "        Args:\n",
    "            display_progress (boolean): whether to display the progress of the function or not\n",
    "\n",
    "        NOTE) disregards temporal factor \n",
    "        \"\"\"\n",
    "        if not self.network: \n",
    "            print(\"Please construct the network first\")\n",
    "            return None \n",
    "            \n",
    "        new_name = self.network.name + \" reduced\"\n",
    "        self.processed_network = nx.DiGraph(name=new_name)\n",
    "        self.processed_network.add_nodes_from(self.network)\n",
    "\n",
    "        if display_progress: print(\"Aggregating nodes by KMA...\")\n",
    "        for n1, n2 in self.network.edges():\n",
    "            sum = 0 \n",
    "            for inner_dict in self.network.get_edge_data(n1, n2).values(): \n",
    "                sum += inner_dict['capacity']\n",
    "            self.processed_network.add_edge(n1, n2, capacity = sum)\n",
    "        \n",
    "        nx.set_edge_attributes(self.processed_network, to_integer(self.network.graph['end_date']), 'time')\n",
    "        self.processed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_Time(): \n",
    "    def __init__(self, location_grouping='kma', origin=None, facility_id=None, intermediate=None): \n",
    "        self.start_date=None\n",
    "        self.end_date=None\n",
    "        self.cm_network = None   \n",
    "        self.cm_finder = None\n",
    "        self.origin=origin\n",
    "        self.facility_id=facility_id\n",
    "        self.intermediate=intermediate\n",
    "        self.location_grouping = location_grouping\n",
    "        self.weekly_graphs = {}\n",
    "    \n",
    "    def update_dates(self): \n",
    "        \"\"\"\n",
    "        Updates the start and end date by 7 days\n",
    "        \"\"\" \n",
    "        self.start_date += timedelta(days=7)\n",
    "        self.end_date += timedelta(days=7)\n",
    "\n",
    "    def construct_or_update_tg(self, filter_key='load_count', display_progress=False, display_path_info=False):\n",
    "        \"\"\" \n",
    "        Either (1) creates a network graph given a start and end date \n",
    "            or (2) updates the network graph to the next week's graph \n",
    "\n",
    "        Args: \n",
    "            start_date, end_date: start & end date of the first two weeks of the simulation\n",
    "\n",
    "        Returns: n/a\n",
    "        \"\"\"        \n",
    "        # construct or update cm_finder.network \n",
    "        if not self.cm_network: \n",
    "            self.cm_network = temporalNetwork(self.start_date, self.end_date, display_progress, self.location_grouping, self.origin, self.facility_id, self.intermediate)\n",
    "            self.cm_network.construct_network_graph()\n",
    "            self.cm_finder = CM_Finder(location_grouping=self.location_grouping)\n",
    "        else: \n",
    "            self.cm_network.move_to_next_week()\n",
    "\n",
    "        self.cm_finder.origin_location_list = self.cm_network.origin_location_list\n",
    "        self.cm_finder.network = self.cm_network.network\n",
    "        \n",
    "        # self.cm_finder.group_to_DiGraph(display_progress = display_progress)\n",
    "        self.weekly_graphs[self.end_date] = self.cm_finder.network\n",
    "\n",
    "    def temporal_query(self, start_date, temporal=True, looback = 7, number_of_weeks=None, termination_date = None, \\\n",
    "                    filter_key = \"load_count\", display_progress=False, display_path_info = False): \n",
    "        \"\"\"\n",
    "        Given a start date, run the simulation for number_of_weeks or until termination_date is reached.\n",
    "\n",
    "        Args: \n",
    "            start_date: start date of the first week of the simulation\n",
    "            temporal: if True, run the simulation for every two weeks, if False, run the simulation from start_date until end_date\n",
    "            number_of_weeks: number of weeks to run the simulation for\n",
    "            termination_date: date to stop the simulation\n",
    "            filter_key: key to filter the network on (load_count or path_score)\n",
    "            display_progress: if True, display progress bar\n",
    "            display_path_info: if True, display path info\n",
    "\n",
    "        Returns: a dictionary of simulation result for each week\n",
    "            \n",
    "        Note:\n",
    "        * termination_date: termination date of the entire analysis, when end_date reaches termination_date, the query loop terminates,\n",
    "        * end_date: the end date of the two-week window, will be updated every week\n",
    "\n",
    "        - Once the parameters (location_grouping, origin, facility_id, intermediate) are used to initialise the cm_time class, \n",
    "          they will be used for any further analysis until new initialisation happens.\n",
    "          query_weekly method will only perform analysis, no alterations can be made by calling solely this.\n",
    "\n",
    "        - If temporal=True, --> end_date != termination_date, eventually at the end of simulations, end_date = termination_date\n",
    "            and number_of_weeks is given, end_date = start_date + 13 days for the first simulation, termination_date = start_date + 7 days * number_of_weeks\n",
    "            and termination_date is given, end_date = start_date + 13 days for the first simulation and termination_date=termination_date for the simulation\n",
    "          If temporal=False --> end_date = termination_date \n",
    "            and number_of_weeks is given, end_date = start_date + 7 days * number_of_weeks for the simulation \n",
    "            and termination_date is given, end_date=termination_date for the simulation\n",
    "\n",
    "        * simulation_results: a dictionary with key as the end_date and value as the simulation result\n",
    "        \"\"\"\n",
    "\n",
    "        # create information needed for a new query with the given start_date and number_of_weeks\n",
    "        # possible bug when end_date > termination_date.\n",
    "        if temporal: \n",
    "            self.start_date, self.end_date = start_date, start_date + timedelta(days=looback-1)\n",
    "\n",
    "            if number_of_weeks: termination_date = self.start_date + timedelta(days=7) * number_of_weeks\n",
    "            elif termination_date: termination_date = termination_date\n",
    "            else: raise Exception(\"Neither number of weeks nor termination date was given to set the simulation time period.\")\n",
    "        \n",
    "            # run simulation for every two weeks until termination_date\n",
    "            while self.end_date <= termination_date:    \n",
    "                self.construct_or_update_tg(filter_key=filter_key, \\\n",
    "                                    display_progress=display_progress, display_path_info=display_path_info)\n",
    "                self.update_dates() \n",
    "                \n",
    "        else: \n",
    "            if number_of_weeks: self.start_date, self.end_date = start_date, start_date + timedelta(days=7) * number_of_weeks\n",
    "            elif termination_date: self.start_date, self.end_date = start_date, termination_date\n",
    "            else: raise Exception(\"Neither number of weeks nor termination date was given to set the simulation time period.\")\n",
    "            self.run_single_simulation(filter_key=filter_key, \\\n",
    "                                display_progress=display_progress, display_path_info=display_path_info)\n",
    "\n",
    "        return self.weekly_graphs\n",
    "\n",
    "    def if_edge(node1, node2, curr_graph): \n",
    "        adjacency_matrix = curr_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_time = CM_Time(location_grouping = 'zip3') \n",
    "weekly_graphs = cm_time.temporal_query(start_date=datetime(2021,1,1).date(), looback=7, termination_date= datetime(2023,6, 1).date(), \\\n",
    "                    display_progress=False, display_path_info = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operator_l2(u, v):\n",
    "    return (u - v) ** 2\n",
    "\n",
    "def operator_sub(u, v):\n",
    "    return (u - v)\n",
    "\n",
    "binary_operator = operator_l2\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def link_examples_to_features(link_examples, transform_node):\n",
    "    return [\n",
    "        operator_l2(transform_node(src), transform_node(dst)) for src, dst in link_examples\n",
    "    ]\n",
    "\n",
    "def link_examples_to_features_sub(link_examples, transform_node):\n",
    "    return [\n",
    "        operator_sub(transform_node(src), transform_node(dst)) for src, dst in link_examples\n",
    "    ]\n",
    "\n",
    "def link_prediction_classifier(max_iter=2000):\n",
    "    lr_clf = LogisticRegressionCV(Cs=10, cv=10, scoring=\"roc_auc\", max_iter=max_iter, penalty=\"l2\") #, solver=\"liblinear\")\n",
    "    return Pipeline(steps=[(\"sc\", StandardScaler()), (\"clf\", lr_clf)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(graph, prediction_window_size=2): \n",
    "    # graph = stellar graph\n",
    "    # identify edges based on dates\n",
    "    edges, weights = np.array(graph.edges(include_edge_weight=True)[0]), np.array(graph.edges(include_edge_weight=True)[1])\n",
    "    lower_lim = sorted(list(set(weights)))[-prediction_window_size]\n",
    "    index_test, index_train = np.where(weights >= lower_lim)[0], np.where(weights < lower_lim)[0]\n",
    "\n",
    "    # create test & train edge sets\n",
    "    test_edges, test_labels = edges[index_test], weights[index_test]\n",
    "    train_edges, train_labels = edges[index_train], weights[index_train]\n",
    "    test_weighted_edges = np.rec.fromarrays([test_edges[:,0], test_edges[:,1], test_labels])\n",
    "    train_weighted_edges = np.rec.fromarrays([train_edges[:,0], train_edges[:,1], train_labels])\n",
    "    # print(set(test_labels), set(train_labels))\n",
    "\n",
    "    # create test and train graph \n",
    "    test_graph,train_graph = nx.MultiDiGraph(), nx.MultiDiGraph()\n",
    "    test_graph.add_weighted_edges_from(test_weighted_edges,weight='time')\n",
    "    train_graph.add_weighted_edges_from(train_weighted_edges,weight='time')\n",
    "    test_graph, train_graph = StellarGraph.from_networkx(test_graph, edge_weight_attr='time', edge_type_attr='directed'), \\\n",
    "                        StellarGraph.from_networkx(train_graph, edge_weight_attr='time', edge_type_attr='directed'), \n",
    "\n",
    "    # create pos & neg edges \n",
    "    edge_splitter_test = EdgeSplitter(test_graph, graph)\n",
    "    graph_test, examples_test, labels_test = edge_splitter_test.train_test_split( #result_graph, [u, v], edge_data_labels (1 or 0)\n",
    "        p=0.1, method=\"global\",\n",
    "    )\n",
    "\n",
    "    ## train graph \n",
    "    edge_splitter_train = EdgeSplitter(train_graph, graph)\n",
    "    graph_train, examples, labels = edge_splitter_train.train_test_split(\n",
    "        p=0.1, method=\"global\"\n",
    "    )\n",
    "    \n",
    "    # (\n",
    "    #     examples_train,\n",
    "    #     examples_model_selection,\n",
    "    #     labels_train,\n",
    "    #     labels_model_selection,\n",
    "    # ) = sklearn.model_selection.train_test_split(examples, labels, train_size=0.75, test_size=0.25)\n",
    "\n",
    "    # concatenate graph_train and train_graph as by time t, we have learned all previous edges up to t \n",
    "    test_graph_e = graph_test.edges(include_edge_weight=True)\n",
    "    test_graph_edges, test_graph_weights= np.array(test_graph_e[0]),np.array(test_graph_e[1])\n",
    "    test_weighted_edges = np.rec.fromarrays([test_graph_edges[:,0], test_graph_edges[:,1], test_graph_weights])\n",
    "    train_graph_e = train_graph.edges(include_edge_weight=True)\n",
    "    train_graph_edges, train_graph_weights= np.array(train_graph_e[0]),np.array(train_graph_e[1])\n",
    "    train_weighted_edges = np.rec.fromarrays([train_graph_edges[:,0], train_graph_edges[:,1], train_graph_weights])\n",
    "    union_graph_test = nx.MultiDiGraph()\n",
    "    union_graph_test.add_weighted_edges_from(test_weighted_edges)\n",
    "    union_graph_test.add_weighted_edges_from(train_weighted_edges)\n",
    "    union_graph_test = StellarGraph.from_networkx(union_graph_test, edge_weight_attr='time', edge_type_attr='directed')\n",
    "\n",
    "    if set(graph_train.edges(include_edge_weight=True)[1]).intersection(set(graph_test.edges(include_edge_weight=True)[1])): \n",
    "        raise(Exception)\n",
    "    return union_graph_test, graph_test, examples_test, labels_test, graph_train, examples, labels #, examples_train,examples_model_selection,labels_train,labels_model_selection,\n",
    "\n",
    "# union_graph_test, graph_test, examples_test, labels_test, graph_train, examples, labels = data_split(graph)\n",
    "# print(len(graph_test.edges()), len(graph_train.edges()))\n",
    "# print(set(graph_train.edges(include_edge_weight=True)[1]).intersection(set(graph_test.edges(include_edge_weight=True)[1])))\n",
    "# graph_test, examples_test, labels_test, graph_train, examples, labels, examples_train,examples_model_selection,labels_train,labels_model_selection, = data_split(graph)\n",
    "\n",
    "def temporal_model(graph, num_walks_per_node=10, walk_length = 10, context_window_size = 2): \n",
    "    num_cw = len(graph.nodes()) * num_walks_per_node * (walk_length - context_window_size + 1)\n",
    "    temporal_rw = TemporalRandomWalk(graph)\n",
    "    temporal_walks = temporal_rw.run(\n",
    "        num_cw=num_cw,\n",
    "        cw_size=context_window_size,\n",
    "        max_walk_length=walk_length,\n",
    "        walk_bias=\"exponential\",\n",
    "    )\n",
    "    \n",
    "    embedding_size = 128\n",
    "    temporal_model = Word2Vec(\n",
    "        temporal_walks,\n",
    "        vector_size=embedding_size,\n",
    "        window=context_window_size,\n",
    "        min_count=0,\n",
    "        sg=1,\n",
    "        workers=2,\n",
    "        epochs=1,)\n",
    "\n",
    "    unseen_node_embedding = np.zeros(embedding_size)\n",
    "\n",
    "    def temporal_embedding(u):\n",
    "        try:\n",
    "            return temporal_model.wv[u]\n",
    "        except KeyError:\n",
    "            return unseen_node_embedding\n",
    "    return temporal_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class TemporalPrediction():\n",
    "    def __init__(self, edge_list, num_walks_per_node=10, walk_length = 10, context_window_size = 2):\n",
    "        edge1, edge2, edge3 = edge_list\n",
    "        edge1_tuple, edge2_tuple, edge3_tuple = tuple(edge1), tuple(edge2), tuple(edge3)\n",
    "        # needed for temporal embedding\n",
    "        self.num_walks_per_node = num_walks_per_node\n",
    "        self.walk_length = walk_length\n",
    "        self.context_window_size = context_window_size\n",
    "\n",
    "        # weight optimization\n",
    "        self.target_edges = [edge1, edge2, edge3]\n",
    "        self.naive_regret = 0\n",
    "        self.past_naive_regrets = []\n",
    "\n",
    "        # record purposes \n",
    "        self.curr_false_positive_set = set()\n",
    "        self.test_errors = []\n",
    "        self.predicted_probs = [] \n",
    "        self.target_regrets = {edge1_tuple: [0], edge2_tuple: [0], edge3_tuple: [0]}\n",
    "        self.naive_target_regrets =  {edge1_tuple: [0], edge2_tuple: [0], edge3_tuple: [0]}\n",
    "        self.past_weight_vectors = {edge1_tuple: [np.arange(0.1, 1.1, 0.1)], edge2_tuple: [np.arange(0.1, 1.1, 0.1)], edge3_tuple: [np.arange(0.1, 1.1, 0.1)]}\n",
    "\n",
    "    def fit_classifier(self, embeddings, labels):\n",
    "        temporal_clf = link_prediction_classifier()\n",
    "        temporal_clf.fit(embeddings, labels)\n",
    "        return temporal_clf\n",
    "\n",
    "    def evaluate_score(self, clf, link_features, link_labels, threshold= 0.5, return_idces=False):\n",
    "        self.predicted_probs = clf.predict_proba(link_features)\n",
    "        positive_column = list(clf.classes_).index(1)\n",
    "        if return_idces: \n",
    "            false_positive_idces = np.where((link_labels == 0) & (self.predicted_probs[:, positive_column] > threshold))[0]\n",
    "            # true_positive_idces = np.where((link_labels == 1) & (predicted[:, positive_column] > 0.5))[0]\n",
    "            return roc_auc_score(link_labels, self.predicted_probs[:, positive_column]), false_positive_idces #, true_positive_idces\n",
    "        return roc_auc_score(link_labels, self.predicted_probs[:, positive_column])\n",
    "\n",
    "    def update_curr_false_positive(self, new_edge_set): \n",
    "        not_false_positive_anymore = self.curr_false_positive_set.intersection(new_edge_set)\n",
    "        print(f\"Not false positive anymore: {len(not_false_positive_anymore)}\")\n",
    "        if not_false_positive_anymore: \n",
    "            self.naive_regret -= len(not_false_positive_anymore)\n",
    "            self.curr_false_positive_set = self.curr_false_positive_set - not_false_positive_anymore\n",
    "            print(f\"New Reduced Regret: {len(self.curr_false_positive_set)}\")            \n",
    "\n",
    "    def predict_probs(self, graph, compute_naive_test_error=False, display_progress=False): \n",
    "        # update regret by checking if they are in the new graph just given  \n",
    "        self.update_curr_false_positive(set(graph.edges()))\n",
    "\n",
    "        # convert networkx graph to stellargraph & split data\n",
    "        graph = StellarGraph.from_networkx(graph, edge_weight_attr='time', edge_type_attr='directed')\n",
    "        union_graph_test, graph_test, examples_test, labels_test, graph_train, examples, labels = data_split(graph)\n",
    "        self.test_edges, self.test_edge_labels = examples_test, labels_test\n",
    "\n",
    "        # fit & learn \n",
    "        temporal_embedding = temporal_model(graph_train, \n",
    "                            num_walks_per_node=self.num_walks_per_node, walk_length=self.walk_length, \\\n",
    "                            context_window_size=self.context_window_size)\n",
    "        temporal_link_features = link_examples_to_features(examples, temporal_embedding)\n",
    "        temporal_clf = self.fit_classifier(temporal_link_features, labels) #fit classifier\n",
    "        \n",
    "        # random walk on the union_graph_test to get the embeddings of potential edges  \n",
    "        temporal_embedding = temporal_model(union_graph_test, \\\n",
    "                                            num_walks_per_node=self.num_walks_per_node, walk_length=self.walk_length, \\\n",
    "                                            context_window_size=self.context_window_size)\n",
    "        temporal_link_features_test = link_examples_to_features(examples_test, temporal_embedding)\n",
    "\n",
    "        # compute probability of potential edges         \n",
    "        temporal_score_test, false_positive_idces = self.evaluate_score(temporal_clf, temporal_link_features_test, labels_test, return_idces=True)\n",
    "\n",
    "        if compute_naive_test_error: #calculate test score\n",
    "            self.test_errors.append(temporal_score_test)\n",
    "            if display_progress: print(f\"Temporal Test Score (ROC AUC): {temporal_score_test:.2f}\")\n",
    "            false_positive_edges = [examples_test[i] for i in false_positive_idces]\n",
    "            \n",
    "            # update regret by adding new false_positive_edges \n",
    "            if false_positive_edges: \n",
    "                false_positive_edges = set([tuple(x) for x in false_positive_edges])\n",
    "                print(f\"New False Positives: {len(false_positive_edges - self.curr_false_positive_set)}\")\n",
    "                self.curr_false_positive_set = self.curr_false_positive_set.union(set(false_positive_edges))\n",
    "                self.naive_regret = len(self.curr_false_positive_set)\n",
    "            self.past_naive_regrets.append(self.naive_regret)\n",
    "            print(f\"Current Regret: {self.naive_regret}\")\n",
    "\n",
    "    def optimize_weights_for_edge(self, learning_rate=0.1, leader_thresholds = np.arange(0.1, 1.1, 0.1)): \n",
    "        # let us choose three edges arbitrarily, then update the weights for each edge and see if they differ \n",
    "        # update regret by checking if they are in the new graph --> weighted majority algorithm \n",
    "        for edge in self.target_edges: \n",
    "            is_tested = np.all(self.test_edges == edge, axis=1)\n",
    "            if sum(is_tested):  #['511', '74'] in lp.test_edges\n",
    "                print(f\"{edge} in test edges\")\n",
    "                edge_index = np.where(is_tested)[0][0]\n",
    "                edge_existence = self.test_edge_labels[edge_index]\n",
    "                edge_existence_prob = self.predicted_probs[edge_index,1] \n",
    "                self.weighted_majority_algorithm_per_edge(tuple(edge), edge_existence_prob, edge_existence, learning_rate, leader_thresholds)\n",
    "\n",
    "    def weighted_majority_algorithm_per_edge(self, edge_tuple, edge_existence_prob, edge_existence, learning_rate = 0.1, leader_thresholds = np.arange(0.1, 1.1, 0.1)):\n",
    "        # question: wouldn't it eventually just continuously decrease the weights of higher threshold leaders? \n",
    "        wait_leaders = [1 if edge_existence_prob > threshold else 0 for threshold in leader_thresholds]\n",
    "        not_wait_leaders = [1 if edge_existence_prob < threshold else 0 for threshold in leader_thresholds]\n",
    "        weight_vector = self.past_weight_vectors[edge_tuple][-1]\n",
    "\n",
    "        # make decision based on each leader's recommendation & weight vector\n",
    "        final_wait_decision = True if np.dot(wait_leaders, weight_vector) > np.dot(not_wait_leaders, weight_vector) else False \n",
    "        naive_decision = True if edge_existence_prob > 0.5 else False\n",
    "\n",
    "        # record regret by checking if they are in the new graph\n",
    "        curr_regret = self.regrets[edge_tuple][-1]\n",
    "        if final_wait_decision != edge_existence:\n",
    "            if final_wait_decision == 1: curr_regret += 2 #false positive treated most harshly \n",
    "            else: curr_regret += 1 #false negative treated less harshly\n",
    "        self.regrets[edge_tuple].append(curr_regret)\n",
    "        \n",
    "        # update naive regret \n",
    "        curr_naive_regret = self.naive_regrets[edge_tuple][-1]\n",
    "        if naive_decision != edge_existence: \n",
    "            if final_wait_decision == 1: curr_naive_regret += 2 #false positive treated most harshly \n",
    "            else: curr_naive_regret += 1 #false negative treated less harshly\n",
    "        self.naive_regrets[edge_tuple].append(curr_naive_regret)\n",
    "\n",
    "        # update & record weight vector \n",
    "        leader_made_right_decision = [1 if wait_decision == edge_existence  else 0 for wait_decision in wait_leaders]\n",
    "        weight_vector = [weight_i * (1-learning_rate) if not right_wait else weight_i \\\n",
    "                        for right_wait, weight_i in zip(leader_made_right_decision, weight_vector)]\n",
    "        self.past_weight_vectors[edge_tuple].append(weight_vector)\n",
    "\n",
    "    def run_expert_simulation(self, graph, learning_rate=0.1, display_progress=False):\n",
    "        self.predict_probs(graph, compute_naive_test_error=True, display_progress=display_progress)\n",
    "        self.optimize_weights_for_edge(learning_rate=learning_rate)\n",
    "        if display_progress: \n",
    "            self.print_current_info(graph.graph['end_date'])\n",
    "\n",
    "    def print_current_info(self, curr_time): \n",
    "        edge1, edge2, edge3 = self.target_regrets.keys()\n",
    "        current_expert_regret = [self.target_regrets[edge1][-1], self.target_regrets[edge2][-1], self.target_regrets[edge3][-1]]\n",
    "        current_naive_regret = [self.naive_target_regrets[edge1][-1], self.naive_target_regrets[edge2][-1], self.naive_target_regrets[edge3][-1]]\n",
    "\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Current Week: {curr_time}\")\n",
    "        # print(f\"training error: {np.round(self.train_errors[-1],2)}, test error: {np.round(self.test_errors[-1],2)}\")\n",
    "        print(f\"Current Expert Regret: {current_expert_regret}\")\n",
    "        print(f\"Current Naive Regret: {current_naive_regret}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "    def graph_train_test_errors(self, time_range, graph_errors = True, graph_regret = False, title=None): \n",
    "        n = min(len(self.train_errors), len(self.test_errors))\n",
    "        if title: plt.title(title)\n",
    "        if graph_errors: \n",
    "            # plt.plot(time_range[:n], self.train_errors[:n], label='train')\n",
    "            plt.plot(time_range[:n], self.test_errors[:n], label='test')\n",
    "        if graph_regret: \n",
    "            plt.plot(time_range[:n], self.curr_regret[:n], label='regret')\n",
    "        plt.legend() \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. given a route, are we doing this? \n",
    "2. or are we doing this for the entire graph? \n",
    "\n",
    "Since we are making the prediction on all potential edges for a graph, how would we do this..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not false positive anymore: 0\n",
      "** Sampled 175 positive and 175 negative edges. **\n",
      "** Sampled 427 positive and 427 negative edges. **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/126 [00:18<38:32, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Test Score (ROC AUC): 0.78\n",
      "New False Positives: 45\n",
      "Current Regret: 45\n",
      "--------------------------------------------------\n",
      "Current Week: 2023-06-01\n",
      "Current Expert Regret: [0, 0, 0]\n",
      "Current Naive Regret: [0, 0, 0]\n",
      "--------------------------------------------------\n",
      "Not false positive anymore: 0\n",
      "** Sampled 175 positive and 175 negative edges. **\n",
      "** Sampled 427 positive and 427 negative edges. **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/126 [00:38<39:42, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Test Score (ROC AUC): 0.74\n",
      "New False Positives: 41\n",
      "Current Regret: 86\n",
      "--------------------------------------------------\n",
      "Current Week: 2023-06-01\n",
      "Current Expert Regret: [0, 0, 0]\n",
      "Current Naive Regret: [0, 0, 0]\n",
      "--------------------------------------------------\n",
      "Not false positive anymore: 0\n",
      "** Sampled 175 positive and 175 negative edges. **\n",
      "** Sampled 427 positive and 427 negative edges. **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/126 [00:56<38:39, 18.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Test Score (ROC AUC): 0.76\n",
      "New False Positives: 83\n",
      "Current Regret: 169\n",
      "--------------------------------------------------\n",
      "Current Week: 2023-06-01\n",
      "Current Expert Regret: [0, 0, 0]\n",
      "Current Naive Regret: [0, 0, 0]\n",
      "--------------------------------------------------\n",
      "Not false positive anymore: 0\n",
      "** Sampled 175 positive and 175 negative edges. **\n",
      "** Sampled 427 positive and 427 negative edges. **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/126 [01:05<44:27, 21.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# for t in range(N-lookback, N): \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m time, graph \u001b[38;5;129;01min\u001b[39;00m tqdm(weekly_graphs\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# compute probability \u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_expert_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[101], line 128\u001b[0m, in \u001b[0;36mTemporalPrediction.run_expert_simulation\u001b[0;34m(self, graph, learning_rate, display_progress)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_expert_simulation\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, display_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_naive_test_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize_weights_for_edge(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m display_progress: \n",
      "Cell \u001b[0;32mIn[101], line 56\u001b[0m, in \u001b[0;36mTemporalPrediction.predict_probs\u001b[0;34m(self, graph, compute_naive_test_error, display_progress)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_edges, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_edge_labels \u001b[38;5;241m=\u001b[39m examples_test, labels_test\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# fit & learn \u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m temporal_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtemporal_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_walks_per_node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_walks_per_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwalk_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwalk_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcontext_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_window_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m temporal_link_features \u001b[38;5;241m=\u001b[39m link_examples_to_features(examples, temporal_embedding)\n\u001b[1;32m     60\u001b[0m temporal_clf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_classifier(temporal_link_features, labels) \u001b[38;5;66;03m#fit classifier\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 65\u001b[0m, in \u001b[0;36mtemporal_model\u001b[0;34m(graph, num_walks_per_node, walk_length, context_window_size)\u001b[0m\n\u001b[1;32m     63\u001b[0m num_cw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(graph\u001b[38;5;241m.\u001b[39mnodes()) \u001b[38;5;241m*\u001b[39m num_walks_per_node \u001b[38;5;241m*\u001b[39m (walk_length \u001b[38;5;241m-\u001b[39m context_window_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m temporal_rw \u001b[38;5;241m=\u001b[39m TemporalRandomWalk(graph)\n\u001b[0;32m---> 65\u001b[0m temporal_walks \u001b[38;5;241m=\u001b[39m \u001b[43mtemporal_rw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_cw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_cw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcw_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_walk_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwalk_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwalk_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexponential\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m embedding_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m     73\u001b[0m temporal_model \u001b[38;5;241m=\u001b[39m Word2Vec(\n\u001b[1;32m     74\u001b[0m     temporal_walks,\n\u001b[1;32m     75\u001b[0m     vector_size\u001b[38;5;241m=\u001b[39membedding_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     80\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/stellargraph/data/explorer.py:1116\u001b[0m, in \u001b[0;36mTemporalRandomWalk.run\u001b[0;34m(self, num_cw, cw_size, max_walk_length, initial_edge_bias, walk_bias, p_walk_success_threshold, seed)\u001b[0m\n\u001b[1;32m   1112\u001b[0m t \u001b[38;5;241m=\u001b[39m times[first_edge_index]\n\u001b[1;32m   1114\u001b[0m remaining_length \u001b[38;5;241m=\u001b[39m num_cw \u001b[38;5;241m-\u001b[39m num_cw_curr \u001b[38;5;241m+\u001b[39m cw_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1116\u001b[0m walk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_walk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_walk_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwalk_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp_rs\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(walk) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m cw_size:\n\u001b[1;32m   1120\u001b[0m     walks\u001b[38;5;241m.\u001b[39mappend(walk)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/stellargraph/data/explorer.py:1183\u001b[0m, in \u001b[0;36mTemporalRandomWalk._walk\u001b[0;34m(self, src, dst, t, length, bias_type, np_rs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m node, time \u001b[38;5;241m=\u001b[39m dst, t\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m-> 1183\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp_rs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp_rs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m         node, time \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/stellargraph/data/explorer.py:1164\u001b[0m, in \u001b[0;36mTemporalRandomWalk._step\u001b[0;34m(self, node, time, bias_type, np_rs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, node, time, bias_type, np_rs):\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;124;03m    Perform 1 temporal step from a node. Returns None if a dead-end is reached.\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m \n\u001b[1;32m   1163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m     neighbours, times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbor_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_edge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m     neighbours \u001b[38;5;241m=\u001b[39m neighbours[times \u001b[38;5;241m>\u001b[39m time]\n\u001b[1;32m   1166\u001b[0m     times \u001b[38;5;241m=\u001b[39m times[times \u001b[38;5;241m>\u001b[39m time]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/stellargraph/core/graph.py:790\u001b[0m, in \u001b[0;36mStellarGraph.neighbor_arrays\u001b[0;34m(self, node, include_edge_weight, edge_types, use_ilocs)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;124;03mObtains the collection of neighbouring nodes connected to the given node\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;124;03mas an array of node_ids. If `include_edge_weight` edge is `True` then\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;124;03m    of edge weights is also returned in a tuple `(neighbor_array, edge_weight_array)`\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_ilocs:\n\u001b[0;32m--> 790\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nodes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_iloc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    792\u001b[0m edge_ilocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edges\u001b[38;5;241m.\u001b[39medge_ilocs(node, ins\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, outs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    793\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edges\u001b[38;5;241m.\u001b[39msources[edge_ilocs]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/stellargraph/core/element_data.py:95\u001b[0m, in \u001b[0;36mExternalIdIndex.to_iloc\u001b[0;34m(self, ids, smaller_type, strict)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_iloc\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids, smaller_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    Convert external IDs ``ids`` to integer locations.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m        smaller_type is False)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     internal_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_valid(ids, internal_ids)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:3437\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3426\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_index_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_indexer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m%\u001b[39m _index_doc_kwargs)\n\u001b[1;32m   3427\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   3428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_indexer\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3434\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m   3435\u001b[0m     \u001b[38;5;66;03m# returned ndarray is np.intp\u001b[39;00m\n\u001b[1;32m   3436\u001b[0m     method \u001b[38;5;241m=\u001b[39m missing\u001b[38;5;241m.\u001b[39mclean_reindex_fill_method(method)\n\u001b[0;32m-> 3437\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_cast_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:5708\u001b[0m, in \u001b[0;36mIndex._maybe_cast_listlike_indexer\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   5704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_cast_listlike_indexer\u001b[39m(\u001b[38;5;28mself\u001b[39m, target) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m   5705\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5706\u001b[0m \u001b[38;5;124;03m    Analogue to maybe_cast_indexer for get_indexer instead of get_loc.\u001b[39;00m\n\u001b[1;32m   5707\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5708\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:6333\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   6331\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m MultiIndex\u001b[38;5;241m.\u001b[39mfrom_arrays(index_like)\n\u001b[1;32m   6332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 6333\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtupleize_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   6334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:495\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# other iterable of some kind\u001b[39;00m\n\u001b[1;32m    494\u001b[0m subarr \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:391\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PandasArray\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrange\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RangeIndex\n\u001b[0;32m--> 391\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_extract_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    394\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m pandas_dtype(dtype)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:6381\u001b[0m, in \u001b[0;36mmaybe_extract_name\u001b[0;34m(name, obj, cls)\u001b[0m\n\u001b[1;32m   6376\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrange\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RangeIndex\n\u001b[1;32m   6378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RangeIndex(\u001b[38;5;241m0\u001b[39m, n, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 6381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmaybe_extract_name\u001b[39m(name, obj, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Hashable:\n\u001b[1;32m   6382\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6383\u001b[0m \u001b[38;5;124;03m    If no name is passed, then extract it from data, validating hashability.\u001b[39;00m\n\u001b[1;32m   6384\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   6385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (Index, ABCSeries)):\n\u001b[1;32m   6386\u001b[0m         \u001b[38;5;66;03m# Note we don't just check for \"name\" attribute since that would\u001b[39;00m\n\u001b[1;32m   6387\u001b[0m         \u001b[38;5;66;03m#  pick up e.g. dtype.name\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# follow the leader \n",
    "# N, lookback = 136, 1\n",
    "# leader_num, learning_rate = 10, 0.1\n",
    "# leader_thresholds, weight_vector, loss_vector = np.arange(0.1, 1.1, 1/leader_num), [1/leader_num]*leader_num, [0] * leader_num\n",
    "\n",
    "# Input: learning rate η > 0\n",
    "# Initialize: let L0 ∈ RN be the all-zero vector\n",
    "edge_list = [['455', '453'], \n",
    "       ['503', '660'],  \n",
    "       ['532', '432']]\n",
    "lp = TemporalPrediction(edge_list)\n",
    "# for t in range(N-lookback, N): \n",
    "for time, graph in tqdm(weekly_graphs.items()):\n",
    "    # compute probability \n",
    "    lp.run_expert_simulation(graph, display_progress=True)\n",
    "    \n",
    "    # play pt and observe loss vector ℓt ∈ [0, 1]N\n",
    "\n",
    "    # when the correct answer is received, penalize each mistaken expert by multiplying its weight by learning_rate\n",
    "    # update Lt = Lt−1 + ℓt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [np.array(['273', '604'], dtype=object), np.array(['664', '973'], dtype=object), np.array(['142', '806'], dtype=object), np.array(['981', '475'], dtype=object), np.array(['744', '20'], dtype=object), np.array(['28', '115'], dtype=object), np.array(['18', '268'], dtype=object)]\n",
    "print(test)\n",
    "print(set([tuple(x) for x in test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
