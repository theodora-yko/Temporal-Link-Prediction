{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 00:08:15.421282: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "from utils import * \n",
    "import stellargraph as sg\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import EdgeSplitter, BiasedRandomWalk, TemporalRandomWalk\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "from math import isclose\n",
    "from sklearn.decomposition import PCA\n",
    "import multiprocessing\n",
    "import sklearn.model_selection \n",
    "from gensim.models import Word2Vec\n",
    "# python3 -m pip install tqdm seaborn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM_Time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class temporalNetwork(): \n",
    "    def __init__(self, start_date, end_date, location_grouping='kma', origin=None, facility_id=None, intermediate=None):\n",
    "        \"\"\" \n",
    "        Note: \n",
    "            start_date and end_date should be both None as they are used as a signal to \n",
    "            CM_Time's run_simulation to whether construct a new graph or update the graph with new information \n",
    "        \"\"\"\n",
    "        self.start_date=start_date\n",
    "        self.end_date=end_date\n",
    "        self.network=None\n",
    "        self.origin_location_list=None\n",
    "        self.location_grouping=location_grouping\n",
    "        self.inbound_data = None \n",
    "        self.outbound_data = None\n",
    "\n",
    "    def construct_network_graph(self):\n",
    "        \"\"\" \n",
    "        Given an inbound and outbound dataframe, construct a network graph and stores it in the class variable self.network\n",
    "        Args: \n",
    "            an_inbound_df (pd.DataFrame): inbound dataframe\n",
    "            an_outbound_df (pd.DataFrame): outbound dataframe\n",
    "            start_date (datetime): start date of the network graph\n",
    "            end_date (datetime): end date of the network graph\n",
    "            location_grouping(string): 'kma' or 'zip3'\n",
    "        \"\"\"\n",
    "        # pull data & construct an empty multiDiGraph\n",
    "\n",
    "        date = self.start_date.strftime(\"%Y-%m-%d\") + \"_\" + self.end_date.strftime(\"%Y-%m-%d\")\n",
    "        an_inbound_df, an_outbound_df = pd.read_csv(f\"data/inbound_data/inbound_data_{date}.csv\"), pd.read_csv(f\"data/outbound_data/outbound_data_{date}.csv\")\n",
    "        an_inbound_df.load_date, an_outbound_df.load_date = pd.to_datetime(an_inbound_df.load_date), pd.to_datetime(an_outbound_df.load_date)\n",
    "        for colin, colout in zip(an_inbound_df.columns, an_outbound_df.columns): \n",
    "            if colin not in [\"total_loads\", \"load_date\"]: \n",
    "                an_inbound_df[colin] = an_inbound_df[colin].astype(str)\n",
    "            if colout not in [\"total_loads\", \"load_date\"]:\n",
    "                an_outbound_df[colout] = an_outbound_df[colout].astype(str)\n",
    "\n",
    "        network_graph = nx.MultiDiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # network_graph = nx.DiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # idf, odf = an_inbound_df.copy(), an_outbound_df.copy()\n",
    "\n",
    "        # idf.to_csv(f\"data/inbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "        # odf.to_csv(f\"data/outbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "\n",
    "        # add nodes & edges \n",
    "        node_1 = f\"origin_{self.location_grouping}_id\"\n",
    "        node_2 = f\"facility_{self.location_grouping}_id\"\n",
    "        node_3 = f\"destination_{self.location_grouping}_id\"\n",
    "\n",
    "        network_graph = add_nodes_given_df(network_graph, an_inbound_df, [node_1, 'facility_id']) \n",
    "        network_graph = add_nodes_given_df(network_graph, an_outbound_df, ['facility_id', node_2, node_3]) \n",
    "\n",
    "        network_graph = add_edges_given_graph(network_graph, an_inbound_df, an_outbound_df, self.location_grouping)\n",
    "        \n",
    "        # update the variables \n",
    "        self.network = network_graph\n",
    "        self.origin_location_list = an_inbound_df[f'origin_{self.location_grouping}_id'].unique()\n",
    "        self.inbound_data, self.outbound_data = an_inbound_df, an_outbound_df\n",
    "        print(f\"Current time of the graph: {self.start_date} to {self.end_date}\") \n",
    "        \n",
    "    def move_to_next_week(self): \n",
    "        \"\"\" \n",
    "        Given the new week's inbound and outbound dataframes, \n",
    "        update self.network graph, self.start_date, and self.end_date to a week after current start date and end date \n",
    "        \n",
    "        Args:\n",
    "            next_inbound_df (pd.DataFrame): new week's inbound dataframe\n",
    "            next_outbound_df (pd.DataFrame): new week's outbound dataframe\n",
    "            display_progress (boolean): whether to display the progress of the function or not\n",
    "        \"\"\"\n",
    "        # update the dates, pull new week's data, & store some informations\n",
    "        self.start_date, self.end_date = self.start_date + timedelta(days=7), self.end_date + timedelta(days=7)\n",
    "        date = self.start_date.strftime(\"%Y-%m-%d\") + \"_\" + self.end_date.strftime(\"%Y-%m-%d\")\n",
    "        next_inbound_df, next_outbound_df = pd.read_csv(f\"data/inbound_data/inbound_data_{date}.csv\"), pd.read_csv(f\"data/outbound_data/outbound_data_{date}.csv\")\n",
    "        next_inbound_df.load_date, next_outbound_df.load_date = pd.to_datetime(next_inbound_df.load_date), pd.to_datetime(next_outbound_df.load_date)\n",
    "        for colin, colout in zip(next_inbound_df.columns, next_outbound_df.columns): \n",
    "            if colin not in [\"total_loads\", \"load_date\"]: \n",
    "                next_inbound_df[colin] = next_inbound_df[colin].astype(str)\n",
    "            if colout not in [\"total_loads\", \"load_date\"]:\n",
    "                next_outbound_df[colout] = next_outbound_df[colout].astype(str)\n",
    "\n",
    "        # next_inbound_df.to_csv(f\"data/inbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "        # next_outbound_df.to_csv(f\"data/outbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "\n",
    "        ## if not len(next_inbound_df) and not len(next_outbound_df): \n",
    "        curr_nodes, curr_edges = set(self.network.nodes()), list(self.network.edges(data=True, keys=True)) # needed for efficient removal of nodes & edges\n",
    "        \n",
    "        node_1 = f\"origin_{self.location_grouping}_id\"\n",
    "        node_2 = f\"facility_{self.location_grouping}_id\"\n",
    "        node_3 = f\"destination_{self.location_grouping}_id\"\n",
    "\n",
    "        # or statement is used to check if either inbound or outbound df exists, \n",
    "        # so we can add any final delivery data to the graph\n",
    "        if len(next_inbound_df) or len(next_outbound_df): \n",
    "            # add new nodes \n",
    "            new_nodes = set()\n",
    "            if len(next_inbound_df): new_nodes = set(next_inbound_df[[node_1, 'facility_id']].to_numpy().flatten())\n",
    "            if len(next_outbound_df): new_nodes.union(set(next_outbound_df[[node_2, node_3, 'facility_id']].to_numpy().flatten()))\n",
    "            new_nodes = new_nodes.difference(curr_nodes)\n",
    "            self.network.add_nodes_from(new_nodes)\n",
    "\n",
    "            # add new edges\n",
    "            self.network = add_edges_given_graph(self.network, next_inbound_df, next_outbound_df, self.location_grouping)\n",
    "\n",
    "        # remove old edges\n",
    "        past_edges = []\n",
    "        for edge in curr_edges: \n",
    "            if edge[2] < to_integer(self.start_date): \n",
    "                self.network.remove_edge(edge[0], edge[1], edge[2])\n",
    "                past_edges.append(edge)\n",
    "\n",
    "        # remove isolated nodes\n",
    "        isolated_nodes = list(nx.isolates(self.network))\n",
    "        self.network.remove_nodes_from(isolated_nodes)\n",
    "        \n",
    "        # update variables\n",
    "        self.network.graph['start_date']= self.start_date\n",
    "        self.network.graph['end_date']= self.end_date\n",
    "        self.inbound_data, self.outbound_data = next_inbound_df, next_outbound_df\n",
    "        if self.location_grouping == 'kma': self.origin_location_list = list(set([x.split(\" \")[0] for x in self.network.nodes() if len(x) == 6])) \n",
    "        else: self.origin_location_list = list(set([x.split(\" \")[0] for x in self.network.nodes() if len(x) == 3]))\n",
    "        \n",
    "        print(f\"Current time of the graph: {self.start_date} to {self.end_date}\")  \n",
    "\n",
    "    def print_network_information(self, given_network, print_network_time=False): \n",
    "        \"\"\"\n",
    "        Given a network, print out the information of the network\n",
    "        Args: \n",
    "            given_network (nx.MultiDiGraph): a network graph\n",
    "        Returns: N/A\n",
    "        \"\"\"\n",
    "        print(\"---------------------------------------------------------------------------------------------\") \n",
    "        print(given_network)\n",
    "        print(f\"Is the given network a DAG for load_network?: {nx.is_directed_acyclic_graph(given_network)}\")\n",
    "        print(f\"Number of self loops: {nx.number_of_selfloops(given_network)}\")\n",
    "        if print_network_time: print(f\"Current time of the graph: {given_network.graph['start_date']} to {given_network.graph['end_date']}\")\n",
    "        else: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_Finder():\n",
    "    def __init__(self, location_grouping='kma', origin_location_list=None, network = None):\n",
    "        self.network=network\n",
    "        self.processed_network=None\n",
    "        self.processed=False \n",
    "        self.origin_location_list=origin_location_list\n",
    "        self.match_failure = None\n",
    "        self.remove_failure = 0\n",
    "        self.location_grouping=location_grouping\n",
    "        \n",
    "    def group_to_DiGraph(self, display_progress = False):\n",
    "        \"\"\"\n",
    "        #TODO: explain why aggregate_faciility_zip then group_to_DiGraph (kma-> facility_zip -> kma to kma->kma->kma, aggregate to faciliy KMA)\n",
    "        Assuming that self.network is constructed, \n",
    "        sums the edge weights for edges with the same nodes in self.network variable and \n",
    "        stores the new graph with aggregated edges in self.processed_network variable and returns False if successful \n",
    "\n",
    "        Args:\n",
    "            display_progress (boolean): whether to display the progress of the function or not\n",
    "\n",
    "        NOTE) disregards temporal factor \n",
    "        \"\"\"\n",
    "        if not self.network: \n",
    "            print(\"Please construct the network first\")\n",
    "            return None \n",
    "            \n",
    "        new_name = self.network.name + \" reduced\"\n",
    "        self.processed_network = nx.DiGraph(name=new_name)\n",
    "        self.processed_network.add_nodes_from(self.network)\n",
    "\n",
    "        if display_progress: print(\"Aggregating nodes by KMA...\")\n",
    "        for n1, n2 in self.network.edges():\n",
    "            sum = 0 \n",
    "            for inner_dict in self.network.get_edge_data(n1, n2).values(): \n",
    "                sum += inner_dict['capacity']\n",
    "            self.processed_network.add_edge(n1, n2, capacity = sum)\n",
    "        \n",
    "        nx.set_edge_attributes(self.processed_network, to_integer(self.network.graph['end_date']), 'time')\n",
    "        self.processed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_Time(): \n",
    "    def __init__(self, location_grouping='kma', origin=None, facility_id=None, intermediate=None): \n",
    "        self.start_date=None\n",
    "        self.end_date=None\n",
    "        self.cm_network = None   \n",
    "        self.cm_finder = None\n",
    "        self.origin=origin\n",
    "        self.facility_id=facility_id\n",
    "        self.intermediate=intermediate\n",
    "        self.location_grouping = location_grouping\n",
    "        self.weekly_graphs = {}\n",
    "    \n",
    "    def update_dates(self): \n",
    "        \"\"\"\n",
    "        Updates the start and end date by 7 days\n",
    "        \"\"\" \n",
    "        self.start_date += timedelta(days=7)\n",
    "        self.end_date += timedelta(days=7)\n",
    "\n",
    "    def construct_or_update_tg(self, filter_key='load_count', display_progress=False, display_path_info=False):\n",
    "        \"\"\" \n",
    "        Either (1) creates a network graph given a start and end date \n",
    "            or (2) updates the network graph to the next week's graph \n",
    "\n",
    "        Args: \n",
    "            start_date, end_date: start & end date of the first two weeks of the simulation\n",
    "\n",
    "        Returns: n/a\n",
    "        \"\"\"        \n",
    "        # construct or update cm_finder.network \n",
    "        if not self.cm_network: \n",
    "            self.cm_network = temporalNetwork(self.start_date, self.end_date, self.location_grouping, self.origin, self.facility_id, self.intermediate)\n",
    "            self.cm_network.construct_network_graph()\n",
    "            self.cm_finder = CM_Finder(location_grouping=self.location_grouping)\n",
    "        else: \n",
    "            self.cm_network.move_to_next_week()\n",
    "\n",
    "        self.cm_finder.origin_location_list = self.cm_network.origin_location_list\n",
    "        self.cm_finder.network = self.cm_network.network\n",
    "        \n",
    "        # self.cm_finder.group_to_DiGraph(display_progress = display_progress)\n",
    "        self.weekly_graphs[self.end_date] = self.cm_finder.network\n",
    "\n",
    "    def temporal_query(self, start_date, temporal=True, looback = 7, number_of_weeks=None, termination_date = None, \\\n",
    "                    filter_key = \"load_count\", display_progress=False, display_path_info = False): \n",
    "        \"\"\"\n",
    "        Given a start date, run the simulation for number_of_weeks or until termination_date is reached.\n",
    "\n",
    "        Args: \n",
    "            start_date: start date of the first week of the simulation\n",
    "            temporal: if True, run the simulation for every two weeks, if False, run the simulation from start_date until end_date\n",
    "            number_of_weeks: number of weeks to run the simulation for\n",
    "            termination_date: date to stop the simulation\n",
    "            filter_key: key to filter the network on (load_count or path_score)\n",
    "            display_progress: if True, display progress bar\n",
    "            display_path_info: if True, display path info\n",
    "\n",
    "        Returns: a dictionary of simulation result for each week\n",
    "            \n",
    "        Note:\n",
    "        * termination_date: termination date of the entire analysis, when end_date reaches termination_date, the query loop terminates,\n",
    "        * end_date: the end date of the two-week window, will be updated every week\n",
    "\n",
    "        - Once the parameters (location_grouping, origin, facility_id, intermediate) are used to initialise the cm_time class, \n",
    "          they will be used for any further analysis until new initialisation happens.\n",
    "          query_weekly method will only perform analysis, no alterations can be made by calling solely this.\n",
    "\n",
    "        - If temporal=True, --> end_date != termination_date, eventually at the end of simulations, end_date = termination_date\n",
    "            and number_of_weeks is given, end_date = start_date + 13 days for the first simulation, termination_date = start_date + 7 days * number_of_weeks\n",
    "            and termination_date is given, end_date = start_date + 13 days for the first simulation and termination_date=termination_date for the simulation\n",
    "          If temporal=False --> end_date = termination_date \n",
    "            and number_of_weeks is given, end_date = start_date + 7 days * number_of_weeks for the simulation \n",
    "            and termination_date is given, end_date=termination_date for the simulation\n",
    "\n",
    "        * simulation_results: a dictionary with key as the end_date and value as the simulation result\n",
    "        \"\"\"\n",
    "\n",
    "        # create information needed for a new query with the given start_date and number_of_weeks\n",
    "        # possible bug when end_date > termination_date.\n",
    "        if temporal: \n",
    "            self.start_date, self.end_date = start_date, start_date + timedelta(days=looback-1)\n",
    "\n",
    "            if number_of_weeks: termination_date = self.start_date + timedelta(days=7) * number_of_weeks\n",
    "            elif termination_date: termination_date = termination_date\n",
    "            else: raise Exception(\"Neither number of weeks nor termination date was given to set the simulation time period.\")\n",
    "        \n",
    "            # run simulation for every two weeks until termination_date\n",
    "            while self.end_date <= termination_date:    \n",
    "                self.construct_or_update_tg(filter_key=filter_key, \\\n",
    "                                    display_progress=display_progress, display_path_info=display_path_info)\n",
    "                self.update_dates() \n",
    "                \n",
    "        else: \n",
    "            if number_of_weeks: self.start_date, self.end_date = start_date, start_date + timedelta(days=7) * number_of_weeks\n",
    "            elif termination_date: self.start_date, self.end_date = start_date, termination_date\n",
    "            else: raise Exception(\"Neither number of weeks nor termination date was given to set the simulation time period.\")\n",
    "            self.run_single_simulation(filter_key=filter_key, \\\n",
    "                                display_progress=display_progress, display_path_info=display_path_info)\n",
    "\n",
    "        return self.weekly_graphs\n",
    "\n",
    "    def if_edge(node1, node2, curr_graph): \n",
    "        adjacency_matrix = curr_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct graph old & new \n",
    "cm_time = CM_Time(location_grouping = 'zip3') \n",
    "weekly_graphs = cm_time.temporal_query(start_date=datetime(2021,1,1).date(), looback=7, termination_date= datetime(2023,6, 1).date(), \\\n",
    "                    display_progress=False, display_path_info = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operator_l2(u, v):\n",
    "    return (u - v) ** 2\n",
    "\n",
    "def operator_sub(u, v):\n",
    "    return (u - v)\n",
    "\n",
    "binary_operator = operator_l2\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def link_examples_to_features(link_examples, transform_node):\n",
    "    return [\n",
    "        operator_l2(transform_node(src), transform_node(dst)) for src, dst in link_examples\n",
    "    ]\n",
    "\n",
    "def link_examples_to_features_sub(link_examples, transform_node):\n",
    "    return [\n",
    "        operator_sub(transform_node(src), transform_node(dst)) for src, dst in link_examples\n",
    "    ]\n",
    "\n",
    "def link_prediction_classifier(max_iter=2000):\n",
    "    lr_clf = LogisticRegressionCV(Cs=10, cv=10, scoring=\"roc_auc\", max_iter=max_iter, penalty=\"l2\") #, solver=\"liblinear\")\n",
    "    return Pipeline(steps=[(\"sc\", StandardScaler()), (\"clf\", lr_clf)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(graph, prediction_window_size=2): \n",
    "    # graph = stellar graph\n",
    "    # identify edges based on dates\n",
    "    edges, weights = np.array(graph.edges(include_edge_weight=True)[0]), np.array(graph.edges(include_edge_weight=True)[1])\n",
    "    lower_lim = sorted(list(set(weights)))[-prediction_window_size]\n",
    "    index_test, index_train = np.where(weights >= lower_lim)[0], np.where(weights < lower_lim)[0]\n",
    "\n",
    "    # create test & train edge sets\n",
    "    test_edges, test_labels = edges[index_test], weights[index_test]\n",
    "    train_edges, train_labels = edges[index_train], weights[index_train]\n",
    "    test_weighted_edges = np.rec.fromarrays([test_edges[:,0], test_edges[:,1], test_labels])\n",
    "    train_weighted_edges = np.rec.fromarrays([train_edges[:,0], train_edges[:,1], train_labels])\n",
    "    # print(set(test_labels), set(train_labels))\n",
    "\n",
    "    # create test and train graph \n",
    "    test_graph,train_graph = nx.MultiDiGraph(), nx.MultiDiGraph()\n",
    "    test_graph.add_weighted_edges_from(test_weighted_edges,weight='time')\n",
    "    train_graph.add_weighted_edges_from(train_weighted_edges,weight='time')\n",
    "    test_graph, train_graph = StellarGraph.from_networkx(test_graph, edge_weight_attr='time', edge_type_attr='directed'), \\\n",
    "                        StellarGraph.from_networkx(train_graph, edge_weight_attr='time', edge_type_attr='directed'), \n",
    "\n",
    "    # create pos & neg edges \n",
    "    edge_splitter_test = EdgeSplitter(test_graph, graph)\n",
    "    graph_test, examples_test, labels_test = edge_splitter_test.train_test_split( #result_graph, [u, v], edge_data_labels (1 or 0)\n",
    "        p=0.1, method=\"global\",\n",
    "    )\n",
    "\n",
    "    ## train graph \n",
    "    edge_splitter_train = EdgeSplitter(train_graph, graph)\n",
    "    graph_train, examples, labels = edge_splitter_train.train_test_split(\n",
    "        p=0.1, method=\"global\"\n",
    "    )\n",
    "    \n",
    "    # (\n",
    "    #     examples_train,\n",
    "    #     examples_model_selection,\n",
    "    #     labels_train,\n",
    "    #     labels_model_selection,\n",
    "    # ) = sklearn.model_selection.train_test_split(examples, labels, train_size=0.75, test_size=0.25)\n",
    "\n",
    "    # concatenate graph_train and train_graph as by time t, we have learned all previous edges up to t \n",
    "    test_graph_e = graph_test.edges(include_edge_weight=True)\n",
    "    test_graph_edges, test_graph_weights= np.array(test_graph_e[0]),np.array(test_graph_e[1])\n",
    "    test_weighted_edges = np.rec.fromarrays([test_graph_edges[:,0], test_graph_edges[:,1], test_graph_weights])\n",
    "    train_graph_e = train_graph.edges(include_edge_weight=True)\n",
    "    train_graph_edges, train_graph_weights= np.array(train_graph_e[0]),np.array(train_graph_e[1])\n",
    "    train_weighted_edges = np.rec.fromarrays([train_graph_edges[:,0], train_graph_edges[:,1], train_graph_weights])\n",
    "    union_graph_test = nx.MultiDiGraph()\n",
    "    union_graph_test.add_weighted_edges_from(test_weighted_edges)\n",
    "    union_graph_test.add_weighted_edges_from(train_weighted_edges)\n",
    "    union_graph_test = StellarGraph.from_networkx(union_graph_test, edge_weight_attr='time', edge_type_attr='directed')\n",
    "\n",
    "    if set(graph_train.edges(include_edge_weight=True)[1]).intersection(set(graph_test.edges(include_edge_weight=True)[1])): \n",
    "        raise(Exception)\n",
    "    return union_graph_test, graph_test, examples_test, labels_test, graph_train, examples, labels #, examples_train,examples_model_selection,labels_train,labels_model_selection,\n",
    "\n",
    "# union_graph_test, graph_test, examples_test, labels_test, graph_train, examples, labels = data_split(graph)\n",
    "# print(len(graph_test.edges()), len(graph_train.edges()))\n",
    "# print(set(graph_train.edges(include_edge_weight=True)[1]).intersection(set(graph_test.edges(include_edge_weight=True)[1])))\n",
    "# graph_test, examples_test, labels_test, graph_train, examples, labels, examples_train,examples_model_selection,labels_train,labels_model_selection, = data_split(graph)\n",
    "\n",
    "def temporal_model(graph, num_walks_per_node=10, walk_length = 10, context_window_size = 2): \n",
    "    num_cw = len(graph.nodes()) * num_walks_per_node * (walk_length - context_window_size + 1)\n",
    "    temporal_rw = TemporalRandomWalk(graph)\n",
    "    temporal_walks = temporal_rw.run(\n",
    "        num_cw=num_cw,\n",
    "        cw_size=context_window_size,\n",
    "        max_walk_length=walk_length,\n",
    "        walk_bias=\"exponential\",\n",
    "    )\n",
    "    \n",
    "    embedding_size = 128\n",
    "    temporal_model = Word2Vec(\n",
    "        temporal_walks,\n",
    "        vector_size=embedding_size,\n",
    "        window=context_window_size,\n",
    "        min_count=0,\n",
    "        sg=1,\n",
    "        workers=2,\n",
    "        epochs=1,)\n",
    "\n",
    "    unseen_node_embedding = np.zeros(embedding_size)\n",
    "\n",
    "    def temporal_embedding(u):\n",
    "        try:\n",
    "            return temporal_model.wv[u]\n",
    "        except KeyError:\n",
    "            return unseen_node_embedding\n",
    "    return temporal_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_classifier(graph_train, examples, labels):\n",
    "    temporal_embedding = temporal_model(graph_train)\n",
    "    temporal_link_features = link_examples_to_features(examples, temporal_embedding)\n",
    "    temporal_clf = link_prediction_classifier()\n",
    "    temporal_clf.fit(temporal_link_features, labels)\n",
    "    return temporal_clf\n",
    "\n",
    "def evaluate_score(clf, link_features, link_labels, return_idces=False):\n",
    "    predicted = clf.predict_proba(link_features)\n",
    "    positive_column = list(clf.classes_).index(1)\n",
    "    if return_idces: \n",
    "        false_positive_idces = np.where((link_labels == 0) & (predicted[:, positive_column] > 0.5))[0]\n",
    "        true_positive_idces = np.where((link_labels == 1) & (predicted[:, positive_column] > 0.5))[0]\n",
    "        return roc_auc_score(link_labels, predicted[:, positive_column]), false_positive_idces, true_positive_idces\n",
    "    return roc_auc_score(link_labels, predicted[:, positive_column])\n",
    "\n",
    "def run_granular_analysis(graph, num_walks_per_node = 10, walk_length = 10, context_window_size = 2, display_progress=False): \n",
    "    union_graph_test, graph_test, examples_test, labels_test, graph_train, examples, labels = data_split(graph)\n",
    "    temporal_embedding = temporal_model(graph_train, \\\n",
    "                                        num_walks_per_node=num_walks_per_node, walk_length=walk_length, \\\n",
    "                                        context_window_size=context_window_size)\n",
    "    # fit & learn \n",
    "    temporal_clf = fit_classifier(graph_train, examples, labels)\n",
    "    temporal_link_features = link_examples_to_features(examples, temporal_embedding)\n",
    "    temporal_score_train = evaluate_score(temporal_clf, temporal_link_features, labels)\n",
    "    if display_progress: print(f\"Temporal Training Score (ROC AUC): {temporal_score_train:.2f}\")\n",
    "\n",
    "    # random walk on the test graph + train_graph & calculate test score\n",
    "    temporal_embedding = temporal_model(union_graph_test, \\\n",
    "                                        num_walks_per_node=num_walks_per_node, walk_length=walk_length, \\\n",
    "                                        context_window_size=context_window_size)\n",
    "    temporal_link_features_test = link_examples_to_features(examples_test, temporal_embedding)\n",
    "    temporal_score_test, false_positive_idces, true_positive_idces = evaluate_score(temporal_clf, temporal_link_features_test, labels_test, return_idces=True)\n",
    "    false_positive_edges = [examples_test[i] for i in false_positive_idces]\n",
    "    true_positive_edges = [examples_test[i] for i in true_positive_idces]\n",
    "    if display_progress: print(f\"Temporal Test Score (ROC AUC): {temporal_score_test:.2f}\")\n",
    "\n",
    "    return temporal_clf, temporal_score_train, temporal_score_test, temporal_link_features_test, false_positive_edges, true_positive_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Sampled 175 positive and 175 negative edges. **\n",
      "** Sampled 427 positive and 427 negative edges. **\n",
      "Temporal Training Score (ROC AUC): 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:29<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Test Score (ROC AUC): 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_errors, test_errors = defaultdict(list), defaultdict(list)\n",
    "for time, graph in tqdm(weekly_graphs.items()): \n",
    "    graph_t = StellarGraph.from_networkx(graph, edge_weight_attr='time', edge_type_attr='directed')\n",
    "    temporal_clf, temporal_score_train, temporal_score_test, temporal_link_features_test, false_positive_edges, true_positive_edges = run_granular_analysis(graph_t, display_progress= True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for edge in false_positive_edges: \n",
    "#     if graph.has_edge(edge[0], edge[1]): print(f\"false_positive returned but is true positive: {edge}\")\n",
    "# for edge in true_positive_edges: \n",
    "#     if not graph.has_edge(edge[0], edge[1]): print(f\"true positive returned but is false positive: {edge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
