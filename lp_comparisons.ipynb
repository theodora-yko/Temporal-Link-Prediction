{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install stellargraph chardet networkx seaborn numpy pandas tqdm gensim scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 00:51:41.729792: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "from utils import * \n",
    "import stellargraph as sg\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import EdgeSplitter, BiasedRandomWalk, TemporalRandomWalk\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from math import isclose\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from TRW_nodesim import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM_Time() & Weekly Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class temporalNetwork(): \n",
    "    def __init__(self, start_date, end_date, display_progress=False, location_grouping='kma', origin=None, facility_id=None, intermediate=None):\n",
    "        \"\"\" \n",
    "        Note: \n",
    "            start_date and end_date should be both None as they are used as a signal to \n",
    "            CM_Time's run_simulation to whether construct a new graph or update the graph with new information \n",
    "        \"\"\"\n",
    "        self.display_progress = display_progress\n",
    "        self.start_date=start_date\n",
    "        self.end_date=end_date\n",
    "        self.network=None\n",
    "        self.origin_location_list=None\n",
    "        self.location_grouping=location_grouping\n",
    "        self.inbound_data = None \n",
    "        self.outbound_data = None\n",
    "\n",
    "    def construct_network_graph(self):\n",
    "        \"\"\" \n",
    "        Given an inbound and outbound dataframe, construct a network graph and stores it in the class variable self.network\n",
    "        Args: \n",
    "            an_inbound_df (pd.DataFrame): inbound dataframe\n",
    "            an_outbound_df (pd.DataFrame): outbound dataframe\n",
    "            start_date (datetime): start date of the network graph\n",
    "            end_date (datetime): end date of the network graph\n",
    "            location_grouping(string): 'kma' or 'zip3'\n",
    "        \"\"\"\n",
    "        # pull data & construct an empty multiDiGraph\n",
    "\n",
    "        date = self.start_date.strftime(\"%Y-%m-%d\") + \"_\" + self.end_date.strftime(\"%Y-%m-%d\")\n",
    "        an_inbound_df, an_outbound_df = pd.read_csv(f\"inbound_{self.location_grouping}_data/inbound_data_{date}.csv\"), pd.read_csv(f\"outbound_{self.location_grouping}_data/outbound_data_{date}.csv\")\n",
    "        an_inbound_df.load_date, an_outbound_df.load_date = pd.to_datetime(an_inbound_df.load_date), pd.to_datetime(an_outbound_df.load_date)\n",
    "        for colin, colout in zip(an_inbound_df.columns, an_outbound_df.columns): \n",
    "            if colin not in [\"total_loads\", \"load_date\"]: \n",
    "                an_inbound_df[colin] = an_inbound_df[colin].astype(str)\n",
    "            if colout not in [\"total_loads\", \"load_date\"]:\n",
    "                an_outbound_df[colout] = an_outbound_df[colout].astype(str)\n",
    "\n",
    "        network_graph = nx.MultiDiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # network_graph = nx.DiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # idf, odf = an_inbound_df.copy(), an_outbound_df.copy()\n",
    "\n",
    "        # idf.to_csv(f\"data/inbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "        # odf.to_csv(f\"data/outbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "\n",
    "        # add nodes & edges \n",
    "        node_1 = f\"origin_{self.location_grouping}_id\"\n",
    "        node_2 = f\"facility_{self.location_grouping}_id\"\n",
    "        node_3 = f\"destination_{self.location_grouping}_id\"\n",
    "\n",
    "        network_graph = add_nodes_given_df(network_graph, an_inbound_df, [node_1, 'facility_id']) \n",
    "        network_graph = add_nodes_given_df(network_graph, an_outbound_df, ['facility_id', node_2, node_3]) \n",
    "\n",
    "        network_graph = add_edges_given_graph(network_graph, an_inbound_df, an_outbound_df, self.location_grouping)\n",
    "        \n",
    "        # update the variables \n",
    "        self.network = network_graph\n",
    "        self.origin_location_list = an_inbound_df[f'origin_{self.location_grouping}_id'].unique()\n",
    "        self.inbound_data, self.outbound_data = an_inbound_df, an_outbound_df\n",
    "        if self.display_progress: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\") \n",
    "        \n",
    "    def move_to_next_week(self): \n",
    "        \"\"\" \n",
    "        Given the new week's inbound and outbound dataframes, \n",
    "        update self.network graph, self.start_date, and self.end_date to a week after current start date and end date \n",
    "        \n",
    "        Args:\n",
    "            next_inbound_df (pd.DataFrame): new week's inbound dataframe\n",
    "            next_outbound_df (pd.DataFrame): new week's outbound dataframe\n",
    "            display_progress (boolean): whether to display the progress of the function or not\n",
    "        \"\"\"\n",
    "        # update the dates, pull new week's data, & store some informations\n",
    "        self.start_date, self.end_date = self.start_date + timedelta(days=7), self.end_date + timedelta(days=7)\n",
    "        date = self.start_date.strftime(\"%Y-%m-%d\") + \"_\" + self.end_date.strftime(\"%Y-%m-%d\")\n",
    "        next_inbound_df, next_outbound_df = pd.read_csv(f\"inbound_{self.location_grouping}_data/inbound_data_{date}.csv\"), pd.read_csv(f\"outbound_{self.location_grouping}_data/outbound_data_{date}.csv\")\n",
    "        next_inbound_df.load_date, next_outbound_df.load_date = pd.to_datetime(next_inbound_df.load_date), pd.to_datetime(next_outbound_df.load_date)\n",
    "        for colin, colout in zip(next_inbound_df.columns, next_outbound_df.columns): \n",
    "            if colin not in [\"total_loads\", \"load_date\"]: \n",
    "                next_inbound_df[colin] = next_inbound_df[colin].astype(str)\n",
    "            if colout not in [\"total_loads\", \"load_date\"]:\n",
    "                next_outbound_df[colout] = next_outbound_df[colout].astype(str)\n",
    "\n",
    "        network_graph = nx.MultiDiGraph(name=f\"new network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # network_graph = nx.DiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # idf, odf = an_inbound_df.copy(), an_outbound_df.copy()\n",
    "\n",
    "        # idf.to_csv(f\"data/inbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "        # odf.to_csv(f\"data/outbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "\n",
    "        # add nodes & edges \n",
    "        node_1 = f\"origin_{self.location_grouping}_id\"\n",
    "        node_2 = f\"facility_{self.location_grouping}_id\"\n",
    "        node_3 = f\"destination_{self.location_grouping}_id\"\n",
    "\n",
    "        network_graph = add_nodes_given_df(network_graph, next_inbound_df, [node_1, 'facility_id']) \n",
    "        network_graph = add_nodes_given_df(network_graph, next_outbound_df, ['facility_id', node_2, node_3]) \n",
    "\n",
    "        network_graph = add_edges_given_graph(network_graph, next_inbound_df, next_outbound_df, self.location_grouping)\n",
    "\n",
    "         # update the variables \n",
    "        self.network = network_graph\n",
    "        self.origin_location_list = next_inbound_df[f'origin_{self.location_grouping}_id'].unique()\n",
    "        self.inbound_data, self.outbound_data = next_inbound_df, next_outbound_df\n",
    "        if self.display_progress: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\") \n",
    "       \n",
    "\n",
    "    def print_network_information(self, given_network, print_network_time=False): \n",
    "        \"\"\"\n",
    "        Given a network, print out the information of the network\n",
    "        Args: \n",
    "            given_network (nx.MultiDiGraph): a network graph\n",
    "        Returns: N/A\n",
    "        \"\"\"\n",
    "        print(\"---------------------------------------------------------------------------------------------\") \n",
    "        print(given_network)\n",
    "        print(f\"Is the given network a DAG for load_network?: {nx.is_directed_acyclic_graph(given_network)}\")\n",
    "        print(f\"Number of self loops: {nx.number_of_selfloops(given_network)}\")\n",
    "        if print_network_time: print(f\"Current time of the graph: {given_network.graph['start_date']} to {given_network.graph['end_date']}\")\n",
    "        else: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_Finder():\n",
    "    def __init__(self, location_grouping='kma', origin_location_list=None, network = None):\n",
    "        self.network=network\n",
    "        self.processed_network=None\n",
    "        self.processed=False \n",
    "        self.origin_location_list=origin_location_list\n",
    "        self.match_failure = None\n",
    "        self.remove_failure = 0\n",
    "        self.location_grouping=location_grouping\n",
    "        \n",
    "    def group_to_DiGraph(self, display_progress = False):\n",
    "        \"\"\"\n",
    "        #TODO: explain why aggregate_faciility_zip then group_to_DiGraph (kma-> facility_zip -> kma to kma->kma->kma, aggregate to faciliy KMA)\n",
    "        Assuming that self.network is constructed, \n",
    "        sums the edge weights for edges with the same nodes in self.network variable and \n",
    "        stores the new graph with aggregated edges in self.processed_network variable and returns False if successful \n",
    "\n",
    "        Args:\n",
    "            display_progress (boolean): whether to display the progress of the function or not\n",
    "\n",
    "        NOTE) disregards temporal factor \n",
    "        \"\"\"\n",
    "        if not self.network: \n",
    "            print(\"Please construct the network first\")\n",
    "            return None \n",
    "            \n",
    "        new_name = self.network.name + \" reduced\"\n",
    "        self.processed_network = nx.DiGraph(name=new_name)\n",
    "        self.processed_network.add_nodes_from(self.network)\n",
    "\n",
    "        if display_progress: print(\"Aggregating nodes by KMA...\")\n",
    "        for n1, n2 in self.network.edges():\n",
    "            sum = 0 \n",
    "            for inner_dict in self.network.get_edge_data(n1, n2).values(): \n",
    "                sum += inner_dict['capacity']\n",
    "            self.processed_network.add_edge(n1, n2, capacity = sum)\n",
    "        \n",
    "        nx.set_edge_attributes(self.processed_network, to_integer(self.network.graph['end_date']), 'time')\n",
    "        self.processed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_Time(): \n",
    "    def __init__(self, location_grouping='kma', origin=None, facility_id=None, intermediate=None): \n",
    "        self.start_date=None\n",
    "        self.end_date=None\n",
    "        self.cm_network = None   \n",
    "        self.cm_finder = None\n",
    "        self.origin=origin\n",
    "        self.facility_id=facility_id\n",
    "        self.intermediate=intermediate\n",
    "        self.location_grouping = location_grouping\n",
    "        self.weekly_graphs = {}\n",
    "    \n",
    "    def update_dates(self): \n",
    "        \"\"\"\n",
    "        Updates the start and end date by 7 days\n",
    "        \"\"\" \n",
    "        self.start_date += timedelta(days=7)\n",
    "        self.end_date += timedelta(days=7)\n",
    "\n",
    "    def construct_or_update_tg(self, filter_key='load_count', display_progress=False, display_path_info=False):\n",
    "        \"\"\" \n",
    "        Either (1) creates a network graph given a start and end date \n",
    "            or (2) updates the network graph to the next week's graph \n",
    "\n",
    "        Args: \n",
    "            start_date, end_date: start & end date of the first two weeks of the simulation\n",
    "\n",
    "        Returns: n/a\n",
    "        \"\"\"        \n",
    "        # construct or update cm_finder.network \n",
    "        if not self.cm_network: \n",
    "            self.cm_network = temporalNetwork(self.start_date, self.end_date, display_progress, self.location_grouping, self.origin, self.facility_id, self.intermediate)\n",
    "            self.cm_network.construct_network_graph()\n",
    "            self.cm_finder = CM_Finder(location_grouping=self.location_grouping)\n",
    "        else: \n",
    "            self.cm_network.move_to_next_week()\n",
    "\n",
    "        self.cm_finder.origin_location_list = self.cm_network.origin_location_list\n",
    "        self.cm_finder.network = self.cm_network.network\n",
    "        \n",
    "        # self.cm_finder.group_to_DiGraph(display_progress = display_progress)\n",
    "        self.weekly_graphs[self.end_date] = self.cm_finder.network\n",
    "\n",
    "    def temporal_query(self, start_date, temporal=True, looback = 7, number_of_weeks=None, termination_date = None, \\\n",
    "                    filter_key = \"load_count\", display_progress=False, display_path_info = False): \n",
    "        \"\"\"\n",
    "        Given a start date, run the simulation for number_of_weeks or until termination_date is reached.\n",
    "\n",
    "        Args: \n",
    "            start_date: start date of the first week of the simulation\n",
    "            temporal: if True, run the simulation for every two weeks, if False, run the simulation from start_date until end_date\n",
    "            number_of_weeks: number of weeks to run the simulation for\n",
    "            termination_date: date to stop the simulation\n",
    "            filter_key: key to filter the network on (load_count or path_score)\n",
    "            display_progress: if True, display progress bar\n",
    "            display_path_info: if True, display path info\n",
    "\n",
    "        Returns: a dictionary of simulation result for each week\n",
    "            \n",
    "        Note:\n",
    "        * termination_date: termination date of the entire analysis, when end_date reaches termination_date, the query loop terminates,\n",
    "        * end_date: the end date of the two-week window, will be updated every week\n",
    "\n",
    "        - Once the parameters (location_grouping, origin, facility_id, intermediate) are used to initialise the cm_time class, \n",
    "          they will be used for any further analysis until new initialisation happens.\n",
    "          query_weekly method will only perform analysis, no alterations can be made by calling solely this.\n",
    "\n",
    "        - If temporal=True, --> end_date != termination_date, eventually at the end of simulations, end_date = termination_date\n",
    "            and number_of_weeks is given, end_date = start_date + 13 days for the first simulation, termination_date = start_date + 7 days * number_of_weeks\n",
    "            and termination_date is given, end_date = start_date + 13 days for the first simulation and termination_date=termination_date for the simulation\n",
    "          If temporal=False --> end_date = termination_date \n",
    "            and number_of_weeks is given, end_date = start_date + 7 days * number_of_weeks for the simulation \n",
    "            and termination_date is given, end_date=termination_date for the simulation\n",
    "\n",
    "        * simulation_results: a dictionary with key as the end_date and value as the simulation result\n",
    "        \"\"\"\n",
    "\n",
    "        # create information needed for a new query with the given start_date and number_of_weeks\n",
    "        # possible bug when end_date > termination_date.\n",
    "        if temporal: \n",
    "            self.start_date, self.end_date = start_date, start_date + timedelta(days=looback-1)\n",
    "\n",
    "            if number_of_weeks: termination_date = self.start_date + timedelta(days=7) * number_of_weeks\n",
    "            elif termination_date: termination_date = termination_date\n",
    "            else: raise Exception(\"Neither number of weeks nor termination date was given to set the simulation time period.\")\n",
    "        \n",
    "            # run simulation for every two weeks until termination_date\n",
    "            while self.end_date <= termination_date:    \n",
    "                self.construct_or_update_tg(filter_key=filter_key, \\\n",
    "                                    display_progress=display_progress, display_path_info=display_path_info)\n",
    "                self.update_dates() \n",
    "                \n",
    "        else: \n",
    "            if number_of_weeks: self.start_date, self.end_date = start_date, start_date + timedelta(days=7) * number_of_weeks\n",
    "            elif termination_date: self.start_date, self.end_date = start_date, termination_date\n",
    "            else: raise Exception(\"Neither number of weeks nor termination date was given to set the simulation time period.\")\n",
    "            self.run_single_simulation(filter_key=filter_key, \\\n",
    "                                display_progress=display_progress, display_path_info=display_path_info)\n",
    "\n",
    "        return self.weekly_graphs\n",
    "\n",
    "    def if_edge(node1, node2, curr_graph): \n",
    "        adjacency_matrix = curr_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k1, k2 = list(weekly_graphs.keys())[:2]\n",
    "# weekly_graphs[k1] == weekly_graphs[k2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_time = CM_Time(location_grouping = 'kma') \n",
    "weekly_kma_graphs = cm_time.temporal_query(start_date=datetime(2021,1,1).date(), looback=7, termination_date= datetime(2023,6, 1).date(), \\\n",
    "                    display_progress=False, display_path_info = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_time = CM_Time(location_grouping = 'zip3') \n",
    "weekly_zip3_graphs = cm_time.temporal_query(start_date=datetime(2021,1,1).date(), looback=7, termination_date= datetime(2023,6, 1).date(), \\\n",
    "                    display_progress=False, display_path_info = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operator_prod(u, v):\n",
    "    return u * v\n",
    "def operator_l2(u, v):\n",
    "    return (u - v) ** 2\n",
    "def operator_l1(u, v):\n",
    "    return abs(u - v)\n",
    "def operator_sub(u, v):\n",
    "    return (u - v)\n",
    "\n",
    "operators = [operator_prod, operator_l2, operator_l1, operator_sub]\n",
    "operator_names = [\"prod\", \"l2\", \"l1\", \"sub\"]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def all_edge_to_features(link_examples, transform_node):\n",
    "    features_per_operator = {} \n",
    "    for operator,operator_name in zip(operators, operator_names): \n",
    "        features_per_operator[operator_name] = edge_to_features_given_operator(link_examples, transform_node, operator)\n",
    "    return features_per_operator\n",
    "\n",
    "def edge_to_features_given_operator(link_examples, transform_node, operator):\n",
    "    return [\n",
    "        operator(transform_node(src), transform_node(dst)) for src, dst in link_examples\n",
    "    ]\n",
    "\n",
    "def edge_to_features_given_operator_name(link_examples, transform_node, operator_name):\n",
    "    operator = operators[operator_names.index(operator_name)]\n",
    "    return edge_to_features_given_operator(link_examples, transform_node, operator)\n",
    "\n",
    "def link_prediction_classifier(max_iter=2000):\n",
    "    lr_clf = LogisticRegressionCV(Cs=10, cv=10, scoring=\"roc_auc\", max_iter=max_iter, penalty=\"l2\") #, solver=\"liblinear\")\n",
    "    return Pipeline(steps=[(\"sc\", StandardScaler()), (\"clf\", lr_clf)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_model(graph, num_walks_per_node=10, walk_length = 10, context_window_size = 2): \n",
    "    num_cw = len(graph.nodes()) * num_walks_per_node * (walk_length - context_window_size + 1)\n",
    "    temporal_rw = TemporalRandomWalk(graph)\n",
    "    temporal_walks = temporal_rw.run(\n",
    "        num_cw=num_cw,\n",
    "        cw_size=context_window_size,\n",
    "        max_walk_length=walk_length,\n",
    "        walk_bias=\"exponential\",\n",
    "    )\n",
    "    \n",
    "    embedding_size = 128\n",
    "    temporal_model = Word2Vec(\n",
    "        temporal_walks,\n",
    "        vector_size=embedding_size,\n",
    "        window=context_window_size,\n",
    "        min_count=0,\n",
    "        sg=1,\n",
    "        workers=2,\n",
    "        epochs=1,)\n",
    "\n",
    "    unseen_node_embedding = np.zeros(embedding_size)\n",
    "\n",
    "    def temporal_embedding(u):\n",
    "        try:\n",
    "            return temporal_model.wv[u]\n",
    "        except KeyError:\n",
    "            return unseen_node_embedding\n",
    "    return temporal_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from collections import defaultdict, deque\n",
    "from scipy import stats\n",
    "from scipy.special import softmax\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.core.schema import GraphSchema\n",
    "from stellargraph.core.utils import is_real_iterable\n",
    "from stellargraph.core.experimental import experimental\n",
    "from stellargraph.random import random_state\n",
    "\n",
    "\n",
    "class GraphWalk(object):\n",
    "    \"\"\"\n",
    "    Base class for exploring graphs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph, temporal=True, graph_schema=None, seed=None):\n",
    "        self.graph = graph\n",
    "        self.temporal=temporal\n",
    "\n",
    "        # Initialize the random state\n",
    "        self._check_seed(seed)\n",
    "\n",
    "        self._random_state, self._np_random_state = random_state(seed)\n",
    "\n",
    "        # We require a StellarGraph for this\n",
    "        if not isinstance(graph, StellarGraph):\n",
    "            raise TypeError(\"Graph must be a StellarGraph or StellarDiGraph.\")\n",
    "\n",
    "        if not graph_schema:\n",
    "            self.graph_schema = self.graph.create_graph_schema()\n",
    "        else:\n",
    "            self.graph_schema = graph_schema\n",
    "\n",
    "        if type(self.graph_schema) is not GraphSchema:\n",
    "            self._raise_error(\n",
    "                \"The parameter graph_schema should be either None or of type GraphSchema.\"\n",
    "            )\n",
    "\n",
    "    def get_adjacency_types(self):\n",
    "        # Allow additional info for heterogeneous graphs.\n",
    "        adj = getattr(self, \"adj_types\", None)\n",
    "        if not adj:\n",
    "            # Create a dict of adjacency lists per edge type, for faster neighbour sampling from graph in SampledHeteroBFS:\n",
    "            self.adj_types = adj = self.graph._adjacency_types(self.graph_schema)\n",
    "        return adj\n",
    "\n",
    "    def _check_seed(self, seed):\n",
    "        if seed is not None:\n",
    "            if type(seed) != int:\n",
    "                self._raise_error(\n",
    "                    \"The random number generator seed value, seed, should be integer type or None.\"\n",
    "                )\n",
    "            if seed < 0:\n",
    "                self._raise_error(\n",
    "                    \"The random number generator seed value, seed, should be non-negative integer or None.\"\n",
    "                )\n",
    "\n",
    "    def _get_random_state(self, seed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seed: The optional seed value for a given run.\n",
    "\n",
    "        Returns:\n",
    "            The random state as determined by the seed.\n",
    "        \"\"\"\n",
    "        if seed is None:\n",
    "            # Restore the random state\n",
    "            return self._random_state\n",
    "        # seed the random number generator\n",
    "        rs, _ = random_state(seed)\n",
    "        return rs\n",
    "\n",
    "    def neighbors(self, node):\n",
    "        if not self.graph.has_node(node):\n",
    "            self._raise_error(\"node {} not in graph\".format(node))\n",
    "        return self.graph.neighbors(node)\n",
    "\n",
    "    def run(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        To be overridden by subclasses. It is the main entry point for performing random walks on the given\n",
    "        graph.\n",
    "\n",
    "        It should return the sequences of nodes in each random walk.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _raise_error(self, msg):\n",
    "        raise ValueError(\"({}) {}\".format(type(self).__name__, msg))\n",
    "\n",
    "    def _check_common_parameters(self, nodes, n, length, seed):\n",
    "        \"\"\"\n",
    "        Checks that the parameter values are valid or raises ValueError exceptions with a message indicating the\n",
    "        parameter (the first one encountered in the checks) with invalid value.\n",
    "\n",
    "        Args:\n",
    "            nodes: <list> A list of root node ids from which to commence the random walks.\n",
    "            n: <int> Number of walks per node id.\n",
    "            length: <int> Maximum length of each walk.\n",
    "            seed: <int> Random number generator seed.\n",
    "        \"\"\"\n",
    "        self._check_nodes(nodes)\n",
    "        self._check_repetitions(n)\n",
    "        self._check_length(length)\n",
    "        self._check_seed(seed)\n",
    "\n",
    "    def _check_nodes(self, nodes):\n",
    "        if nodes is None:\n",
    "            self._raise_error(\"A list of root node IDs was not provided.\")\n",
    "        if not is_real_iterable(nodes):\n",
    "            self._raise_error(\"Nodes parameter should be an iterable of node IDs.\")\n",
    "        if (\n",
    "            len(nodes) == 0\n",
    "        ):  # this is not an error but maybe a warning should be printed to inform the caller\n",
    "            warnings.warn(\n",
    "                \"No root node IDs given. An empty list will be returned as a result.\",\n",
    "                RuntimeWarning,\n",
    "                stacklevel=3,\n",
    "            )\n",
    "\n",
    "    def _check_repetitions(self, n):\n",
    "        if type(n) != int:\n",
    "            self._raise_error(\n",
    "                \"The number of walks per root node, n, should be integer type.\"\n",
    "            )\n",
    "        if n <= 0:\n",
    "            self._raise_error(\n",
    "                \"The number of walks per root node, n, should be a positive integer.\"\n",
    "            )\n",
    "\n",
    "    def _check_length(self, length):\n",
    "        if type(length) != int:\n",
    "            self._raise_error(\"The walk length, length, should be integer type.\")\n",
    "        if length <= 0:\n",
    "            # Technically, length 0 should be okay, but by consensus is invalid.\n",
    "            self._raise_error(\"The walk length, length, should be a positive integer.\")\n",
    "\n",
    "    # For neighbourhood sampling\n",
    "    def _check_sizes(self, n_size):\n",
    "        err_msg = \"The neighbourhood size must be a list of non-negative integers.\"\n",
    "        if not isinstance(n_size, list):\n",
    "            self._raise_error(err_msg)\n",
    "        if len(n_size) == 0:\n",
    "            # Technically, length 0 should be okay, but by consensus it is invalid.\n",
    "            self._raise_error(\"The neighbourhood size list should not be empty.\")\n",
    "        for d in n_size:\n",
    "            if type(d) != int or d < 0:\n",
    "                self._raise_error(err_msg)\n",
    "\n",
    "def naive_weighted_choices(rs, weights):\n",
    "    \"\"\"\n",
    "    Select an index at random, weighted by the iterator `weights` of\n",
    "    arbitrary (non-negative) floats. That is, `x` will be returned\n",
    "    with probability `weights[x]/sum(weights)`.\n",
    "\n",
    "    For doing a single sample with arbitrary weights, this is much (5x\n",
    "    or more) faster than numpy.random.choice, because the latter\n",
    "    requires a lot of preprocessing (normalized probabilties), and\n",
    "    does a lot of conversions/checks/preprocessing internally.\n",
    "    \"\"\"\n",
    "\n",
    "    # divide the interval [0, sum(weights)) into len(weights)\n",
    "    # subintervals [x_i, x_{i+1}), where the width x_{i+1} - x_i ==\n",
    "    # weights[i]\n",
    "    subinterval_ends = []\n",
    "    running_total = 0\n",
    "    for w in weights:\n",
    "        if w < 0:\n",
    "            raise ValueError(\"Detected negative weight: {}\".format(w))\n",
    "        running_total += w\n",
    "        subinterval_ends.append(running_total)\n",
    "\n",
    "    # pick a place in the overall interval\n",
    "    x = rs.random() * running_total\n",
    "\n",
    "    # find the subinterval that contains the place, by looking for the\n",
    "    # first subinterval where the end is (strictly) after it\n",
    "    for idx, end in enumerate(subinterval_ends):\n",
    "        if x < end:\n",
    "            break\n",
    "\n",
    "    return idx\n",
    "\n",
    "class NS_weighted_RandomWalk(GraphWalk):\n",
    "    \"\"\"\n",
    "    Performs temporal random walks on the given graph. The graph should contain numerical edge\n",
    "    weights that correspond to the time at which the edge was created. Exact units are not relevant\n",
    "    for the algorithm, only the relative differences (e.g. seconds, days, etc).\n",
    "    \"\"\"\n",
    "    def run(\n",
    "        self,\n",
    "        num_cw,\n",
    "        cw_size,\n",
    "        max_walk_length=80,\n",
    "        initial_edge_bias=None,\n",
    "        walk_bias=None,\n",
    "        p_walk_success_threshold=0.01,\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform a time respecting random walk starting from randomly selected temporal edges.\n",
    "\n",
    "        Args:\n",
    "            num_cw (int): Total number of context windows to generate. For comparable\n",
    "                results to most other random walks, this should be a multiple of the number\n",
    "                of nodes in the graph.\n",
    "            cw_size (int): Size of context window. Also used as the minimum walk length,\n",
    "                since a walk must generate at least 1 context window for it to be useful.\n",
    "            max_walk_length (int): Maximum length of each random walk. Should be greater\n",
    "                than or equal to the context window size.\n",
    "            initial_edge_bias (str, optional): Distribution to use when choosing a random\n",
    "                initial temporal edge to start from. Available options are:\n",
    "\n",
    "                * None (default) - The initial edge is picked from a uniform distribution.\n",
    "                * \"exponential\" - Heavily biased towards more recent edges.\n",
    "\n",
    "            walk_bias (str, optional): Distribution to use when choosing a random\n",
    "                neighbour to walk through. Available options are:\n",
    "\n",
    "                * None (default) - Neighbours are picked from a uniform distribution.\n",
    "                * \"exponential\" - Exponentially decaying probability, resulting in a bias towards shorter time gaps.\n",
    "\n",
    "            p_walk_success_threshold (float): Lower bound for the proportion of successful\n",
    "                (i.e. longer than minimum length) walks. If the 95% percentile of the\n",
    "                estimated proportion is less than the provided threshold, a RuntimeError\n",
    "                will be raised. The default value of 0.01 means an error is raised if less than 1%\n",
    "                of the attempted random walks are successful. This parameter exists to catch any\n",
    "                potential situation where too many unsuccessful walks can cause an infinite or very\n",
    "                slow loop.\n",
    "            seed (int, optional): Random number generator seed; default is None.\n",
    "\n",
    "        Returns:\n",
    "            List of lists of node ids for each of the random walks.\n",
    "\n",
    "        \"\"\"\n",
    "        if cw_size < 2:\n",
    "            raise ValueError(\n",
    "                f\"cw_size: context window size should be greater than 1, found {cw_size}\"\n",
    "            )\n",
    "        if max_walk_length < cw_size:\n",
    "            raise ValueError(\n",
    "                f\"max_walk_length: maximum walk length should not be less than the context window size, found {max_walk_length}\"\n",
    "            )\n",
    "\n",
    "        np_rs = self._np_random_state if seed is None else np.random.RandomState(seed)\n",
    "        walks = []\n",
    "        num_cw_curr = 0\n",
    "\n",
    "        edges, times = self.graph.edges(include_edge_weight=True)\n",
    "        edge_biases = self._temporal_biases(\n",
    "            times, None, bias_type=initial_edge_bias, is_forward=False,\n",
    "        )\n",
    "\n",
    "        successes = 0\n",
    "        failures = 0\n",
    "\n",
    "        def not_progressing_enough():\n",
    "            # Estimate the probability p of a walk being long enough; the 95% percentile is used to\n",
    "            # be more stable with respect to randomness. This uses Beta(1, 1) as the prior, since\n",
    "            # it's uniform on p\n",
    "            posterior = stats.beta.ppf(0.95, 1 + successes, 1 + failures)\n",
    "            return posterior < p_walk_success_threshold\n",
    "\n",
    "        # loop runs until we have enough context windows in total\n",
    "        while num_cw_curr < num_cw:\n",
    "            first_edge_index = self._sample(len(edges), edge_biases, np_rs)\n",
    "            src, dst = edges[first_edge_index]\n",
    "            t = times[first_edge_index]\n",
    "\n",
    "            remaining_length = num_cw - num_cw_curr + cw_size - 1\n",
    "\n",
    "            walk = self._walk(\n",
    "                src, dst, t, min(max_walk_length, remaining_length), walk_bias, np_rs\n",
    "            )\n",
    "            if len(walk) >= cw_size:\n",
    "                walks.append(walk)\n",
    "                num_cw_curr += len(walk) - cw_size + 1\n",
    "                successes += 1\n",
    "            else:\n",
    "                failures += 1\n",
    "                if not_progressing_enough():\n",
    "                    raise RuntimeError(\n",
    "                        f\"Discarded {failures} walks out of {failures + successes}. \"\n",
    "                        \"Too many temporal walks are being discarded for being too short. \"\n",
    "                        f\"Consider using a smaller context window size (currently cw_size={cw_size}).\"\n",
    "                    )\n",
    "\n",
    "        return walks\n",
    "\n",
    "\n",
    "    def _sample(self, n, biases, np_rs):\n",
    "        if biases is not None:\n",
    "            assert len(biases) == n\n",
    "            return naive_weighted_choices(np_rs, biases)\n",
    "        else:\n",
    "            return np_rs.choice(n)\n",
    "\n",
    "    def _exp_biases(self, times, t_0, decay):\n",
    "        # t_0 assumed to be smaller than all time values\n",
    "        return softmax(t_0 - np.array(times) if decay else np.array(times) - t_0)\n",
    "\n",
    "    def _temporal_biases(self, times, time, bias_type, is_forward):\n",
    "        if bias_type is None:\n",
    "            # default to uniform random sampling\n",
    "            return None\n",
    "\n",
    "        # time is None indicates we should obtain the minimum available time for t_0\n",
    "        t_0 = time if time is not None else min(times)\n",
    "\n",
    "        if bias_type == \"exponential\":\n",
    "            # exponential decay bias needs to be reversed if looking backwards in time\n",
    "            return self._exp_biases(times, t_0, decay=is_forward)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported bias type\")\n",
    "\n",
    "    def _step(self, node, time, bias_type, np_rs):\n",
    "        \"\"\"\n",
    "        Perform 1 temporal step from a node. Returns None if a dead-end is reached.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.temporal: \n",
    "            neighbours = [\n",
    "                (neighbour, t)\n",
    "                for neighbour, t in self.graph.neighbors(node, include_edge_weight=True)\n",
    "                if t > time\n",
    "            ]\n",
    "        else: \n",
    "            neighbours = [\n",
    "                (neighbour, t)\n",
    "                for neighbour, t in self.graph.neighbors(node, include_edge_weight=True)\n",
    "            ]\n",
    "        \n",
    "        def compute_jc(u_neighbours, v):\n",
    "            # print(StellarGraph.neighbor_arrays(self.graph, v))\n",
    "            v_neighbours = set(StellarGraph.neighbor_arrays(self.graph, v))\n",
    "            union_size = len(u_neighbours.union(v_neighbours))\n",
    "            if union_size == 0:\n",
    "                return 0\n",
    "            return len(u_neighbours.intersection(v_neighbours)) / union_size\n",
    "        node_degrees = self.graph.node_degrees()\n",
    "\n",
    "        if neighbours:\n",
    "            times = [t for _, t in neighbours]\n",
    "            biases = []\n",
    "            node_degree = node_degrees[node]\n",
    "            u_neighbours = set(StellarGraph.neighbor_arrays(self.graph, node))\n",
    "            for ngh, t in neighbours: #G.neighbors(node):\n",
    "                # print(ngh)\n",
    "                pval=compute_jc(u_neighbours, ngh) + 1.0/node_degree\n",
    "                biases.append(pval)\n",
    "                 \n",
    "            # biases = self._temporal_biases(times, time, bias_type, is_forward=True)\n",
    "            if not len(biases): biases = None \n",
    "            # print(len(neighbours), )\n",
    "            chosen_neighbour_index = self._sample(len(neighbours), biases, np_rs)\n",
    "            next_node, next_time = neighbours[chosen_neighbour_index]\n",
    "            return next_node, next_time\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _walk(self, src, dst, t, length, bias_type, np_rs):\n",
    "        walk = [src, dst]\n",
    "        node, time = dst, t\n",
    "        for _ in range(length - 2):\n",
    "            result = self._step(node, time=time, bias_type=bias_type, np_rs=np_rs)\n",
    "\n",
    "            if result is not None:\n",
    "                node, time = result\n",
    "                walk.append(node)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return walk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class nodesim_static_walk():\n",
    "\tdef __init__(self, graph):\n",
    "\t\tprint(graph)\n",
    "\t\tself.G = graph\n",
    "\t\t\n",
    "\tdef nodesim_walk(self, walk_length, start_node):\n",
    "\t\t'''\n",
    "\t\tSimulate nodesim random walk starting from a given node.\n",
    "\t\t'''\n",
    "\t\tG = self.G\n",
    "\t\tprobabilities=self.probabilities\n",
    "\t\tneighbors=self.neighbors\n",
    "\t\twalk = [start_node]\n",
    "\t\twhile len(walk) < walk_length:\n",
    "\t\t\tcur = walk[-1]\n",
    "\t\t\tnextnode=random.choices(list(neighbors[cur]), list(probabilities[cur]))[0]\n",
    "\t\t\twalk.append(nextnode)\n",
    "\t\treturn walk\n",
    "\n",
    "\tdef simulate_walks(self, num_walks, walk_length):\n",
    "\t\t'''\n",
    "\t\tRepeatedly run random walks from each node.\n",
    "\t\t'''\n",
    "\t\tG = self.G\n",
    "\t\twalks = []\n",
    "\t\tnodes = list(G.nodes())\n",
    "\t\tprint('Walk iteration:')\n",
    "\t\tfor walk_iter in range(num_walks):\n",
    "\t\t\tprint(str(walk_iter+1), '/', str(num_walks))\n",
    "\t\t\trandom.shuffle(nodes)\n",
    "\t\t\tfor node in nodes:\n",
    "\t\t\t\twalks.append(self.nodesim_walk(walk_length=walk_length, start_node=node))\n",
    "\t\treturn walks\n",
    "\n",
    "\tdef compute_edge_probs(self):\n",
    "\t\t'''\n",
    "\t\tCompute transition probabilities for nodesim random walks.\n",
    "\t\t'''\n",
    "\n",
    "\t\tdef compute_jc(u_neighbours, v):\n",
    "            # print(StellarGraph.neighbor_arrays(self.graph, v))\n",
    "\t\t\tv_neighbours = set(StellarGraph.neighbor_arrays(self.G, v))\n",
    "\t\t\tunion_size = len(u_neighbours.union(v_neighbours))\n",
    "\t\t\tif union_size == 0: return 0\n",
    "\t\t\treturn len(u_neighbours.intersection(v_neighbours)) / union_size\n",
    "\n",
    "\t\tG = self.G\n",
    "\t\tnode_degrees = G.node_degrees()\n",
    "\t\t\n",
    "\t\tprobs={}\n",
    "\t\tnghs={}\n",
    "\t\tnode_degrees = G.node_degrees()\n",
    "\t\tfor node in G.nodes():\n",
    "\t\t\tnghbrs=[]\n",
    "\t\t\tpr=[]\n",
    "\t\t\tnode_degree = node_degrees[node]\n",
    "\t\t\tu_neighbours = set(StellarGraph.neighbor_arrays(G, node))\n",
    "\t\t\tfor ngh in G.neighbors(node):\n",
    "\t\t\t\tnghbrs.append(ngh)\n",
    "\t\t\t\tpval=compute_jc(u_neighbours, ngh) + 1.0/node_degree\n",
    "\t\t\t\tpr.append(pval)\n",
    "\t\t\t\t\n",
    "\t\t\ts=sum(pr)\n",
    "\t\t\tpr=[x / s for x in pr]\t\t\n",
    "\t\t\tprobs[node]=pr\n",
    "\t\t\tnghs[node]=nghbrs\n",
    "\t\t\t\t\n",
    "\t\tself.probabilities=probs\n",
    "\t\tself.neighbors=nghs\n",
    "\t\treturn 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(graph, prediction_window_size=2, test_neg_size=None): \n",
    "    # identify first 5 & last 2 days' edges based on dates\n",
    "    graph_with_negative_edges = graph.copy()\n",
    "    graph = StellarGraph.from_networkx(graph, edge_weight_attr='time', edge_type_attr='directed')\n",
    "    edges, weights = np.array(graph.edges(include_edge_weight=True)[0]), np.array(graph.edges(include_edge_weight=True)[1])\n",
    "    lower_lim = sorted(list(set(weights)))[-prediction_window_size]\n",
    "    index_test, index_train = np.where(weights >= lower_lim)[0], np.where(weights < lower_lim)[0]\n",
    "\n",
    "    # create test & train edge sets\n",
    "    test_pos_edges, test_time_labels = edges[index_test], weights[index_test]\n",
    "    train_edges, train_time_labels = edges[index_train], weights[index_train]\n",
    "    # test_pos_edges_with_time_attribute = np.rec.fromarrays([test_pos_edges[:,0], test_pos_edges[:,1], test_time_labels])\n",
    "    train_edges_with_time_attribute = np.rec.fromarrays([train_edges[:,0], train_edges[:,1], train_time_labels])\n",
    "\n",
    "    # create test and train graph \n",
    "    train_graph = nx.MultiDiGraph()\n",
    "    train_graph.add_weighted_edges_from(train_edges_with_time_attribute,weight='time') \n",
    "\n",
    "    # create pos & neg edges for test graph \n",
    "    if not test_neg_size: \n",
    "        n = len(test_pos_edges) \n",
    "    else: \n",
    "        n = len(test_pos_edges) * test_neg_size\n",
    "    test_neg_edges = list(nx.non_edges(train_graph)) #take the first x number of non edges\n",
    "    test_pos_edges_tuple = [tuple(x) for x in test_pos_edges]\n",
    "    test_neg_edges = np.array(list(set(test_neg_edges) - set(test_pos_edges_tuple)))[:n]\n",
    "    test_edges = np.vstack((test_pos_edges, test_neg_edges))\n",
    "    test_labels = np.hstack((np.ones(len(test_pos_edges)), np.zeros(len(test_neg_edges))))\n",
    "    \n",
    "    # to make sure there is no negative edge in the train graph that are in the test graph\n",
    "    graph_with_negative_edges.add_edges_from(test_neg_edges)\n",
    "\n",
    "    ## create train & validating edge sets from train graph \n",
    "    train_graph = StellarGraph.from_networkx(train_graph, edge_weight_attr='time', edge_type_attr='directed')\n",
    "    edge_splitter_train = EdgeSplitter(train_graph, graph_with_negative_edges)\n",
    "    graph_train, examples, labels = edge_splitter_train.train_test_split(\n",
    "        p=0.1, method=\"global\"\n",
    "    )\n",
    "    (\n",
    "        examples_train,\n",
    "        examples_validate, \n",
    "        labels_train,\n",
    "        labels_validate\n",
    "    ) = train_test_split(examples, labels, train_size=0.7, test_size=0.3)\n",
    "\n",
    "    \n",
    "    return test_edges, test_labels, graph_train, examples_train, labels_train, examples_validate, labels_validate\n",
    "\n",
    "def random_walk_model(graph, num_walks_per_node=10, walk_length = 10, context_window_size = 2, nodesim=False, temporal=True): \n",
    "    num_cw = len(graph.nodes()) * num_walks_per_node * (walk_length - context_window_size + 1)\n",
    "    if temporal:\n",
    "        if nodesim: \n",
    "            rw_model = NS_weighted_RandomWalk(graph)\n",
    "        else:      \n",
    "            rw_model = TemporalRandomWalk(graph)\n",
    "        walks = rw_model.run(\n",
    "        num_cw=num_cw,\n",
    "        cw_size=context_window_size,\n",
    "        max_walk_length=walk_length,\n",
    "        walk_bias=\"exponential\",\n",
    "        )\n",
    "\n",
    "    else: \n",
    "        if nodesim: \n",
    "            rw_model = NS_weighted_RandomWalk(graph, temporal=False)\n",
    "            walks = rw_model.run(\n",
    "            num_cw=num_cw,\n",
    "            cw_size=context_window_size,\n",
    "            max_walk_length=walk_length,\n",
    "            walk_bias=\"exponential\",\n",
    "            )\n",
    "\n",
    "        else: \n",
    "            rw_model = BiasedRandomWalk(graph)\n",
    "            walks = rw_model.run(\n",
    "            nodes=graph.nodes(), n=num_walks_per_node, length=walk_length\n",
    "            )\n",
    "    \n",
    "    embedding_size = 128\n",
    "    node_embedding = Word2Vec(\n",
    "        walks,\n",
    "        vector_size=embedding_size,\n",
    "        window=context_window_size,\n",
    "        min_count=0,\n",
    "        sg=1,\n",
    "        workers=2,\n",
    "        epochs=1,)\n",
    "\n",
    "    unseen_node_embedding = np.zeros(embedding_size)\n",
    "\n",
    "    def get_node_embedding(u):\n",
    "        try:\n",
    "            return node_embedding.wv[u]\n",
    "        except KeyError:\n",
    "            return unseen_node_embedding\n",
    "    return get_node_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class WeeklyDecisionMaker():\n",
    "    def __init__(self, num_walks_per_node=10, walk_length = 10, context_window_size=2, expert_num=10, reward=False):\n",
    "        # needed for temporal embedding\n",
    "        self.num_walks_per_node = num_walks_per_node\n",
    "        self.walk_length = walk_length\n",
    "        self.context_window_size = context_window_size\n",
    "\n",
    "        # weight optimization\n",
    "        self.reward = reward\n",
    "        self.naive_regret = 0\n",
    "        self.past_naive_regrets = defaultdict(list)\n",
    "        self.graph = None \n",
    "        self.expert_num = expert_num\n",
    "        self.hedge_total_regret = [0]\n",
    "        self.naive_total_regret = [0]\n",
    "\n",
    "        # record purposes \n",
    "        self.test_edges = []\n",
    "        self.test_edge_labels = []\n",
    "        # self.curr_false_positive_set = set()\n",
    "        self.validation_scores = {'temporal': defaultdict(list), 'static': defaultdict(list), 'nodesim_temporal': defaultdict(list), 'nodesim_static': defaultdict(list)}\n",
    "        self.test_scores = defaultdict(list) #[]\n",
    "        self.predicted_probs = [] \n",
    "        self.weight_vectors= defaultdict(list)\n",
    "\n",
    "    def initialize_dictionaries(self, edge_tuple): \n",
    "        self.weight_vectors[edge_tuple] = [1/self.expert_num] * self.expert_num\n",
    "        \n",
    "    def fit_classifier(self, embeddings, labels):\n",
    "        classifier = link_prediction_classifier()\n",
    "        classifier.fit(embeddings, labels)\n",
    "        return classifier\n",
    "\n",
    "    def evaluate_score(self, clf, link_features, link_labels, threshold= 0.5, return_idces=False):\n",
    "        self.predicted_probs = clf.predict_proba(link_features)\n",
    "        positive_column = list(clf.classes_).index(1)\n",
    "        if return_idces: \n",
    "            false_positive_idces = np.where((link_labels == 0) & (self.predicted_probs[:, positive_column] > threshold))[0]\n",
    "            # true_positive_idces = np.where((link_labels == 1) & (predicted[:, positive_column] > 0.5))[0]\n",
    "            return roc_auc_score(link_labels, self.predicted_probs[:, positive_column]), false_positive_idces #, true_positive_idces\n",
    "        return roc_auc_score(link_labels, self.predicted_probs[:, positive_column])\n",
    "\n",
    "    def update_curr_false_positive(self, new_edge_set, display_progress=False): \n",
    "        not_false_positive_anymore = self.curr_false_positive_set.intersection(new_edge_set)\n",
    "        if display_progress: print(f\"Not false positive anymore: {len(not_false_positive_anymore)}\")\n",
    "        if not_false_positive_anymore: \n",
    "            self.naive_regret -= len(not_false_positive_anymore)\n",
    "            self.curr_false_positive_set = self.curr_false_positive_set - not_false_positive_anymore\n",
    "            if display_progress: print(f\"New Reduced Regret: {len(self.curr_false_positive_set)}\")            \n",
    "\n",
    "    def predict_probs(self, graph, display_progress=False, temporal=True): \n",
    "        # update regret by checking if they are in the new graph just given  \n",
    "        # self.update_curr_false_positive(set(graph.edges()), display_progress=display_progress)\n",
    "\n",
    "        # convert networkx graph to stellargraph & split data\n",
    "        # graph = StellarGraph.from_networkx(graph, edge_weight_attr='time', edge_type_attr='directed')\n",
    "        if display_progress: print(\"Splitting data...\")\n",
    "        links_test, labels_test, train_graph, links_train, labels_train, links_validate, labels_validate  = data_split(graph)\n",
    "        self.test_edges, self.test_edge_labels = links_test, labels_test\n",
    "\n",
    "        # fit & learn \n",
    "        if display_progress: print(\"Computing node embeddings...\")\n",
    "        nodesim_static_embedding = random_walk_model(train_graph,\n",
    "                            num_walks_per_node=self.num_walks_per_node, walk_length=self.walk_length, \\\n",
    "                            context_window_size=self.context_window_size, nodesim=True, temporal= False)\n",
    "        nodesim_temporal_embedding = random_walk_model(train_graph,\n",
    "                            num_walks_per_node=self.num_walks_per_node, walk_length=self.walk_length, \\\n",
    "                            context_window_size=self.context_window_size, nodesim=True, temporal= True)\n",
    "        static_embedding = random_walk_model(train_graph,\n",
    "                            num_walks_per_node=self.num_walks_per_node, walk_length=self.walk_length, \\\n",
    "                            context_window_size=self.context_window_size, temporal= False)\n",
    "        temporal_embedding = random_walk_model(train_graph, \n",
    "                            num_walks_per_node=self.num_walks_per_node, walk_length=self.walk_length, \\\n",
    "                            context_window_size=self.context_window_size, temporal= True)\n",
    "       \n",
    "        # get edge embeddings for all edges\n",
    "        if display_progress: print(\"Getting edge embeddings...\")\n",
    "        embedded_features = {'temporal': defaultdict(list), 'static': defaultdict(list), 'nodesim_temporal': defaultdict(list), 'nodesim_static': defaultdict(list)}\n",
    "        for links in [links_train, links_validate]:\n",
    "            for operator,operator_name in zip(operators, operator_names): \n",
    "                temporal_edge_embedding = edge_to_features_given_operator(links, temporal_embedding, operator)\n",
    "                static_edge_embedding = edge_to_features_given_operator(links, static_embedding, operator)\n",
    "                nodesim_temporal_edge_embedding = edge_to_features_given_operator(links, nodesim_temporal_embedding, operator)\n",
    "                nodesim_static_edge_embedding = edge_to_features_given_operator(links, nodesim_static_embedding, operator)\n",
    "                embedded_features['temporal'][operator_name].append(temporal_edge_embedding)\n",
    "                embedded_features['static'][operator_name].append(static_edge_embedding)\n",
    "                embedded_features['nodesim_temporal'][operator_name].append(nodesim_temporal_edge_embedding)\n",
    "                embedded_features['nodesim_static'][operator_name].append(nodesim_static_edge_embedding)\n",
    "                \n",
    "        # temporal_edge_embedding_train_per_operator = edge_to_features(links_validate, temporal_embedding)\n",
    "        \n",
    "        fitted_classifier_per_operator = {}\n",
    "        \n",
    "        for embedding_type in ['nodesim_temporal', 'nodesim_static', 'temporal', 'static']: \n",
    "            for operator_name in operator_names:\n",
    "                if display_progress: print(f\"Fitting classifiers & Evaluating scores ... for {operator_name} .. \")\n",
    "                fitted_classifier_per_operator[operator_name] = self.fit_classifier(embedded_features[embedding_type][operator_name][0], labels_train) #fit classifier\n",
    "                validation_score_per_operator = self.evaluate_score(fitted_classifier_per_operator[operator_name], embedded_features[embedding_type][operator_name][1], labels_validate)\n",
    "                self.validation_scores[embedding_type][operator_name].append(validation_score_per_operator)\n",
    "                if display_progress:  print(f\"{embedding_type} Validation Score (ROC AUC): {validation_score_per_operator:.2f}\")\n",
    "\n",
    "            # choose the best classifier based on validation score\n",
    "            if display_progress: print(\"Choosing the best classifier based on validation score ...\")\n",
    "            best_operator = operator_names[np.argmax([self.validation_scores[embedding_type][operator_name][-1] for operator_name in operator_names])]\n",
    "            if display_progress: print(f\"Best Operator: {best_operator}\")\n",
    "\n",
    "            # test the best classifier on test set\n",
    "            test_embedding = None\n",
    "            if embedding_type == 'temporal':\n",
    "                test_embedding = edge_to_features_given_operator_name(links_test, temporal_embedding, best_operator)\n",
    "            elif embedding_type == 'static':\n",
    "                test_embedding = edge_to_features_given_operator_name(links_test, static_embedding, best_operator)\n",
    "            elif embedding_type == 'nodesim_temporal':\n",
    "                test_embedding = edge_to_features_given_operator_name(links_test, nodesim_temporal_embedding, best_operator)\n",
    "            else: \n",
    "                test_embedding = edge_to_features_given_operator_name(links_test, nodesim_static_embedding, best_operator)\n",
    "\n",
    "            test_score = self.evaluate_score(fitted_classifier_per_operator[operator_name], test_embedding, labels_test)\n",
    "            self.test_scores[embedding_type].append(test_score)\n",
    "            if display_progress: \n",
    "                print(f\"{embedding_type} Test Score (ROC AUC): {test_score:.2f}\\n\")\n",
    "        \n",
    "    def optimize_weights_for_edge(self, learning_rate=0.1, leader_thresholds = np.arange(0.1, 1.1, 0.1)): \n",
    "        self.hedge_total_regret.append(self.hedge_total_regret[-1])\n",
    "        self.naive_total_regret.append(self.naive_total_regret[-1])\n",
    "\n",
    "        # update regret for all test edges, keep track of weight vectors per edge \n",
    "        for edge_index, edge in enumerate(self.test_edges): \n",
    "            edge_tuple = tuple(edge)\n",
    "            if edge_tuple not in self.weight_vectors.keys(): \n",
    "                self.initialize_dictionaries(edge_tuple)\n",
    "            edge_existence = self.test_edge_labels[edge_index]\n",
    "            edge_existence_prob = self.predicted_probs[edge_index,1] \n",
    "            self.hedge_per_edge(edge_tuple, edge_existence_prob, edge_existence, learning_rate, leader_thresholds)\n",
    "\n",
    "    def hedge_per_edge(self, edge_tuple, edge_existence_prob, edge_existence, learning_rate = 0.1, leader_thresholds = np.arange(0.1, 1.1, 0.1)):\n",
    "        wait_leaders = [1 if edge_existence_prob > threshold else 0 for threshold in leader_thresholds]\n",
    "        not_wait_leaders = [1 if edge_existence_prob < threshold else 0 for threshold in leader_thresholds]\n",
    "        weight_vector = self.weight_vectors[edge_tuple]\n",
    "\n",
    "        # make decision based on each leader's recommendation & weight vector\n",
    "        final_wait_decision = True if np.dot(wait_leaders, weight_vector) > np.dot(not_wait_leaders, weight_vector) else False \n",
    "        naive_decision = True if edge_existence_prob > 0.5 else False\n",
    "\n",
    "        # record regret by checking if they are in the new graph & update weight \n",
    "        leader_made_wrong_decision = [1 if wait_decision != edge_existence  else 0 for wait_decision in wait_leaders]\n",
    "        loss = 0\n",
    "        if final_wait_decision != edge_existence:\n",
    "            if final_wait_decision == 1: \n",
    "                loss = 3 #false positive treated most harshly \n",
    "            else: loss = 1 #false negative treated less harshly\n",
    "        elif self.reward: \n",
    "            leader_made_wrong_decision = [-1 * wrong_decision for wrong_decision in leader_made_wrong_decision]\n",
    "            if final_wait_decision == 1: \n",
    "                loss = -2 #true positive rewarded most  \n",
    "            else: loss = -1 #true negative rewarded second most\n",
    "\n",
    "        weight_vector = [weight_i * np.exp(-learning_rate * loss * wrong_decision) for wrong_decision, weight_i in zip(leader_made_wrong_decision, weight_vector)]        \n",
    "        weight_vector = weight_vector/ np.sum(weight_vector) # renormalize weight_vector \n",
    "        self.weight_vectors[edge_tuple] = weight_vector\n",
    "        self.hedge_total_regret[-1] += loss\n",
    "        \n",
    "        # update naive regret \n",
    "        curr_naive_loss = 0\n",
    "        if naive_decision != edge_existence: \n",
    "            if final_wait_decision == 1: \n",
    "                curr_naive_loss = 3 #false positive treated most harshly \n",
    "            else: curr_naive_loss = 1 #false negative treated less harshly\n",
    "        \n",
    "        elif self.reward: \n",
    "            if naive_decision == 1: \n",
    "                curr_naive_loss = -2 #true positive rewarded most\n",
    "            else: curr_naive_loss = -1 #true negative rewarded second most\n",
    "\n",
    "        self.naive_total_regret[-1] += curr_naive_loss\n",
    "\n",
    "    def run_expert_simulation(self, graph, learning_rate=0.1, temporal=True, display_progress=False):\n",
    "        self.predict_probs(graph, display_progress=display_progress, temporal=temporal)\n",
    "        self.optimize_weights_for_edge(learning_rate=learning_rate)\n",
    "        if display_progress: self.print_info_given_week(graph.graph['end_date'])\n",
    "        \n",
    "    def print_info_given_week(self, curr_time): \n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Current Week: {curr_time}\")\n",
    "        # print(f\"training error: {np.round(self.train_errors[-1],2)}, test error: {np.round(self.test_errors[-1],2)}\")\n",
    "        print(f\"Total Number of Test Edges: {len(self.test_edges)}\")\n",
    "        print(f\"Current Expert Regret: { self.hedge_total_regret[-1]}\")\n",
    "        print(f\"Current Naive Regret: {self.naive_total_regret[-1]}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "    def graph_train_test_errors(self, time_range, graph_errors = True, graph_regret = False, title=None): \n",
    "        n = min(len(self.train_errors), len(self.test_errors))\n",
    "        if title: plt.title(title)\n",
    "        if graph_errors: \n",
    "            # plt.plot(time_range[:n], self.train_errors[:n], label='train')\n",
    "            plt.plot(time_range[:n], self.test_errors[:n], label='test')\n",
    "        if graph_regret: \n",
    "            plt.plot(time_range[:n], self.curr_regret[:n], label='regret')\n",
    "        plt.legend() \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "n = len(zip3_comparison.validation_errors['temporal'])\n",
    "time_range = list(weekly_zip3_graphs.keys())[:n]\n",
    "ax[0].plot(time_range, zip3_comparison.validation_errors['temporal'], label='temporal')\n",
    "ax[0].plot(time_range, zip3_comparison.validation_errors['static'], label='static')\n",
    "ax[0].plot(time_range, zip3_comparison.validation_errors['temporal'], label='temporal')\n",
    "ax[0].plot(time_range, zip3_comparison.validation_errors['static'], label='static')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n",
      "** Sampled 336 positive and 336 negative edges. **\n",
      "Computing node embeddings...\n",
      "Getting edge embeddings...\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n",
      "nodesim_temporal Validation Score (ROC AUC): 0.87\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodesim_temporal Validation Score (ROC AUC): 0.88\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodesim_temporal Validation Score (ROC AUC): 0.88\n",
      "Fitting classifiers & Evaluating scores ... for sub .. \n",
      "nodesim_temporal Validation Score (ROC AUC): 0.85\n",
      "Choosing the best classifier based on validation score ...\n",
      "Best Operator: l2\n",
      "nodesim_temporal Test Score (ROC AUC): 0.85\n",
      "\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n",
      "nodesim_static Validation Score (ROC AUC): 0.84\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n",
      "nodesim_static Validation Score (ROC AUC): 0.69\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n",
      "nodesim_static Validation Score (ROC AUC): 0.74\n",
      "Fitting classifiers & Evaluating scores ... for sub .. \n",
      "nodesim_static Validation Score (ROC AUC): 0.67\n",
      "Choosing the best classifier based on validation score ...\n",
      "Best Operator: prod\n",
      "nodesim_static Test Score (ROC AUC): 0.80\n",
      "\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n",
      "temporal Validation Score (ROC AUC): 0.88\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n",
      "temporal Validation Score (ROC AUC): 0.89\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n",
      "temporal Validation Score (ROC AUC): 0.88\n",
      "Fitting classifiers & Evaluating scores ... for sub .. \n",
      "temporal Validation Score (ROC AUC): 0.84\n",
      "Choosing the best classifier based on validation score ...\n",
      "Best Operator: l2\n",
      "temporal Test Score (ROC AUC): 0.78\n",
      "\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static Validation Score (ROC AUC): 0.86\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n",
      "static Validation Score (ROC AUC): 0.72\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n",
      "static Validation Score (ROC AUC): 0.69\n",
      "Fitting classifiers & Evaluating scores ... for sub .. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/126 [10:46<22:26:17, 646.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static Validation Score (ROC AUC): 0.74\n",
      "Choosing the best classifier based on validation score ...\n",
      "Best Operator: prod\n",
      "static Test Score (ROC AUC): 0.23\n",
      "\n",
      "Splitting data...\n",
      "** Sampled 474 positive and 474 negative edges. **\n",
      "Computing node embeddings...\n",
      "Getting edge embeddings...\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n",
      "nodesim_temporal Validation Score (ROC AUC): 0.89\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n",
      "nodesim_temporal Validation Score (ROC AUC): 0.81\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n",
      "nodesim_temporal Validation Score (ROC AUC): 0.81\n",
      "Fitting classifiers & Evaluating scores ... for sub .. \n",
      "nodesim_temporal Validation Score (ROC AUC): 0.73\n",
      "Choosing the best classifier based on validation score ...\n",
      "Best Operator: prod\n",
      "nodesim_temporal Test Score (ROC AUC): 0.80\n",
      "\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n",
      "nodesim_static Validation Score (ROC AUC): 0.87\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n",
      "nodesim_static Validation Score (ROC AUC): 0.72\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n",
      "nodesim_static Validation Score (ROC AUC): 0.77\n",
      "Fitting classifiers & Evaluating scores ... for sub .. \n",
      "nodesim_static Validation Score (ROC AUC): 0.64\n",
      "Choosing the best classifier based on validation score ...\n",
      "Best Operator: prod\n",
      "nodesim_static Test Score (ROC AUC): 0.84\n",
      "\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n",
      "temporal Validation Score (ROC AUC): 0.90\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n",
      "temporal Validation Score (ROC AUC): 0.79\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n",
      "temporal Validation Score (ROC AUC): 0.81\n",
      "Fitting classifiers & Evaluating scores ... for sub .. \n",
      "temporal Validation Score (ROC AUC): 0.72\n",
      "Choosing the best classifier based on validation score ...\n",
      "Best Operator: prod\n",
      "temporal Test Score (ROC AUC): 0.84\n",
      "\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n",
      "static Validation Score (ROC AUC): 0.91\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n",
      "static Validation Score (ROC AUC): 0.72\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n",
      "static Validation Score (ROC AUC): 0.73\n",
      "Fitting classifiers & Evaluating scores ... for sub .. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 2/126 [26:56<28:49:29, 836.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static Validation Score (ROC AUC): 0.66\n",
      "Choosing the best classifier based on validation score ...\n",
      "Best Operator: prod\n",
      "static Test Score (ROC AUC): 0.87\n",
      "\n",
      "Splitting data...\n",
      "** Sampled 352 positive and 352 negative edges. **\n",
      "Computing node embeddings...\n",
      "Getting edge embeddings...\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n",
      "nodesim_temporal Validation Score (ROC AUC): 0.92\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n",
      "nodesim_temporal Validation Score (ROC AUC): 0.88\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n",
      "nodesim_temporal Validation Score (ROC AUC): 0.88\n",
      "Fitting classifiers & Evaluating scores ... for sub .. \n",
      "nodesim_temporal Validation Score (ROC AUC): 0.79\n",
      "Choosing the best classifier based on validation score ...\n",
      "Best Operator: prod\n",
      "nodesim_temporal Test Score (ROC AUC): 0.20\n",
      "\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodesim_static Validation Score (ROC AUC): 0.89\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n",
      "nodesim_static Validation Score (ROC AUC): 0.68\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n",
      "nodesim_static Validation Score (ROC AUC): 0.73\n",
      "Fitting classifiers & Evaluating scores ... for sub .. \n",
      "nodesim_static Validation Score (ROC AUC): 0.61\n",
      "Choosing the best classifier based on validation score ...\n",
      "Best Operator: prod\n",
      "nodesim_static Test Score (ROC AUC): 0.60\n",
      "\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n",
      "temporal Validation Score (ROC AUC): 0.93\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n",
      "temporal Validation Score (ROC AUC): 0.86\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n",
      "temporal Validation Score (ROC AUC): 0.86\n",
      "Fitting classifiers & Evaluating scores ... for sub .. \n",
      "temporal Validation Score (ROC AUC): 0.78\n",
      "Choosing the best classifier based on validation score ...\n",
      "Best Operator: prod\n",
      "temporal Test Score (ROC AUC): 0.81\n",
      "\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n",
      "static Validation Score (ROC AUC): 0.92\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n",
      "static Validation Score (ROC AUC): 0.76\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n",
      "static Validation Score (ROC AUC): 0.75\n",
      "Fitting classifiers & Evaluating scores ... for sub .. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 3/126 [38:58<26:48:22, 784.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static Validation Score (ROC AUC): 0.65\n",
      "Choosing the best classifier based on validation score ...\n",
      "Best Operator: prod\n",
      "static Test Score (ROC AUC): 0.22\n",
      "\n",
      "Splitting data...\n",
      "** Sampled 355 positive and 355 negative edges. **\n",
      "Computing node embeddings...\n",
      "Getting edge embeddings...\n",
      "Fitting classifiers & Evaluating scores ... for prod .. \n",
      "nodesim_temporal Validation Score (ROC AUC): 0.89\n",
      "Fitting classifiers & Evaluating scores ... for l2 .. \n",
      "nodesim_temporal Validation Score (ROC AUC): 0.83\n",
      "Fitting classifiers & Evaluating scores ... for l1 .. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 3/126 [52:42<36:01:12, 1054.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m zip3_comparison \u001b[38;5;241m=\u001b[39m WeeklyDecisionMaker(reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m graph \u001b[38;5;129;01min\u001b[39;00m tqdm(weekly_zip3_graphs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mzip3_comparison\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 98\u001b[0m, in \u001b[0;36mWeeklyDecisionMaker.predict_probs\u001b[0;34m(self, graph, display_progress, temporal)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m operator_name \u001b[38;5;129;01min\u001b[39;00m operator_names:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m display_progress: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting classifiers & Evaluating scores ... for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m .. \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m     fitted_classifier_per_operator[operator_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43membedding_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43moperator_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#fit classifier\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     validation_score_per_operator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_score(fitted_classifier_per_operator[operator_name], embedded_features[embedding_type][operator_name][\u001b[38;5;241m1\u001b[39m], labels_validate)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_scores[embedding_type][operator_name]\u001b[38;5;241m.\u001b[39mappend(validation_score_per_operator)\n",
      "Cell \u001b[0;32mIn[19], line 32\u001b[0m, in \u001b[0;36mWeeklyDecisionMaker.fit_classifier\u001b[0;34m(self, embeddings, labels)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_classifier\u001b[39m(\u001b[38;5;28mself\u001b[39m, embeddings, labels):\n\u001b[1;32m     31\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m link_prediction_classifier()\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m classifier\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    419\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 420\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1878\u001b[0m, in \u001b[0;36mLogisticRegressionCV.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1875\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1876\u001b[0m     prefer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocesses\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintercept_scaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1901\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1902\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_encoded_labels\u001b[49m\n\u001b[1;32m   1903\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml1_ratios_\u001b[49m\n\u001b[1;32m   1905\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1907\u001b[0m \u001b[38;5;66;03m# _log_reg_scoring_path will output different shapes depending on the\u001b[39;00m\n\u001b[1;32m   1908\u001b[0m \u001b[38;5;66;03m# multi_class param, so we need to reshape the outputs accordingly.\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;66;03m# Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m#  (n_classes, n_folds, n_Cs . n_l1_ratios) or\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;66;03m#  (1, n_folds, n_Cs . n_l1_ratios)\u001b[39;00m\n\u001b[1;32m   1918\u001b[0m coefs_paths, Cs, scores, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/joblib/parallel.py:1855\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1853\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1854\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/joblib/parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1784\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:732\u001b[0m, in \u001b[0;36m_log_reg_scoring_path\u001b[0;34m(X, y, train, test, pos_class, Cs, scoring, fit_intercept, max_iter, tol, class_weight, verbose, solver, penalty, dual, intercept_scaling, multi_class, random_state, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    729\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n\u001b[1;32m    730\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight[train]\n\u001b[0;32m--> 732\u001b[0m coefs, Cs, n_iter \u001b[38;5;241m=\u001b[39m \u001b[43m_logistic_regression_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_scaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m log_reg \u001b[38;5;241m=\u001b[39m LogisticRegression(solver\u001b[38;5;241m=\u001b[39msolver, multi_class\u001b[38;5;241m=\u001b[39mmulti_class)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# The score method of Logistic Regression has a classes_ attribute.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:452\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    448\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C\n\u001b[1;32m    449\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[1;32m    450\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[1;32m    451\u001b[0m ]\n\u001b[0;32m--> 452\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[1;32m    461\u001b[0m     solver,\n\u001b[1;32m    462\u001b[0m     opt_res,\n\u001b[1;32m    463\u001b[0m     max_iter,\n\u001b[1;32m    464\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[1;32m    465\u001b[0m )\n\u001b[1;32m    466\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/scipy/optimize/_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    693\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    694\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 696\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    699\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    700\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/scipy/optimize/_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    353\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:286\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:256\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[0;32m--> 256\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:167\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_grad\u001b[39m():\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py38_env/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:164\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.grad_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_wrapped\u001b[39m(x):\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39matleast_1d(grad(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs))\n",
      "File \u001b[0;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "zip3_comparison = WeeklyDecisionMaker(reward=False)\n",
    "for graph in tqdm(weekly_zip3_graphs.values()):\n",
    "    zip3_comparison.predict_probs(graph, display_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. given a route, are we doing this? \n",
    "2. or are we doing this for the entire graph? \n",
    "\n",
    "Since we are making the prediction on all potential edges for a graph, how would we do this..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg_graph = StellarGraph.from_networkx(graph, edge_weight_attr='time', edge_type_attr='directed')\n",
    "# [ngh for ngh, t in sg_graph.neighbors('561', include_edge_weight=True) if t > 20211108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# n = len(zip3_comparison.test_scores['temporal'])\n",
    "# time_range = list(weekly_zip3_graphs.keys())[:n]\n",
    "# for operator_name in operator_names: \n",
    "#     ax[0].plot(time_range, zip3_comparison.validation_scores['temporal'][operator_name], label='temporal ' + operator_name)\n",
    "#     ax[1].plot(time_range[:-1], zip3_comparison.validation_scores['static'][operator_name][:44], label='static ' + operator_name)\n",
    "#     # ax.plot(time_range[:-1], zip3_comparison.test_scores['static'], label='static ' + operator_name)\n",
    "# # ax.hlines(np.mean(zip3_comparison.test_scores['temporal']), time_range[0], time_range[-1], label='temporal av', color='r')\n",
    "# # ax.plot(time_range[:-1], zip3_comparison.test_scores['static'], label='static')\n",
    "# # ax.hlines(np.mean(zip3_comparison.test_scores['static']), time_range[0], time_range[-1], label='static av', color='b')\n",
    "\n",
    "# ax[0].legend()\n",
    "# ax[1].legend()\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.read_csv(\"results/temporal_zip3_dm_result.csv\")\n",
    "# hedge_results, naive_results = list(results['hedge']), list(results['naive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kma_comparison = WeeklyDecisionMaker(reward=False)\n",
    "for graph in tqdm(weekly_kma_graphs.values()):\n",
    "    kma_comparison.run_expert_simulation(graph, display_progress=True)\n",
    "\n",
    "# fig, ax = plt.subplots(1,1, figsize= (10, 5))\n",
    "# fig.suptitle(\"Regret Comparison of Static Random Walk based Decision Maker on KMA data\")\n",
    "# ax.plot(weekly_zip3_graphs.keys(), static_kma_dm.hedge_total_regret[1:], label='hedge')\n",
    "# ax.plot(weekly_zip3_graphs.keys(), static_kma_dm.naive_total_regret[1:], label='naive')\n",
    "# ax.legend()\n",
    "# ax.set_xlabel(\"Week\")\n",
    "# ax.set_ylabel(\"Regret\")\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
