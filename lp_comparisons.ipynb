{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stellargraph\n",
      "  Using cached stellargraph-1.2.1-py3-none-any.whl (435 kB)\n",
      "Collecting chardet\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.4-cp38-cp38-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl (11.7 MB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.2-cp38-cp38-macosx_10_9_x86_64.whl (24.1 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl (10.1 MB)\n",
      "Collecting matplotlib>=2.2\n",
      "  Using cached matplotlib-3.7.5-cp38-cp38-macosx_10_12_x86_64.whl (7.4 MB)\n",
      "Collecting tensorflow>=2.1.0\n",
      "  Using cached tensorflow-2.13.1-cp38-cp38-macosx_10_15_x86_64.whl (216.2 MB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Using cached scipy-1.10.1-cp38-cp38-macosx_10_9_x86_64.whl (35.0 MB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from matplotlib>=2.2->stellargraph) (24.0)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached pillow-10.3.0-cp38-cp38-macosx_10_10_x86_64.whl (3.5 MB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Using cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.5-cp38-cp38-macosx_10_9_x86_64.whl (68 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.1.1-cp38-cp38-macosx_10_9_x86_64.whl (247 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.51.0-cp38-cp38-macosx_10_9_x86_64.whl (2.3 MB)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting wrapt\n",
      "  Using cached wrapt-1.16.0-cp38-cp38-macosx_10_9_x86_64.whl (37 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-macosx_10_14_x86_64.whl (1.7 MB)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.62.2-cp38-cp38-macosx_10_10_universal2.whl (10.1 MB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.3-cp38-cp38-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from tensorflow>=2.1.0->stellargraph) (56.0.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.11.0-cp38-cp38-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl (26.5 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib>=2.2->stellargraph) (3.18.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./venv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->stellargraph) (7.1.0)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp38-cp38-macosx_10_9_x86_64.whl (121 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Using cached MarkupSafe-2.1.5-cp38-cp38-macosx_10_9_x86_64.whl (14 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: pytz, libclang, flatbuffers, wrapt, wheel, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyparsing, pyasn1, protobuf, pillow, oauthlib, numpy, networkx, MarkupSafe, kiwisolver, keras, joblib, importlib-resources, idna, grpcio, google-pasta, gast, fonttools, cycler, charset-normalizer, chardet, certifi, cachetools, absl-py, werkzeug, smart-open, scipy, rsa, requests, pyasn1-modules, pandas, opt-einsum, markdown, h5py, contourpy, astunparse, scikit-learn, requests-oauthlib, matplotlib, google-auth, gensim, seaborn, google-auth-oauthlib, tensorboard, tensorflow, stellargraph\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 certifi-2024.2.2 chardet-5.2.0 charset-normalizer-3.3.2 contourpy-1.1.1 cycler-0.12.1 flatbuffers-24.3.25 fonttools-4.51.0 gast-0.4.0 gensim-4.3.2 google-auth-2.29.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.62.2 h5py-3.11.0 idna-3.7 importlib-resources-6.4.0 joblib-1.4.0 keras-2.13.1 kiwisolver-1.4.5 libclang-18.1.1 markdown-3.6 matplotlib-3.7.5 networkx-3.1 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 pandas-2.0.3 pillow-10.3.0 protobuf-4.25.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 pyparsing-3.1.2 pytz-2024.1 requests-2.31.0 requests-oauthlib-2.0.0 rsa-4.9 scikit-learn-1.3.2 scipy-1.10.1 seaborn-0.13.2 smart-open-7.0.4 stellargraph-1.2.1 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.4.0 threadpoolctl-3.4.0 tqdm-4.66.2 typing-extensions-4.5.0 tzdata-2024.1 urllib3-2.2.1 werkzeug-3.0.2 wheel-0.43.0 wrapt-1.16.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/yko/Documents/Temporal-Link-Prediction/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install stellargraph chardet networkx seaborn numpy pandas tqdm gensim scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 20:59:06.099423: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "from utils import * \n",
    "import stellargraph as sg\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import EdgeSplitter, BiasedRandomWalk, TemporalRandomWalk\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from math import isclose\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from TRW_nodesim import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM_Time() & Weekly Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class temporalNetwork(): \n",
    "    def __init__(self, start_date, end_date, display_progress=False, location_grouping='kma', origin=None, facility_id=None, intermediate=None):\n",
    "        \"\"\" \n",
    "        Note: \n",
    "            start_date and end_date should be both None as they are used as a signal to \n",
    "            CM_Time's run_simulation to whether construct a new graph or update the graph with new information \n",
    "        \"\"\"\n",
    "        self.display_progress = display_progress\n",
    "        self.start_date=start_date\n",
    "        self.end_date=end_date\n",
    "        self.network=None\n",
    "        self.origin_location_list=None\n",
    "        self.location_grouping=location_grouping\n",
    "        self.inbound_data = None \n",
    "        self.outbound_data = None\n",
    "\n",
    "    def construct_network_graph(self):\n",
    "        \"\"\" \n",
    "        Given an inbound and outbound dataframe, construct a network graph and stores it in the class variable self.network\n",
    "        Args: \n",
    "            an_inbound_df (pd.DataFrame): inbound dataframe\n",
    "            an_outbound_df (pd.DataFrame): outbound dataframe\n",
    "            start_date (datetime): start date of the network graph\n",
    "            end_date (datetime): end date of the network graph\n",
    "            location_grouping(string): 'kma' or 'zip3'\n",
    "        \"\"\"\n",
    "        # pull data & construct an empty multiDiGraph\n",
    "\n",
    "        date = self.start_date.strftime(\"%Y-%m-%d\") + \"_\" + self.end_date.strftime(\"%Y-%m-%d\")\n",
    "        an_inbound_df, an_outbound_df = pd.read_csv(f\"inbound_{self.location_grouping}_data/inbound_data_{date}.csv\"), pd.read_csv(f\"outbound_{self.location_grouping}_data/outbound_data_{date}.csv\")\n",
    "        an_inbound_df.load_date, an_outbound_df.load_date = pd.to_datetime(an_inbound_df.load_date), pd.to_datetime(an_outbound_df.load_date)\n",
    "        for colin, colout in zip(an_inbound_df.columns, an_outbound_df.columns): \n",
    "            if colin not in [\"total_loads\", \"load_date\"]: \n",
    "                an_inbound_df[colin] = an_inbound_df[colin].astype(str)\n",
    "            if colout not in [\"total_loads\", \"load_date\"]:\n",
    "                an_outbound_df[colout] = an_outbound_df[colout].astype(str)\n",
    "\n",
    "        network_graph = nx.MultiDiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # network_graph = nx.DiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # idf, odf = an_inbound_df.copy(), an_outbound_df.copy()\n",
    "\n",
    "        # idf.to_csv(f\"data/inbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "        # odf.to_csv(f\"data/outbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "\n",
    "        # add nodes & edges \n",
    "        node_1 = f\"origin_{self.location_grouping}_id\"\n",
    "        node_2 = f\"facility_{self.location_grouping}_id\"\n",
    "        node_3 = f\"destination_{self.location_grouping}_id\"\n",
    "\n",
    "        network_graph = add_nodes_given_df(network_graph, an_inbound_df, [node_1, 'facility_id']) \n",
    "        network_graph = add_nodes_given_df(network_graph, an_outbound_df, ['facility_id', node_2, node_3]) \n",
    "\n",
    "        network_graph = add_edges_given_graph(network_graph, an_inbound_df, an_outbound_df, self.location_grouping)\n",
    "        \n",
    "        # update the variables \n",
    "        self.network = network_graph\n",
    "        self.origin_location_list = an_inbound_df[f'origin_{self.location_grouping}_id'].unique()\n",
    "        self.inbound_data, self.outbound_data = an_inbound_df, an_outbound_df\n",
    "        if self.display_progress: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\") \n",
    "        \n",
    "    def move_to_next_week(self): \n",
    "        \"\"\" \n",
    "        Given the new week's inbound and outbound dataframes, \n",
    "        update self.network graph, self.start_date, and self.end_date to a week after current start date and end date \n",
    "        \n",
    "        Args:\n",
    "            next_inbound_df (pd.DataFrame): new week's inbound dataframe\n",
    "            next_outbound_df (pd.DataFrame): new week's outbound dataframe\n",
    "            display_progress (boolean): whether to display the progress of the function or not\n",
    "        \"\"\"\n",
    "        # update the dates, pull new week's data, & store some informations\n",
    "        self.start_date, self.end_date = self.start_date + timedelta(days=7), self.end_date + timedelta(days=7)\n",
    "        date = self.start_date.strftime(\"%Y-%m-%d\") + \"_\" + self.end_date.strftime(\"%Y-%m-%d\")\n",
    "        next_inbound_df, next_outbound_df = pd.read_csv(f\"inbound_{self.location_grouping}_data/inbound_data_{date}.csv\"), pd.read_csv(f\"outbound_{self.location_grouping}_data/outbound_data_{date}.csv\")\n",
    "        next_inbound_df.load_date, next_outbound_df.load_date = pd.to_datetime(next_inbound_df.load_date), pd.to_datetime(next_outbound_df.load_date)\n",
    "        for colin, colout in zip(next_inbound_df.columns, next_outbound_df.columns): \n",
    "            if colin not in [\"total_loads\", \"load_date\"]: \n",
    "                next_inbound_df[colin] = next_inbound_df[colin].astype(str)\n",
    "            if colout not in [\"total_loads\", \"load_date\"]:\n",
    "                next_outbound_df[colout] = next_outbound_df[colout].astype(str)\n",
    "\n",
    "        network_graph = nx.MultiDiGraph(name=f\"new network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # network_graph = nx.DiGraph(name=f\"original network\", start_date=self.start_date, end_date=self.end_date)\n",
    "        # idf, odf = an_inbound_df.copy(), an_outbound_df.copy()\n",
    "\n",
    "        # idf.to_csv(f\"data/inbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "        # odf.to_csv(f\"data/outbound_data_{self.start_date}_{self.end_date}.csv\", index=False)\n",
    "\n",
    "        # add nodes & edges \n",
    "        node_1 = f\"origin_{self.location_grouping}_id\"\n",
    "        node_2 = f\"facility_{self.location_grouping}_id\"\n",
    "        node_3 = f\"destination_{self.location_grouping}_id\"\n",
    "\n",
    "        network_graph = add_nodes_given_df(network_graph, next_inbound_df, [node_1, 'facility_id']) \n",
    "        network_graph = add_nodes_given_df(network_graph, next_outbound_df, ['facility_id', node_2, node_3]) \n",
    "\n",
    "        network_graph = add_edges_given_graph(network_graph, next_inbound_df, next_outbound_df, self.location_grouping)\n",
    "\n",
    "         # update the variables \n",
    "        self.network = network_graph\n",
    "        self.origin_location_list = next_inbound_df[f'origin_{self.location_grouping}_id'].unique()\n",
    "        self.inbound_data, self.outbound_data = next_inbound_df, next_outbound_df\n",
    "        if self.display_progress: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\") \n",
    "       \n",
    "\n",
    "    def print_network_information(self, given_network, print_network_time=False): \n",
    "        \"\"\"\n",
    "        Given a network, print out the information of the network\n",
    "        Args: \n",
    "            given_network (nx.MultiDiGraph): a network graph\n",
    "        Returns: N/A\n",
    "        \"\"\"\n",
    "        print(\"---------------------------------------------------------------------------------------------\") \n",
    "        print(given_network)\n",
    "        print(f\"Is the given network a DAG for load_network?: {nx.is_directed_acyclic_graph(given_network)}\")\n",
    "        print(f\"Number of self loops: {nx.number_of_selfloops(given_network)}\")\n",
    "        if print_network_time: print(f\"Current time of the graph: {given_network.graph['start_date']} to {given_network.graph['end_date']}\")\n",
    "        else: print(f\"Current time of the graph: {self.start_date} to {self.end_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_Finder():\n",
    "    def __init__(self, location_grouping='kma', origin_location_list=None, network = None):\n",
    "        self.network=network\n",
    "        self.processed_network=None\n",
    "        self.processed=False \n",
    "        self.origin_location_list=origin_location_list\n",
    "        self.match_failure = None\n",
    "        self.remove_failure = 0\n",
    "        self.location_grouping=location_grouping\n",
    "        \n",
    "    def group_to_DiGraph(self, display_progress = False):\n",
    "        \"\"\"\n",
    "        #TODO: explain why aggregate_faciility_zip then group_to_DiGraph (kma-> facility_zip -> kma to kma->kma->kma, aggregate to faciliy KMA)\n",
    "        Assuming that self.network is constructed, \n",
    "        sums the edge weights for edges with the same nodes in self.network variable and \n",
    "        stores the new graph with aggregated edges in self.processed_network variable and returns False if successful \n",
    "\n",
    "        Args:\n",
    "            display_progress (boolean): whether to display the progress of the function or not\n",
    "\n",
    "        NOTE) disregards temporal factor \n",
    "        \"\"\"\n",
    "        if not self.network: \n",
    "            print(\"Please construct the network first\")\n",
    "            return None \n",
    "            \n",
    "        new_name = self.network.name + \" reduced\"\n",
    "        self.processed_network = nx.DiGraph(name=new_name)\n",
    "        self.processed_network.add_nodes_from(self.network)\n",
    "\n",
    "        if display_progress: print(\"Aggregating nodes by KMA...\")\n",
    "        for n1, n2 in self.network.edges():\n",
    "            sum = 0 \n",
    "            for inner_dict in self.network.get_edge_data(n1, n2).values(): \n",
    "                sum += inner_dict['capacity']\n",
    "            self.processed_network.add_edge(n1, n2, capacity = sum)\n",
    "        \n",
    "        nx.set_edge_attributes(self.processed_network, to_integer(self.network.graph['end_date']), 'time')\n",
    "        self.processed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_Time(): \n",
    "    def __init__(self, location_grouping='kma', origin=None, facility_id=None, intermediate=None): \n",
    "        self.start_date=None\n",
    "        self.end_date=None\n",
    "        self.cm_network = None   \n",
    "        self.cm_finder = None\n",
    "        self.origin=origin\n",
    "        self.facility_id=facility_id\n",
    "        self.intermediate=intermediate\n",
    "        self.location_grouping = location_grouping\n",
    "        self.weekly_graphs = {}\n",
    "    \n",
    "    def update_dates(self): \n",
    "        \"\"\"\n",
    "        Updates the start and end date by 7 days\n",
    "        \"\"\" \n",
    "        self.start_date += timedelta(days=7)\n",
    "        self.end_date += timedelta(days=7)\n",
    "\n",
    "    def construct_or_update_tg(self, filter_key='load_count', display_progress=False, display_path_info=False):\n",
    "        \"\"\" \n",
    "        Either (1) creates a network graph given a start and end date \n",
    "            or (2) updates the network graph to the next week's graph \n",
    "\n",
    "        Args: \n",
    "            start_date, end_date: start & end date of the first two weeks of the simulation\n",
    "\n",
    "        Returns: n/a\n",
    "        \"\"\"        \n",
    "        # construct or update cm_finder.network \n",
    "        if not self.cm_network: \n",
    "            self.cm_network = temporalNetwork(self.start_date, self.end_date, display_progress, self.location_grouping, self.origin, self.facility_id, self.intermediate)\n",
    "            self.cm_network.construct_network_graph()\n",
    "            self.cm_finder = CM_Finder(location_grouping=self.location_grouping)\n",
    "        else: \n",
    "            self.cm_network.move_to_next_week()\n",
    "\n",
    "        self.cm_finder.origin_location_list = self.cm_network.origin_location_list\n",
    "        self.cm_finder.network = self.cm_network.network\n",
    "        \n",
    "        # self.cm_finder.group_to_DiGraph(display_progress = display_progress)\n",
    "        self.weekly_graphs[self.end_date] = self.cm_finder.network\n",
    "\n",
    "    def temporal_query(self, start_date, temporal=True, looback = 7, number_of_weeks=None, termination_date = None, \\\n",
    "                    filter_key = \"load_count\", display_progress=False, display_path_info = False): \n",
    "        \"\"\"\n",
    "        Given a start date, run the simulation for number_of_weeks or until termination_date is reached.\n",
    "\n",
    "        Args: \n",
    "            start_date: start date of the first week of the simulation\n",
    "            temporal: if True, run the simulation for every two weeks, if False, run the simulation from start_date until end_date\n",
    "            number_of_weeks: number of weeks to run the simulation for\n",
    "            termination_date: date to stop the simulation\n",
    "            filter_key: key to filter the network on (load_count or path_score)\n",
    "            display_progress: if True, display progress bar\n",
    "            display_path_info: if True, display path info\n",
    "\n",
    "        Returns: a dictionary of simulation result for each week\n",
    "            \n",
    "        Note:\n",
    "        * termination_date: termination date of the entire analysis, when end_date reaches termination_date, the query loop terminates,\n",
    "        * end_date: the end date of the two-week window, will be updated every week\n",
    "\n",
    "        - Once the parameters (location_grouping, origin, facility_id, intermediate) are used to initialise the cm_time class, \n",
    "          they will be used for any further analysis until new initialisation happens.\n",
    "          query_weekly method will only perform analysis, no alterations can be made by calling solely this.\n",
    "\n",
    "        - If temporal=True, --> end_date != termination_date, eventually at the end of simulations, end_date = termination_date\n",
    "            and number_of_weeks is given, end_date = start_date + 13 days for the first simulation, termination_date = start_date + 7 days * number_of_weeks\n",
    "            and termination_date is given, end_date = start_date + 13 days for the first simulation and termination_date=termination_date for the simulation\n",
    "          If temporal=False --> end_date = termination_date \n",
    "            and number_of_weeks is given, end_date = start_date + 7 days * number_of_weeks for the simulation \n",
    "            and termination_date is given, end_date=termination_date for the simulation\n",
    "\n",
    "        * simulation_results: a dictionary with key as the end_date and value as the simulation result\n",
    "        \"\"\"\n",
    "\n",
    "        # create information needed for a new query with the given start_date and number_of_weeks\n",
    "        # possible bug when end_date > termination_date.\n",
    "        if temporal: \n",
    "            self.start_date, self.end_date = start_date, start_date + timedelta(days=looback-1)\n",
    "\n",
    "            if number_of_weeks: termination_date = self.start_date + timedelta(days=7) * number_of_weeks\n",
    "            elif termination_date: termination_date = termination_date\n",
    "            else: raise Exception(\"Neither number of weeks nor termination date was given to set the simulation time period.\")\n",
    "        \n",
    "            # run simulation for every two weeks until termination_date\n",
    "            while self.end_date <= termination_date:    \n",
    "                self.construct_or_update_tg(filter_key=filter_key, \\\n",
    "                                    display_progress=display_progress, display_path_info=display_path_info)\n",
    "                self.update_dates() \n",
    "                \n",
    "        else: \n",
    "            if number_of_weeks: self.start_date, self.end_date = start_date, start_date + timedelta(days=7) * number_of_weeks\n",
    "            elif termination_date: self.start_date, self.end_date = start_date, termination_date\n",
    "            else: raise Exception(\"Neither number of weeks nor termination date was given to set the simulation time period.\")\n",
    "            self.run_single_simulation(filter_key=filter_key, \\\n",
    "                                display_progress=display_progress, display_path_info=display_path_info)\n",
    "\n",
    "        return self.weekly_graphs\n",
    "\n",
    "    def if_edge(node1, node2, curr_graph): \n",
    "        adjacency_matrix = curr_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k1, k2 = list(weekly_graphs.keys())[:2]\n",
    "# weekly_graphs[k1] == weekly_graphs[k2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_time = CM_Time(location_grouping = 'kma') \n",
    "weekly_kma_graphs = cm_time.temporal_query(start_date=datetime(2021,1,1).date(), looback=7, termination_date= datetime(2023,6, 1).date(), \\\n",
    "                    display_progress=False, display_path_info = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_time = CM_Time(location_grouping = 'zip3') \n",
    "weekly_zip3_graphs = cm_time.temporal_query(start_date=datetime(2021,1,1).date(), looback=7, termination_date= datetime(2023,6, 1).date(), \\\n",
    "                    display_progress=False, display_path_info = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operator_prod(u, v):\n",
    "    return u * v\n",
    "\n",
    "def operator_l2(u, v):\n",
    "    return (u - v) ** 2\n",
    "def operator_l1(u, v):\n",
    "    return abs(u - v)\n",
    "def operator_sub(u, v):\n",
    "    return (u - v)\n",
    "\n",
    "operators = [operator_prod, operator_l2, operator_l1, operator_sub]\n",
    "operator_names = [\"prod\", \"l2\", \"l1\", \"sub\"]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def all_edge_to_features(link_examples, transform_node):\n",
    "    features_per_operator = {} \n",
    "    for operator,operator_name in zip(operators, operator_names): \n",
    "        features_per_operator[operator_name] = edge_to_features_given_operator(link_examples, transform_node, operator)\n",
    "    return features_per_operator\n",
    "\n",
    "def edge_to_features_given_operator(link_examples, transform_node, operator):\n",
    "    return [\n",
    "        operator(transform_node(src), transform_node(dst)) for src, dst in link_examples\n",
    "    ]\n",
    "\n",
    "def edge_to_features_given_operator_name(link_examples, transform_node, operator_name):\n",
    "    operator = operators[operator_names.index(operator_name)]\n",
    "    return edge_to_features_given_operator(link_examples, transform_node, operator)\n",
    "\n",
    "def link_prediction_classifier(max_iter=2000):\n",
    "    lr_clf = LogisticRegressionCV(Cs=10, cv=10, scoring=\"roc_auc\", max_iter=max_iter, penalty=\"l2\") #, solver=\"liblinear\")\n",
    "    return Pipeline(steps=[(\"sc\", StandardScaler()), (\"clf\", lr_clf)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_model(graph, num_walks_per_node=10, walk_length = 10, context_window_size = 2): \n",
    "    num_cw = len(graph.nodes()) * num_walks_per_node * (walk_length - context_window_size + 1)\n",
    "    temporal_rw = TemporalRandomWalk(graph)\n",
    "    temporal_walks = temporal_rw.run(\n",
    "        num_cw=num_cw,\n",
    "        cw_size=context_window_size,\n",
    "        max_walk_length=walk_length,\n",
    "        walk_bias=\"exponential\",\n",
    "    )\n",
    "    \n",
    "    embedding_size = 128\n",
    "    temporal_model = Word2Vec(\n",
    "        temporal_walks,\n",
    "        vector_size=embedding_size,\n",
    "        window=context_window_size,\n",
    "        min_count=0,\n",
    "        sg=1,\n",
    "        workers=2,\n",
    "        epochs=1,)\n",
    "\n",
    "    unseen_node_embedding = np.zeros(embedding_size)\n",
    "\n",
    "    def temporal_embedding(u):\n",
    "        try:\n",
    "            return temporal_model.wv[u]\n",
    "        except KeyError:\n",
    "            return unseen_node_embedding\n",
    "    return temporal_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from collections import defaultdict, deque\n",
    "from scipy import stats\n",
    "from scipy.special import softmax\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.core.schema import GraphSchema\n",
    "from stellargraph.core.utils import is_real_iterable\n",
    "from stellargraph.core.experimental import experimental\n",
    "from stellargraph.random import random_state\n",
    "\n",
    "\n",
    "class GraphWalk(object):\n",
    "    \"\"\"\n",
    "    Base class for exploring graphs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph, temporal=True, graph_schema=None, seed=None):\n",
    "        self.graph = graph\n",
    "        self.temporal=temporal\n",
    "\n",
    "        # Initialize the random state\n",
    "        self._check_seed(seed)\n",
    "\n",
    "        self._random_state, self._np_random_state = random_state(seed)\n",
    "\n",
    "        # We require a StellarGraph for this\n",
    "        if not isinstance(graph, StellarGraph):\n",
    "            raise TypeError(\"Graph must be a StellarGraph or StellarDiGraph.\")\n",
    "\n",
    "        if not graph_schema:\n",
    "            self.graph_schema = self.graph.create_graph_schema()\n",
    "        else:\n",
    "            self.graph_schema = graph_schema\n",
    "\n",
    "        if type(self.graph_schema) is not GraphSchema:\n",
    "            self._raise_error(\n",
    "                \"The parameter graph_schema should be either None or of type GraphSchema.\"\n",
    "            )\n",
    "\n",
    "    def get_adjacency_types(self):\n",
    "        # Allow additional info for heterogeneous graphs.\n",
    "        adj = getattr(self, \"adj_types\", None)\n",
    "        if not adj:\n",
    "            # Create a dict of adjacency lists per edge type, for faster neighbour sampling from graph in SampledHeteroBFS:\n",
    "            self.adj_types = adj = self.graph._adjacency_types(self.graph_schema)\n",
    "        return adj\n",
    "\n",
    "    def _check_seed(self, seed):\n",
    "        if seed is not None:\n",
    "            if type(seed) != int:\n",
    "                self._raise_error(\n",
    "                    \"The random number generator seed value, seed, should be integer type or None.\"\n",
    "                )\n",
    "            if seed < 0:\n",
    "                self._raise_error(\n",
    "                    \"The random number generator seed value, seed, should be non-negative integer or None.\"\n",
    "                )\n",
    "\n",
    "    def _get_random_state(self, seed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seed: The optional seed value for a given run.\n",
    "\n",
    "        Returns:\n",
    "            The random state as determined by the seed.\n",
    "        \"\"\"\n",
    "        if seed is None:\n",
    "            # Restore the random state\n",
    "            return self._random_state\n",
    "        # seed the random number generator\n",
    "        rs, _ = random_state(seed)\n",
    "        return rs\n",
    "\n",
    "    def neighbors(self, node):\n",
    "        if not self.graph.has_node(node):\n",
    "            self._raise_error(\"node {} not in graph\".format(node))\n",
    "        return self.graph.neighbors(node)\n",
    "\n",
    "    def run(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        To be overridden by subclasses. It is the main entry point for performing random walks on the given\n",
    "        graph.\n",
    "\n",
    "        It should return the sequences of nodes in each random walk.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _raise_error(self, msg):\n",
    "        raise ValueError(\"({}) {}\".format(type(self).__name__, msg))\n",
    "\n",
    "    def _check_common_parameters(self, nodes, n, length, seed):\n",
    "        \"\"\"\n",
    "        Checks that the parameter values are valid or raises ValueError exceptions with a message indicating the\n",
    "        parameter (the first one encountered in the checks) with invalid value.\n",
    "\n",
    "        Args:\n",
    "            nodes: <list> A list of root node ids from which to commence the random walks.\n",
    "            n: <int> Number of walks per node id.\n",
    "            length: <int> Maximum length of each walk.\n",
    "            seed: <int> Random number generator seed.\n",
    "        \"\"\"\n",
    "        self._check_nodes(nodes)\n",
    "        self._check_repetitions(n)\n",
    "        self._check_length(length)\n",
    "        self._check_seed(seed)\n",
    "\n",
    "    def _check_nodes(self, nodes):\n",
    "        if nodes is None:\n",
    "            self._raise_error(\"A list of root node IDs was not provided.\")\n",
    "        if not is_real_iterable(nodes):\n",
    "            self._raise_error(\"Nodes parameter should be an iterable of node IDs.\")\n",
    "        if (\n",
    "            len(nodes) == 0\n",
    "        ):  # this is not an error but maybe a warning should be printed to inform the caller\n",
    "            warnings.warn(\n",
    "                \"No root node IDs given. An empty list will be returned as a result.\",\n",
    "                RuntimeWarning,\n",
    "                stacklevel=3,\n",
    "            )\n",
    "\n",
    "    def _check_repetitions(self, n):\n",
    "        if type(n) != int:\n",
    "            self._raise_error(\n",
    "                \"The number of walks per root node, n, should be integer type.\"\n",
    "            )\n",
    "        if n <= 0:\n",
    "            self._raise_error(\n",
    "                \"The number of walks per root node, n, should be a positive integer.\"\n",
    "            )\n",
    "\n",
    "    def _check_length(self, length):\n",
    "        if type(length) != int:\n",
    "            self._raise_error(\"The walk length, length, should be integer type.\")\n",
    "        if length <= 0:\n",
    "            # Technically, length 0 should be okay, but by consensus is invalid.\n",
    "            self._raise_error(\"The walk length, length, should be a positive integer.\")\n",
    "\n",
    "    # For neighbourhood sampling\n",
    "    def _check_sizes(self, n_size):\n",
    "        err_msg = \"The neighbourhood size must be a list of non-negative integers.\"\n",
    "        if not isinstance(n_size, list):\n",
    "            self._raise_error(err_msg)\n",
    "        if len(n_size) == 0:\n",
    "            # Technically, length 0 should be okay, but by consensus it is invalid.\n",
    "            self._raise_error(\"The neighbourhood size list should not be empty.\")\n",
    "        for d in n_size:\n",
    "            if type(d) != int or d < 0:\n",
    "                self._raise_error(err_msg)\n",
    "\n",
    "def naive_weighted_choices(rs, weights):\n",
    "    \"\"\"\n",
    "    Select an index at random, weighted by the iterator `weights` of\n",
    "    arbitrary (non-negative) floats. That is, `x` will be returned\n",
    "    with probability `weights[x]/sum(weights)`.\n",
    "\n",
    "    For doing a single sample with arbitrary weights, this is much (5x\n",
    "    or more) faster than numpy.random.choice, because the latter\n",
    "    requires a lot of preprocessing (normalized probabilties), and\n",
    "    does a lot of conversions/checks/preprocessing internally.\n",
    "    \"\"\"\n",
    "\n",
    "    # divide the interval [0, sum(weights)) into len(weights)\n",
    "    # subintervals [x_i, x_{i+1}), where the width x_{i+1} - x_i ==\n",
    "    # weights[i]\n",
    "    subinterval_ends = []\n",
    "    running_total = 0\n",
    "    for w in weights:\n",
    "        if w < 0:\n",
    "            raise ValueError(\"Detected negative weight: {}\".format(w))\n",
    "        running_total += w\n",
    "        subinterval_ends.append(running_total)\n",
    "\n",
    "    # pick a place in the overall interval\n",
    "    x = rs.random() * running_total\n",
    "\n",
    "    # find the subinterval that contains the place, by looking for the\n",
    "    # first subinterval where the end is (strictly) after it\n",
    "    for idx, end in enumerate(subinterval_ends):\n",
    "        if x < end:\n",
    "            break\n",
    "\n",
    "    return idx\n",
    "\n",
    "class NS_weighted_RandomWalk(GraphWalk):\n",
    "    \"\"\"\n",
    "    Performs temporal random walks on the given graph. The graph should contain numerical edge\n",
    "    weights that correspond to the time at which the edge was created. Exact units are not relevant\n",
    "    for the algorithm, only the relative differences (e.g. seconds, days, etc).\n",
    "    \"\"\"\n",
    "    def run(\n",
    "        self,\n",
    "        num_cw,\n",
    "        cw_size,\n",
    "        max_walk_length=80,\n",
    "        initial_edge_bias=None,\n",
    "        walk_bias=None,\n",
    "        p_walk_success_threshold=0.01,\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform a time respecting random walk starting from randomly selected temporal edges.\n",
    "\n",
    "        Args:\n",
    "            num_cw (int): Total number of context windows to generate. For comparable\n",
    "                results to most other random walks, this should be a multiple of the number\n",
    "                of nodes in the graph.\n",
    "            cw_size (int): Size of context window. Also used as the minimum walk length,\n",
    "                since a walk must generate at least 1 context window for it to be useful.\n",
    "            max_walk_length (int): Maximum length of each random walk. Should be greater\n",
    "                than or equal to the context window size.\n",
    "            initial_edge_bias (str, optional): Distribution to use when choosing a random\n",
    "                initial temporal edge to start from. Available options are:\n",
    "\n",
    "                * None (default) - The initial edge is picked from a uniform distribution.\n",
    "                * \"exponential\" - Heavily biased towards more recent edges.\n",
    "\n",
    "            walk_bias (str, optional): Distribution to use when choosing a random\n",
    "                neighbour to walk through. Available options are:\n",
    "\n",
    "                * None (default) - Neighbours are picked from a uniform distribution.\n",
    "                * \"exponential\" - Exponentially decaying probability, resulting in a bias towards shorter time gaps.\n",
    "\n",
    "            p_walk_success_threshold (float): Lower bound for the proportion of successful\n",
    "                (i.e. longer than minimum length) walks. If the 95% percentile of the\n",
    "                estimated proportion is less than the provided threshold, a RuntimeError\n",
    "                will be raised. The default value of 0.01 means an error is raised if less than 1%\n",
    "                of the attempted random walks are successful. This parameter exists to catch any\n",
    "                potential situation where too many unsuccessful walks can cause an infinite or very\n",
    "                slow loop.\n",
    "            seed (int, optional): Random number generator seed; default is None.\n",
    "\n",
    "        Returns:\n",
    "            List of lists of node ids for each of the random walks.\n",
    "\n",
    "        \"\"\"\n",
    "        if cw_size < 2:\n",
    "            raise ValueError(\n",
    "                f\"cw_size: context window size should be greater than 1, found {cw_size}\"\n",
    "            )\n",
    "        if max_walk_length < cw_size:\n",
    "            raise ValueError(\n",
    "                f\"max_walk_length: maximum walk length should not be less than the context window size, found {max_walk_length}\"\n",
    "            )\n",
    "\n",
    "        np_rs = self._np_random_state if seed is None else np.random.RandomState(seed)\n",
    "        walks = []\n",
    "        num_cw_curr = 0\n",
    "\n",
    "        edges, times = self.graph.edges(include_edge_weight=True)\n",
    "        edge_biases = self._temporal_biases(\n",
    "            times, None, bias_type=initial_edge_bias, is_forward=False,\n",
    "        )\n",
    "\n",
    "        successes = 0\n",
    "        failures = 0\n",
    "\n",
    "        def not_progressing_enough():\n",
    "            # Estimate the probability p of a walk being long enough; the 95% percentile is used to\n",
    "            # be more stable with respect to randomness. This uses Beta(1, 1) as the prior, since\n",
    "            # it's uniform on p\n",
    "            posterior = stats.beta.ppf(0.95, 1 + successes, 1 + failures)\n",
    "            return posterior < p_walk_success_threshold\n",
    "\n",
    "        # loop runs until we have enough context windows in total\n",
    "        while num_cw_curr < num_cw:\n",
    "            first_edge_index = self._sample(len(edges), edge_biases, np_rs)\n",
    "            src, dst = edges[first_edge_index]\n",
    "            t = times[first_edge_index]\n",
    "\n",
    "            remaining_length = num_cw - num_cw_curr + cw_size - 1\n",
    "\n",
    "            walk = self._walk(\n",
    "                src, dst, t, min(max_walk_length, remaining_length), walk_bias, np_rs\n",
    "            )\n",
    "            if len(walk) >= cw_size:\n",
    "                walks.append(walk)\n",
    "                num_cw_curr += len(walk) - cw_size + 1\n",
    "                successes += 1\n",
    "            else:\n",
    "                failures += 1\n",
    "                if not_progressing_enough():\n",
    "                    raise RuntimeError(\n",
    "                        f\"Discarded {failures} walks out of {failures + successes}. \"\n",
    "                        \"Too many temporal walks are being discarded for being too short. \"\n",
    "                        f\"Consider using a smaller context window size (currently cw_size={cw_size}).\"\n",
    "                    )\n",
    "\n",
    "        return walks\n",
    "\n",
    "\n",
    "    def _sample(self, n, biases, np_rs):\n",
    "        if biases is not None:\n",
    "            assert len(biases) == n\n",
    "            return naive_weighted_choices(np_rs, biases)\n",
    "        else:\n",
    "            return np_rs.choice(n)\n",
    "\n",
    "    def _exp_biases(self, times, t_0, decay):\n",
    "        # t_0 assumed to be smaller than all time values\n",
    "        return softmax(t_0 - np.array(times) if decay else np.array(times) - t_0)\n",
    "\n",
    "    def _temporal_biases(self, times, time, bias_type, is_forward):\n",
    "        if bias_type is None:\n",
    "            # default to uniform random sampling\n",
    "            return None\n",
    "\n",
    "        # time is None indicates we should obtain the minimum available time for t_0\n",
    "        t_0 = time if time is not None else min(times)\n",
    "\n",
    "        if bias_type == \"exponential\":\n",
    "            # exponential decay bias needs to be reversed if looking backwards in time\n",
    "            return self._exp_biases(times, t_0, decay=is_forward)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported bias type\")\n",
    "\n",
    "    def _step(self, node, time, bias_type, np_rs):\n",
    "        \"\"\"\n",
    "        Perform 1 temporal step from a node. Returns None if a dead-end is reached.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.temporal: \n",
    "            neighbours = [\n",
    "                (neighbour, t)\n",
    "                for neighbour, t in self.graph.neighbors(node, include_edge_weight=True)\n",
    "                if t > time\n",
    "            ]\n",
    "        else: \n",
    "            neighbours = [\n",
    "                (neighbour, t)\n",
    "                for neighbour, t in self.graph.neighbors(node, include_edge_weight=True)\n",
    "            ]\n",
    "        \n",
    "        def compute_jc(u_neighbours, v):\n",
    "            # print(StellarGraph.neighbor_arrays(self.graph, v))\n",
    "            v_neighbours = set(StellarGraph.neighbor_arrays(self.graph, v))\n",
    "            union_size = len(u_neighbours.union(v_neighbours))\n",
    "            if union_size == 0:\n",
    "                return 0\n",
    "            return len(u_neighbours.intersection(v_neighbours)) / union_size\n",
    "        node_degrees = self.graph.node_degrees()\n",
    "\n",
    "        if neighbours:\n",
    "            times = [t for _, t in neighbours]\n",
    "            biases = []\n",
    "            node_degree = node_degrees[node]\n",
    "            u_neighbours = set(StellarGraph.neighbor_arrays(self.graph, node))\n",
    "            for ngh, t in neighbours: #G.neighbors(node):\n",
    "                # print(ngh)\n",
    "                pval=compute_jc(u_neighbours, ngh) + 1.0/node_degree\n",
    "                biases.append(pval)\n",
    "                 \n",
    "            # biases = self._temporal_biases(times, time, bias_type, is_forward=True)\n",
    "            if not len(biases): biases = None \n",
    "            # print(len(neighbours), )\n",
    "            chosen_neighbour_index = self._sample(len(neighbours), biases, np_rs)\n",
    "            next_node, next_time = neighbours[chosen_neighbour_index]\n",
    "            return next_node, next_time\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _walk(self, src, dst, t, length, bias_type, np_rs):\n",
    "        walk = [src, dst]\n",
    "        node, time = dst, t\n",
    "        for _ in range(length - 2):\n",
    "            result = self._step(node, time=time, bias_type=bias_type, np_rs=np_rs)\n",
    "\n",
    "            if result is not None:\n",
    "                node, time = result\n",
    "                walk.append(node)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return walk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class nodesim_static_walk():\n",
    "\tdef __init__(self, graph):\n",
    "\t\tprint(graph)\n",
    "\t\tself.G = graph\n",
    "\t\t\n",
    "\tdef nodesim_walk(self, walk_length, start_node):\n",
    "\t\t'''\n",
    "\t\tSimulate nodesim random walk starting from a given node.\n",
    "\t\t'''\n",
    "\t\tG = self.G\n",
    "\t\tprobabilities=self.probabilities\n",
    "\t\tneighbors=self.neighbors\n",
    "\t\twalk = [start_node]\n",
    "\t\twhile len(walk) < walk_length:\n",
    "\t\t\tcur = walk[-1]\n",
    "\t\t\tnextnode=random.choices(list(neighbors[cur]), list(probabilities[cur]))[0]\n",
    "\t\t\twalk.append(nextnode)\n",
    "\t\treturn walk\n",
    "\n",
    "\tdef simulate_walks(self, num_walks, walk_length):\n",
    "\t\t'''\n",
    "\t\tRepeatedly run random walks from each node.\n",
    "\t\t'''\n",
    "\t\tG = self.G\n",
    "\t\twalks = []\n",
    "\t\tnodes = list(G.nodes())\n",
    "\t\tprint('Walk iteration:')\n",
    "\t\tfor walk_iter in range(num_walks):\n",
    "\t\t\tprint(str(walk_iter+1), '/', str(num_walks))\n",
    "\t\t\trandom.shuffle(nodes)\n",
    "\t\t\tfor node in nodes:\n",
    "\t\t\t\twalks.append(self.nodesim_walk(walk_length=walk_length, start_node=node))\n",
    "\t\treturn walks\n",
    "\n",
    "\tdef compute_edge_probs(self):\n",
    "\t\t'''\n",
    "\t\tCompute transition probabilities for nodesim random walks.\n",
    "\t\t'''\n",
    "\n",
    "\t\tdef compute_jc(u_neighbours, v):\n",
    "            # print(StellarGraph.neighbor_arrays(self.graph, v))\n",
    "\t\t\tv_neighbours = set(StellarGraph.neighbor_arrays(self.G, v))\n",
    "\t\t\tunion_size = len(u_neighbours.union(v_neighbours))\n",
    "\t\t\tif union_size == 0: return 0\n",
    "\t\t\treturn len(u_neighbours.intersection(v_neighbours)) / union_size\n",
    "\n",
    "\t\tG = self.G\n",
    "\t\tnode_degrees = G.node_degrees()\n",
    "\t\t\n",
    "\t\tprobs={}\n",
    "\t\tnghs={}\n",
    "\t\tnode_degrees = G.node_degrees()\n",
    "\t\tfor node in G.nodes():\n",
    "\t\t\tnghbrs=[]\n",
    "\t\t\tpr=[]\n",
    "\t\t\tnode_degree = node_degrees[node]\n",
    "\t\t\tu_neighbours = set(StellarGraph.neighbor_arrays(G, node))\n",
    "\t\t\tfor ngh in G.neighbors(node):\n",
    "\t\t\t\tnghbrs.append(ngh)\n",
    "\t\t\t\tpval=compute_jc(u_neighbours, ngh) + 1.0/node_degree\n",
    "\t\t\t\tpr.append(pval)\n",
    "\t\t\t\t\n",
    "\t\t\ts=sum(pr)\n",
    "\t\t\tpr=[x / s for x in pr]\t\t\n",
    "\t\t\tprobs[node]=pr\n",
    "\t\t\tnghs[node]=nghbrs\n",
    "\t\t\t\t\n",
    "\t\tself.probabilities=probs\n",
    "\t\tself.neighbors=nghs\n",
    "\t\treturn 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(graph, prediction_window_size=2, test_neg_size=None): \n",
    "    # identify first 5 & last 2 days' edges based on dates\n",
    "    graph_with_negative_edges = graph.copy()\n",
    "    graph = StellarGraph.from_networkx(graph, edge_weight_attr='time', edge_type_attr='directed')\n",
    "    edges, weights = np.array(graph.edges(include_edge_weight=True)[0]), np.array(graph.edges(include_edge_weight=True)[1])\n",
    "    lower_lim = sorted(list(set(weights)))[-prediction_window_size]\n",
    "    index_test, index_train = np.where(weights >= lower_lim)[0], np.where(weights < lower_lim)[0]\n",
    "\n",
    "    # create test & train edge sets\n",
    "    test_pos_edges, test_time_labels = edges[index_test], weights[index_test]\n",
    "    train_edges, train_time_labels = edges[index_train], weights[index_train]\n",
    "    # test_pos_edges_with_time_attribute = np.rec.fromarrays([test_pos_edges[:,0], test_pos_edges[:,1], test_time_labels])\n",
    "    train_edges_with_time_attribute = np.rec.fromarrays([train_edges[:,0], train_edges[:,1], train_time_labels])\n",
    "\n",
    "    # create test and train graph \n",
    "    train_graph = nx.MultiDiGraph()\n",
    "    train_graph.add_weighted_edges_from(train_edges_with_time_attribute,weight='time') \n",
    "\n",
    "    # create pos & neg edges for test graph \n",
    "    if not test_neg_size: \n",
    "        n = len(test_pos_edges) \n",
    "    else: \n",
    "        n = len(test_pos_edges) * test_neg_size\n",
    "    test_neg_edges = list(nx.non_edges(train_graph)) #take the first x number of non edges\n",
    "    test_pos_edges_tuple = [tuple(x) for x in test_pos_edges]\n",
    "    test_neg_edges = np.array(list(set(test_neg_edges) - set(test_pos_edges_tuple)))[:n]\n",
    "    test_edges = np.vstack((test_pos_edges, test_neg_edges))\n",
    "    test_labels = np.hstack((np.ones(len(test_pos_edges)), np.zeros(len(test_neg_edges))))\n",
    "    \n",
    "    # to make sure there is no negative edge in the train graph that are in the test graph\n",
    "    graph_with_negative_edges.add_edges_from(test_neg_edges)\n",
    "\n",
    "    ## create train & validating edge sets from train graph \n",
    "    train_graph = StellarGraph.from_networkx(train_graph, edge_weight_attr='time', edge_type_attr='directed')\n",
    "    edge_splitter_train = EdgeSplitter(train_graph, graph_with_negative_edges)\n",
    "    graph_train, examples, labels = edge_splitter_train.train_test_split(\n",
    "        p=0.1, method=\"global\"\n",
    "    )\n",
    "    (\n",
    "        examples_train,\n",
    "        examples_validate, \n",
    "        labels_train,\n",
    "        labels_validate\n",
    "    ) = train_test_split(examples, labels, train_size=0.7, test_size=0.3)\n",
    "\n",
    "    \n",
    "    return test_edges, test_labels, graph_train, examples_train, labels_train, examples_validate, labels_validate\n",
    "\n",
    "def random_walk_model(graph, num_walks_per_node=10, walk_length = 10, context_window_size = 2, nodesim=False, temporal=True): \n",
    "    num_cw = len(graph.nodes()) * num_walks_per_node * (walk_length - context_window_size + 1)\n",
    "    if temporal:\n",
    "        if nodesim: \n",
    "            rw_model = NS_weighted_RandomWalk(graph)\n",
    "        else:      \n",
    "            rw_model = TemporalRandomWalk(graph)\n",
    "        walks = rw_model.run(\n",
    "        num_cw=num_cw,\n",
    "        cw_size=context_window_size,\n",
    "        max_walk_length=walk_length,\n",
    "        walk_bias=\"exponential\",\n",
    "        )\n",
    "\n",
    "    else: \n",
    "        if nodesim: \n",
    "            rw_model = NS_weighted_RandomWalk(graph, temporal=False)\n",
    "            walks = rw_model.run(\n",
    "            num_cw=num_cw,\n",
    "            cw_size=context_window_size,\n",
    "            max_walk_length=walk_length,\n",
    "            walk_bias=\"exponential\",\n",
    "            )\n",
    "\n",
    "        else: \n",
    "            rw_model = BiasedRandomWalk(graph)\n",
    "            walks = rw_model.run(\n",
    "            nodes=graph.nodes(), n=num_walks_per_node, length=walk_length\n",
    "            )\n",
    "    \n",
    "    embedding_size = 128\n",
    "    node_embedding = Word2Vec(\n",
    "        walks,\n",
    "        vector_size=embedding_size,\n",
    "        window=context_window_size,\n",
    "        min_count=0,\n",
    "        sg=1,\n",
    "        workers=2,\n",
    "        epochs=1,)\n",
    "\n",
    "    unseen_node_embedding = np.zeros(embedding_size)\n",
    "\n",
    "    def get_node_embedding(u):\n",
    "        try:\n",
    "            return node_embedding.wv[u]\n",
    "        except KeyError:\n",
    "            return unseen_node_embedding\n",
    "    return get_node_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class WeeklyDecisionMaker():\n",
    "    def __init__(self, num_walks_per_node=10, walk_length = 10, context_window_size=2, expert_num=10, reward=False):\n",
    "        # needed for temporal embedding\n",
    "        self.num_walks_per_node = num_walks_per_node\n",
    "        self.walk_length = walk_length\n",
    "        self.context_window_size = context_window_size\n",
    "\n",
    "        # weight optimization\n",
    "        self.reward = reward\n",
    "        self.naive_regret = 0\n",
    "        self.past_naive_regrets = defaultdict(list)\n",
    "        self.graph = None \n",
    "        self.expert_num = expert_num\n",
    "        self.hedge_total_regret = [0]\n",
    "        self.naive_total_regret = [0]\n",
    "\n",
    "        # record purposes \n",
    "        self.test_edges = []\n",
    "        self.test_edge_labels = []\n",
    "        # self.curr_false_positive_set = set()\n",
    "        self.train_scores =  {'temporal': defaultdict(list), 'static': defaultdict(list), 'nodesim_temporal': defaultdict(list), 'nodesim_static': defaultdict(list)}\n",
    "        self.validation_scores = {'temporal': defaultdict(list), 'static': defaultdict(list), 'nodesim_temporal': defaultdict(list), 'nodesim_static': defaultdict(list)}\n",
    "        self.test_scores = defaultdict(list) #[]\n",
    "        self.predicted_probs = [] \n",
    "        self.weight_vectors= defaultdict(list)\n",
    "\n",
    "    def initialize_dictionaries(self, edge_tuple): \n",
    "        self.weight_vectors[edge_tuple] = [1/self.expert_num] * self.expert_num\n",
    "        \n",
    "    def fit_classifier(self, embeddings, labels):\n",
    "        classifier = link_prediction_classifier()\n",
    "        classifier.fit(embeddings, labels)\n",
    "        return classifier\n",
    "\n",
    "    def evaluate_score(self, clf, link_features, link_labels, threshold= 0.5, return_idces=False):\n",
    "        self.predicted_probs = clf.predict_proba(link_features)\n",
    "        positive_column = list(clf.classes_).index(1)\n",
    "        if return_idces: \n",
    "            false_positive_idces = np.where((link_labels == 0) & (self.predicted_probs[:, positive_column] > threshold))[0]\n",
    "            # true_positive_idces = np.where((link_labels == 1) & (predicted[:, positive_column] > 0.5))[0]\n",
    "            return roc_auc_score(link_labels, self.predicted_probs[:, positive_column]), false_positive_idces #, true_positive_idces\n",
    "        return roc_auc_score(link_labels, self.predicted_probs[:, positive_column])\n",
    "\n",
    "    def update_curr_false_positive(self, new_edge_set, display_progress=False): \n",
    "        not_false_positive_anymore = self.curr_false_positive_set.intersection(new_edge_set)\n",
    "        if display_progress: print(f\"Not false positive anymore: {len(not_false_positive_anymore)}\")\n",
    "        if not_false_positive_anymore: \n",
    "            self.naive_regret -= len(not_false_positive_anymore)\n",
    "            self.curr_false_positive_set = self.curr_false_positive_set - not_false_positive_anymore\n",
    "            if display_progress: print(f\"New Reduced Regret: {len(self.curr_false_positive_set)}\")            \n",
    "\n",
    "    def predict_probs(self, graph, display_progress=False, temporal=True): \n",
    "        # update regret by checking if they are in the new graph just given  \n",
    "        # self.update_curr_false_positive(set(graph.edges()), display_progress=display_progress)\n",
    "\n",
    "        # convert networkx graph to stellargraph & split data\n",
    "        # graph = StellarGraph.from_networkx(graph, edge_weight_attr='time', edge_type_attr='directed')\n",
    "        if display_progress: print(\"Splitting data...\")\n",
    "        links_test, labels_test, train_graph, links_train, labels_train, links_validate, labels_validate  = data_split(graph)\n",
    "        self.test_edges, self.test_edge_labels = links_test, labels_test\n",
    "\n",
    "        # fit & learn \n",
    "        if display_progress: print(\"Computing node embeddings...\")\n",
    "        print(\"Computing node embeddings for static nodesim:\")\n",
    "        nodesim_static_embedding = random_walk_model(train_graph,\n",
    "                            num_walks_per_node=self.num_walks_per_node, walk_length=self.walk_length, \\\n",
    "                            context_window_size=self.context_window_size, nodesim=True, temporal= False)\n",
    "        # print(\"Computing node embeddings for nodesim:\")\n",
    "        # nodesim_temporal_embedding = random_walk_model(train_graph,\n",
    "        #                     num_walks_per_node=self.num_walks_per_node, walk_length=self.walk_length, \\\n",
    "        #                     context_window_size=self.context_window_size, nodesim=True, temporal= True)\n",
    "        # print(\"Computing node embeddings for temporal:\")\n",
    "        # temporal_embedding = random_walk_model(train_graph, \n",
    "        #                     num_walks_per_node=self.num_walks_per_node, walk_length=self.walk_length, \\\n",
    "        #                     context_window_size=self.context_window_size, temporal= True)\n",
    "        # static_embedding = random_walk_model(train_graph,\n",
    "        #                     num_walks_per_node=self.num_walks_per_node, walk_length=self.walk_length, \\\n",
    "        #                     context_window_size=self.context_window_size, temporal= False)\n",
    "        \n",
    "        # get edge embeddings for all edges\n",
    "        if display_progress: print(\"Getting edge embeddings...\")\n",
    "        embedded_features = {'temporal': defaultdict(list), 'static': defaultdict(list), 'nodesim_temporal': defaultdict(list), 'nodesim_static': defaultdict(list)}\n",
    "        for links in [links_train, links_validate]:\n",
    "            for operator,operator_name in zip(operators, operator_names): \n",
    "                temporal_edge_embedding = edge_to_features_given_operator(links, temporal_embedding, operator)\n",
    "                static_edge_embedding = edge_to_features_given_operator(links, static_embedding, operator)\n",
    "                nodesim_temporal_edge_embedding = edge_to_features_given_operator(links, nodesim_temporal_embedding, operator)\n",
    "                # embedded_features['temporal'][operator_name].append(temporal_edge_embedding)\n",
    "                # embedded_features['static'][operator_name].append(static_edge_embedding)\n",
    "                # embedded_features['nodesim_temporal'][operator_name].append(edge_to_features_given_operator(links, nodesim_temporal_embedding, operator))\n",
    "                embedded_features['nodesim_static'][operator_name].append(edge_to_features_given_operator(links, nodesim_static_embedding, operator))\n",
    "\n",
    "        # temporal_edge_embedding_train_per_operator = edge_to_features(links_validate, temporal_embedding)\n",
    "        \n",
    "        fitted_classifier_per_operator = {}\n",
    "        \n",
    "        for embedding_type in ['nodesim_static', 'nodesim_temporal', 'temporal', 'static']: \n",
    "            for operator_name in operator_names:\n",
    "                if display_progress: print(f\"Fitting classifiers & Evaluating scores ... for {operator_name} .. \")\n",
    "                fitted_classifier_per_operator[operator_name] = self.fit_classifier(embedded_features[embedding_type][operator_name][0], labels_train) #fit classifier\n",
    "                train_score_per_operator = self.evaluate_score(fitted_classifier_per_operator[operator_name], embedded_features[embedding_type][operator_name][0], labels_train)\n",
    "                self.train_scores[embedding_type][operator_name].append(train_score_per_operator)\n",
    "                validation_score_per_operator = self.evaluate_score(fitted_classifier_per_operator[operator_name], embedded_features[embedding_type][operator_name][1], labels_validate)\n",
    "                self.validation_scores[embedding_type][operator_name].append(validation_score_per_operator)\n",
    "\n",
    "                if display_progress: \n",
    "                    print(f\"{embedding_type} train Score (ROC AUC): {train_score_per_operator:.2f}\")\n",
    "                    print(f\"{embedding_type} Validation Score (ROC AUC): {validation_score_per_operator:.2f}\")\n",
    "\n",
    "            # choose the best classifier based on validation score\n",
    "            if display_progress: print(\"Choosing the best classifier based on validation score ...\")\n",
    "            best_operator = operator_names[np.argmax([self.validation_scores[embedding_type][operator_name][-1] for operator_name in operator_names])]\n",
    "            if display_progress: print(f\"Best Operator: {best_operator}\")\n",
    "\n",
    "            # test the best classifier on test set\n",
    "            test_embedding = None\n",
    "            if embedding_type == 'temporal':\n",
    "                test_embedding = edge_to_features_given_operator_name(links_test, temporal_embedding, best_operator)\n",
    "            elif embedding_type == 'static':\n",
    "                test_embedding = edge_to_features_given_operator_name(links_test, static_embedding, best_operator)\n",
    "            elif embedding_type == 'nodesim_temporal':\n",
    "                test_embedding = edge_to_features_given_operator_name(links_test, nodesim_temporal_embedding, best_operator)\n",
    "            else: \n",
    "                test_embedding = edge_to_features_given_operator_name(links_test, nodesim_static_embedding, best_operator)\n",
    "\n",
    "            test_score, false_positive_idces = self.evaluate_score(fitted_classifier_per_operator[operator_name], test_embedding, labels_test, return_idces=True)\n",
    "            self.test_scores[embedding_type].append(test_score)\n",
    "            if display_progress: \n",
    "                print(f\"{embedding_type} Test Score (ROC AUC): {test_score:.2f}\\n\")\n",
    "        \n",
    "    def optimize_weights_for_edge(self, learning_rate=0.1, leader_thresholds = np.arange(0.1, 1.1, 0.1)): \n",
    "        self.hedge_total_regret.append(self.hedge_total_regret[-1])\n",
    "        self.naive_total_regret.append(self.naive_total_regret[-1])\n",
    "\n",
    "        # update regret for all test edges, keep track of weight vectors per edge \n",
    "        for edge_index, edge in enumerate(self.test_edges): \n",
    "            edge_tuple = tuple(edge)\n",
    "            if edge_tuple not in self.weight_vectors.keys(): \n",
    "                self.initialize_dictionaries(edge_tuple)\n",
    "            edge_existence = self.test_edge_labels[edge_index]\n",
    "            edge_existence_prob = self.predicted_probs[edge_index,1] \n",
    "            self.hedge_per_edge(edge_tuple, edge_existence_prob, edge_existence, learning_rate, leader_thresholds)\n",
    "\n",
    "    def hedge_per_edge(self, edge_tuple, edge_existence_prob, edge_existence, learning_rate = 0.1, leader_thresholds = np.arange(0.1, 1.1, 0.1)):\n",
    "        wait_leaders = [1 if edge_existence_prob > threshold else 0 for threshold in leader_thresholds]\n",
    "        not_wait_leaders = [1 if edge_existence_prob < threshold else 0 for threshold in leader_thresholds]\n",
    "        weight_vector = self.weight_vectors[edge_tuple]\n",
    "\n",
    "        # make decision based on each leader's recommendation & weight vector\n",
    "        final_wait_decision = True if np.dot(wait_leaders, weight_vector) > np.dot(not_wait_leaders, weight_vector) else False \n",
    "        naive_decision = True if edge_existence_prob > 0.5 else False\n",
    "\n",
    "        # record regret by checking if they are in the new graph & update weight \n",
    "        leader_made_wrong_decision = [1 if wait_decision != edge_existence  else 0 for wait_decision in wait_leaders]\n",
    "        loss = 0\n",
    "        if final_wait_decision != edge_existence:\n",
    "            if final_wait_decision == 1: \n",
    "                loss = 3 #false positive treated most harshly \n",
    "            else: loss = 1 #false negative treated less harshly\n",
    "        elif self.reward: \n",
    "            leader_made_wrong_decision = [-1 * wrong_decision for wrong_decision in leader_made_wrong_decision]\n",
    "            if final_wait_decision == 1: \n",
    "                loss = -2 #true positive rewarded most  \n",
    "            else: loss = -1 #true negative rewarded second most\n",
    "\n",
    "        weight_vector = [weight_i * np.exp(-learning_rate * loss * wrong_decision) for wrong_decision, weight_i in zip(leader_made_wrong_decision, weight_vector)]        \n",
    "        weight_vector = weight_vector/ np.sum(weight_vector) # renormalize weight_vector \n",
    "        self.weight_vectors[edge_tuple] = weight_vector\n",
    "        self.hedge_total_regret[-1] += loss\n",
    "        \n",
    "        # update naive regret \n",
    "        curr_naive_loss = 0\n",
    "        if naive_decision != edge_existence: \n",
    "            if final_wait_decision == 1: \n",
    "                curr_naive_loss = 3 #false positive treated most harshly \n",
    "            else: curr_naive_loss = 1 #false negative treated less harshly\n",
    "        \n",
    "        elif self.reward: \n",
    "            if naive_decision == 1: \n",
    "                curr_naive_loss = -2 #true positive rewarded most\n",
    "            else: curr_naive_loss = -1 #true negative rewarded second most\n",
    "\n",
    "        self.naive_total_regret[-1] += curr_naive_loss\n",
    "\n",
    "    def run_expert_simulation(self, graph, learning_rate=0.1, temporal=True, display_progress=False):\n",
    "        self.predict_probs(graph, display_progress=display_progress, temporal=temporal)\n",
    "        self.optimize_weights_for_edge(learning_rate=learning_rate)\n",
    "        if display_progress: self.print_info_given_week(graph.graph['end_date'])\n",
    "        \n",
    "    def print_info_given_week(self, curr_time): \n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Current Week: {curr_time}\")\n",
    "        # print(f\"training error: {np.round(self.train_errors[-1],2)}, test error: {np.round(self.test_errors[-1],2)}\")\n",
    "        print(f\"Total Number of Test Edges: {len(self.test_edges)}\")\n",
    "        print(f\"Current Expert Regret: { self.hedge_total_regret[-1]}\")\n",
    "        print(f\"Current Naive Regret: {self.naive_total_regret[-1]}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "    def graph_train_test_errors(self, time_range, graph_errors = True, graph_regret = False, title=None): \n",
    "        n = min(len(self.train_errors), len(self.test_errors))\n",
    "        if title: plt.title(title)\n",
    "        if graph_errors: \n",
    "            # plt.plot(time_range[:n], self.train_errors[:n], label='train')\n",
    "            plt.plot(time_range[:n], self.test_errors[:n], label='test')\n",
    "        if graph_regret: \n",
    "            plt.plot(time_range[:n], self.curr_regret[:n], label='regret')\n",
    "        plt.legend() \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n",
      "** Sampled 336 positive and 336 negative edges. **\n",
      "Computing node embeddings...\n",
      "Computing node embeddings for static nodesim:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [05:33<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting edge embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'temporal_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m zip3_comparison \u001b[39m=\u001b[39m WeeklyDecisionMaker(reward\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m graph \u001b[39min\u001b[39;00m tqdm(weekly_zip3_graphs\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m----> 3\u001b[0m     zip3_comparison\u001b[39m.\u001b[39;49mpredict_probs(graph, display_progress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[145], line 86\u001b[0m, in \u001b[0;36mWeeklyDecisionMaker.predict_probs\u001b[0;34m(self, graph, display_progress, temporal)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mfor\u001b[39;00m links \u001b[39min\u001b[39;00m [links_train, links_validate]:\n\u001b[1;32m     85\u001b[0m     \u001b[39mfor\u001b[39;00m operator,operator_name \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(operators, operator_names): \n\u001b[0;32m---> 86\u001b[0m         temporal_edge_embedding \u001b[39m=\u001b[39m edge_to_features_given_operator(links, temporal_embedding, operator)\n\u001b[1;32m     87\u001b[0m         static_edge_embedding \u001b[39m=\u001b[39m edge_to_features_given_operator(links, static_embedding, operator)\n\u001b[1;32m     88\u001b[0m         nodesim_temporal_edge_embedding \u001b[39m=\u001b[39m edge_to_features_given_operator(links, nodesim_temporal_embedding, operator)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'temporal_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "zip3_comparison = WeeklyDecisionMaker(reward=False)\n",
    "for graph in tqdm(weekly_zip3_graphs.values()):\n",
    "    zip3_comparison.predict_probs(graph, display_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. given a route, are we doing this? \n",
    "2. or are we doing this for the entire graph? \n",
    "\n",
    "Since we are making the prediction on all potential edges for a graph, how would we do this..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg_graph = StellarGraph.from_networkx(graph, edge_weight_attr='time', edge_type_attr='directed')\n",
    "# [ngh for ngh, t in sg_graph.neighbors('561', include_edge_weight=True) if t > 20211108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# n = len(zip3_comparison.test_scores['temporal'])\n",
    "# time_range = list(weekly_zip3_graphs.keys())[:n]\n",
    "# for operator_name in operator_names: \n",
    "#     ax[0].plot(time_range, zip3_comparison.validation_scores['temporal'][operator_name], label='temporal ' + operator_name)\n",
    "#     ax[1].plot(time_range[:-1], zip3_comparison.validation_scores['static'][operator_name][:44], label='static ' + operator_name)\n",
    "#     # ax.plot(time_range[:-1], zip3_comparison.test_scores['static'], label='static ' + operator_name)\n",
    "# # ax.hlines(np.mean(zip3_comparison.test_scores['temporal']), time_range[0], time_range[-1], label='temporal av', color='r')\n",
    "# # ax.plot(time_range[:-1], zip3_comparison.test_scores['static'], label='static')\n",
    "# # ax.hlines(np.mean(zip3_comparison.test_scores['static']), time_range[0], time_range[-1], label='static av', color='b')\n",
    "\n",
    "# ax[0].legend()\n",
    "# ax[1].legend()\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.read_csv(\"results/temporal_zip3_dm_result.csv\")\n",
    "# hedge_results, naive_results = list(results['hedge']), list(results['naive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kma_comparison = WeeklyDecisionMaker(reward=False)\n",
    "for graph in tqdm(weekly_kma_graphs.values()):\n",
    "    kma_comparison.run_expert_simulation(graph, display_progress=True)\n",
    "\n",
    "# fig, ax = plt.subplots(1,1, figsize= (10, 5))\n",
    "# fig.suptitle(\"Regret Comparison of Static Random Walk based Decision Maker on KMA data\")\n",
    "# ax.plot(weekly_zip3_graphs.keys(), static_kma_dm.hedge_total_regret[1:], label='hedge')\n",
    "# ax.plot(weekly_zip3_graphs.keys(), static_kma_dm.naive_total_regret[1:], label='naive')\n",
    "# ax.legend()\n",
    "# ax.set_xlabel(\"Week\")\n",
    "# ax.set_ylabel(\"Regret\")\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
